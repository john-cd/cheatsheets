{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"You will find here a collection of cheatsheets, code templates and snippets that I have collected over the years... Given that they were created for my own use, these notes are often very terse and dense. Thank you for your patience, while I am slowly improving their readability. I also have hundreds more to move to GitHub Pages :-) In the meanwhile, feel free to use as you wish. Please email me suggestions and corrections.","title":"Home"},{"location":"Big_Data/Hadoop_Ecosystem/","text":"Hadoop Ecosystem \u00b6 Hadoop is not a single product, but rather a software family. Its common components consist of the following: Pig, a scripting language used to quickly write MapReduce code to handle unstructured sources Hive, used to facilitate structure for the data HCatalog, used to provide inter-operatability between these internal systems HBase, which is essentially a database built on top of Hadoop HDFS, the actual file system for hadoop. Apache Mahout Packaging for Hadoop: BigTop Hadoop structures data using Hive, but can handle unstructured data easily using Pig. Hadoop and Mongo \u00b6 Hadoop and MongoDB Hadoop and MongoDB Use Cases AWS EMR \u00b6 Amazon EMR Best Practices Amazon EMR includes Ganglia Hadoop HBase HCatalog Hive Hue Mahout Oozie Phoenix Pig Prest0 Spark Sqoop Tez Zeppelin ZooKeeper","title":"Hadoop Ecosystem"},{"location":"Big_Data/Hadoop_Ecosystem/#hadoop-ecosystem","text":"Hadoop is not a single product, but rather a software family. Its common components consist of the following: Pig, a scripting language used to quickly write MapReduce code to handle unstructured sources Hive, used to facilitate structure for the data HCatalog, used to provide inter-operatability between these internal systems HBase, which is essentially a database built on top of Hadoop HDFS, the actual file system for hadoop. Apache Mahout Packaging for Hadoop: BigTop Hadoop structures data using Hive, but can handle unstructured data easily using Pig.","title":"Hadoop Ecosystem"},{"location":"Big_Data/Hadoop_Ecosystem/#hadoop-and-mongo","text":"Hadoop and MongoDB Hadoop and MongoDB Use Cases","title":"Hadoop and Mongo"},{"location":"Big_Data/Hadoop_Ecosystem/#aws-emr","text":"Amazon EMR Best Practices Amazon EMR includes Ganglia Hadoop HBase HCatalog Hive Hue Mahout Oozie Phoenix Pig Prest0 Spark Sqoop Tez Zeppelin ZooKeeper","title":"AWS EMR"},{"location":"Big_Data/Install_Spark_2.3_Locally/","text":"Install Spark 2.3 Locally \u00b6 Spark runs on Java 8+, Python 2.7+/3.4+ and R 3.1+. For the Scala API, Spark 2.3.0 uses Scala 2.11. Download Spark \u00b6 Link Java \u00b6 All you need is to have java installed on your system PATH, or the JAVA_HOME environment variable pointing to a Java installation. java -version Scala \u00b6 Download the Scala binaries for windows -- you will need Scala 11.x (not 10.x or 12.x) for Spark 2.3 Latest Scala Scala version 2.11.12 Test correct installation of scala: scala -version Set PATH for Scala if needed: export PATH = $PATH :/usr/local/scala/bin Test that Spark is properly installed: ./bin/spark-shell --master local [ 2 ] On Windows, use CMD or PowerShell, not git bash Error: Failure to locate the winutils binary in the hadoop binary path \u00b6 HADOOP_HOME (or the variable hadoop.home.dir property) needs to be set properly. Known Hadoop for Windows issue: winutils is not included in the Apache distribution You can fix this problem in two ways Install a full native windows Hadoop version. The ASF does not currently release such a version; releases are available externally. Or: get the WINUTILS.EXE binary from a Hadoop redistribution. There is a repository of this for some Hadoop versions on github. Then Set the environment variable %HADOOP_HOME% to point to the directory above the BIN dir containing WINUTILS.EXE. Or: run the Java process with the system property hadoop.home.dir set to the home directory. Explanation on Hadoop Wiki Stack Overflow Windows binaries for some Hadoop versions Run Spark on the local machine \u00b6 To run Spark interactively in a Python interpreter, use bin/pyspark : ./bin/pyspark --master local [ 2 ] Or submit Spark jobs: ./bin/spark-submit examples/src/main/python/pi.py 10 Additional Links \u00b6 Spark Installation Tutorial","title":"Install Spark 2.3 Locally"},{"location":"Big_Data/Install_Spark_2.3_Locally/#install-spark-23-locally","text":"Spark runs on Java 8+, Python 2.7+/3.4+ and R 3.1+. For the Scala API, Spark 2.3.0 uses Scala 2.11.","title":"Install Spark 2.3 Locally"},{"location":"Big_Data/Install_Spark_2.3_Locally/#download-spark","text":"Link","title":"Download Spark"},{"location":"Big_Data/Install_Spark_2.3_Locally/#java","text":"All you need is to have java installed on your system PATH, or the JAVA_HOME environment variable pointing to a Java installation. java -version","title":"Java"},{"location":"Big_Data/Install_Spark_2.3_Locally/#scala","text":"Download the Scala binaries for windows -- you will need Scala 11.x (not 10.x or 12.x) for Spark 2.3 Latest Scala Scala version 2.11.12 Test correct installation of scala: scala -version Set PATH for Scala if needed: export PATH = $PATH :/usr/local/scala/bin Test that Spark is properly installed: ./bin/spark-shell --master local [ 2 ] On Windows, use CMD or PowerShell, not git bash","title":"Scala"},{"location":"Big_Data/Install_Spark_2.3_Locally/#error-failure-to-locate-the-winutils-binary-in-the-hadoop-binary-path","text":"HADOOP_HOME (or the variable hadoop.home.dir property) needs to be set properly. Known Hadoop for Windows issue: winutils is not included in the Apache distribution You can fix this problem in two ways Install a full native windows Hadoop version. The ASF does not currently release such a version; releases are available externally. Or: get the WINUTILS.EXE binary from a Hadoop redistribution. There is a repository of this for some Hadoop versions on github. Then Set the environment variable %HADOOP_HOME% to point to the directory above the BIN dir containing WINUTILS.EXE. Or: run the Java process with the system property hadoop.home.dir set to the home directory. Explanation on Hadoop Wiki Stack Overflow Windows binaries for some Hadoop versions","title":"Error: Failure to locate the winutils binary in the hadoop binary path"},{"location":"Big_Data/Install_Spark_2.3_Locally/#run-spark-on-the-local-machine","text":"To run Spark interactively in a Python interpreter, use bin/pyspark : ./bin/pyspark --master local [ 2 ] Or submit Spark jobs: ./bin/spark-submit examples/src/main/python/pi.py 10","title":"Run Spark on the local machine"},{"location":"Big_Data/Install_Spark_2.3_Locally/#additional-links","text":"Spark Installation Tutorial","title":"Additional Links"},{"location":"Big_Data/Spark_APIs/","text":"DataFrames APIs \u00b6 DataFrame operations: printSchema() select() show() count() groupBy() sum() limit() orderBy() filter() withColumnRenamed() join() withColumn() Example: // In the Regular Expression below: // ^ - Matches beginning of line // .* - Matches any characters, except newline df . filter ( $ \"article\" . rlike ( \"\"\"^Apache_.*\"\"\" )) . orderBy ( $ \"requests\" . desc ) . show () // By default, show will return 20 rows // Import the sql functions package, which includes statistical functions like sum, max, min, avg, etc. import org.apache.spark.sql.functions._ df . groupBy ( \"project\" ). sum (). show () Columns \u00b6 A new column is constructed based on the input columns present in a dataframe: df ( \"columnName\" ) // On a specific DataFrame. col ( \"columnName\" ) // A generic column no yet associated with a DataFrame. col ( \"columnName.field\" ) // Extracting a struct field col ( \"`a.column.with.dots`\" ) // Escape `.` in column names. $ \"columnName\" // Scala short hand for a named column. expr ( \"a + 1\" ) // A column that is constructed from a parsed SQL Expression. lit ( \"abc\" ) // A column that produces a literal (constant) value. Column objects can be composed to form complex expressions: $ \"a\" + 1 $ \"a\" === $ \"b\" File Read \u00b6 CSV - Create a DataFrame with the anticipated structure val clickstreamDF = sqlContext . read . format ( \"com.databricks.spark.csv\" ) . option ( \"header\" , \"true\" ) . option ( \"delimiter\" , \"\\\\t\" ) . option ( \"mode\" , \"PERMISSIVE\" ) . option ( \"inferSchema\" , \"true\" ) . load ( \"dbfs:///databricks-datasets/wikipedia-datasets/data-001/clickstream/raw-uncompressed\" ) PARQUET - To create Dataset[Row] using SparkSession val people = spark . read . parquet ( \"...\" ) val department = spark . read . parquet ( \"...\" ) people . filter ( \"age > 30\" ) . join ( department , people ( \"deptId\" ) === department ( \"id\" )) . groupBy ( department ( \"name\" ), \"gender\" ) . agg ( avg ( people ( \"salary\" )), max ( people ( \"age\" ))) Repartitioning / Caching \u00b6 val clickstreamNoIDs8partDF = clickstreamNoIDsDF . repartition ( 8 ) clickstreamNoIDs8partDF . registerTempTable ( \"Clickstream\" ) sqlContext . cacheTable ( \"Clickstream\" ) An ideal partition size in Spark is about 50 MB - 200 MB. The cache gets stored in Project Tungsten binary compressed columnar format.","title":"Spark APIs"},{"location":"Big_Data/Spark_APIs/#dataframes-apis","text":"DataFrame operations: printSchema() select() show() count() groupBy() sum() limit() orderBy() filter() withColumnRenamed() join() withColumn() Example: // In the Regular Expression below: // ^ - Matches beginning of line // .* - Matches any characters, except newline df . filter ( $ \"article\" . rlike ( \"\"\"^Apache_.*\"\"\" )) . orderBy ( $ \"requests\" . desc ) . show () // By default, show will return 20 rows // Import the sql functions package, which includes statistical functions like sum, max, min, avg, etc. import org.apache.spark.sql.functions._ df . groupBy ( \"project\" ). sum (). show ()","title":"DataFrames APIs"},{"location":"Big_Data/Spark_APIs/#columns","text":"A new column is constructed based on the input columns present in a dataframe: df ( \"columnName\" ) // On a specific DataFrame. col ( \"columnName\" ) // A generic column no yet associated with a DataFrame. col ( \"columnName.field\" ) // Extracting a struct field col ( \"`a.column.with.dots`\" ) // Escape `.` in column names. $ \"columnName\" // Scala short hand for a named column. expr ( \"a + 1\" ) // A column that is constructed from a parsed SQL Expression. lit ( \"abc\" ) // A column that produces a literal (constant) value. Column objects can be composed to form complex expressions: $ \"a\" + 1 $ \"a\" === $ \"b\"","title":"Columns"},{"location":"Big_Data/Spark_APIs/#file-read","text":"CSV - Create a DataFrame with the anticipated structure val clickstreamDF = sqlContext . read . format ( \"com.databricks.spark.csv\" ) . option ( \"header\" , \"true\" ) . option ( \"delimiter\" , \"\\\\t\" ) . option ( \"mode\" , \"PERMISSIVE\" ) . option ( \"inferSchema\" , \"true\" ) . load ( \"dbfs:///databricks-datasets/wikipedia-datasets/data-001/clickstream/raw-uncompressed\" ) PARQUET - To create Dataset[Row] using SparkSession val people = spark . read . parquet ( \"...\" ) val department = spark . read . parquet ( \"...\" ) people . filter ( \"age > 30\" ) . join ( department , people ( \"deptId\" ) === department ( \"id\" )) . groupBy ( department ( \"name\" ), \"gender\" ) . agg ( avg ( people ( \"salary\" )), max ( people ( \"age\" )))","title":"File Read"},{"location":"Big_Data/Spark_APIs/#repartitioning-caching","text":"val clickstreamNoIDs8partDF = clickstreamNoIDsDF . repartition ( 8 ) clickstreamNoIDs8partDF . registerTempTable ( \"Clickstream\" ) sqlContext . cacheTable ( \"Clickstream\" ) An ideal partition size in Spark is about 50 MB - 200 MB. The cache gets stored in Project Tungsten binary compressed columnar format.","title":"Repartitioning / Caching"},{"location":"Big_Data/Spark_Basics/","text":"Spark Basics \u00b6 Main Web Site Apache Spark on Wikipedia Useful Links \u00b6 Ampcamp big data bootcamp RDDs Simplified Elasticsearch and Apache Lucene for Apache Spark and MLlib Spark on AWS Running Apache Spark on AWS Running Apache Spark EMR and EC2 scripts on AWS with read write S3 Spark on EMR - How to Submit a Spark Application with EMR Steps Databricks Reference Apps Introduction to Apache Spark with Examples and Use Cases Spark and MongoDB \u00b6 Using MongoDB with Apache Spark MongoDB Spark connector Spark and NLP \u00b6 Dictionary Based Annotation at Scale with Spark, SolrTextTagger and OpenNLP Here is a complete set of example on how to use DL4J (Deep Learning for Java) that uses UIMA on the SPARK platform Deep Learning for Java and in the following project the use of CTAKES UIMA module from within the Spark framework Natural Language Processing with Apache Spark GraphX \u00b6 Graphx programming guide Apache Zeppelin \u00b6 Connect to Zeppelin using the same SSH tunneling method to connect to other web servers on the master node. Zeppelin server is found at port 8890. Zeppelin","title":"Spark Basics"},{"location":"Big_Data/Spark_Basics/#spark-basics","text":"Main Web Site Apache Spark on Wikipedia","title":"Spark Basics"},{"location":"Big_Data/Spark_Basics/#useful-links","text":"Ampcamp big data bootcamp RDDs Simplified Elasticsearch and Apache Lucene for Apache Spark and MLlib Spark on AWS Running Apache Spark on AWS Running Apache Spark EMR and EC2 scripts on AWS with read write S3 Spark on EMR - How to Submit a Spark Application with EMR Steps Databricks Reference Apps Introduction to Apache Spark with Examples and Use Cases","title":"Useful Links"},{"location":"Big_Data/Spark_Basics/#spark-and-mongodb","text":"Using MongoDB with Apache Spark MongoDB Spark connector","title":"Spark and MongoDB"},{"location":"Big_Data/Spark_Basics/#spark-and-nlp","text":"Dictionary Based Annotation at Scale with Spark, SolrTextTagger and OpenNLP Here is a complete set of example on how to use DL4J (Deep Learning for Java) that uses UIMA on the SPARK platform Deep Learning for Java and in the following project the use of CTAKES UIMA module from within the Spark framework Natural Language Processing with Apache Spark","title":"Spark and NLP"},{"location":"Big_Data/Spark_Basics/#graphx","text":"Graphx programming guide","title":"GraphX"},{"location":"Big_Data/Spark_Basics/#apache-zeppelin","text":"Connect to Zeppelin using the same SSH tunneling method to connect to other web servers on the master node. Zeppelin server is found at port 8890. Zeppelin","title":"Apache Zeppelin"},{"location":"Big_Data/Spark_Development_with_sbt_and_InteliJ/","text":"Setup a Spark Development Environment with IntelliJ and sbt \u00b6 Useful Links \u00b6 Hortonworks tutorial Packaging and Submission Steps using sbt \u00b6 Package a jar containing your application: $ sbt package ... [ info ] Packaging { .. } / { .. } /target/scala-2.11/simple-project_2.11-1.0.jar Don't use sbt run Then use [spark submit] ( https://spark.apache.org/docs/latest/submitting-applications.html#launching-applications-with-spark-submit ) to run your application YOUR_SPARK_HOME/bin/spark-submit \\ --class \"SimpleApp\" \\ --master local [ 4 ] \\ target/scala-2.11/simple-project_2.11-1.0.jar Open the Spark UI to monitor: http://localhost:4040 Plugins \u00b6 sbt-spark-package \u00b6 The Sbt Plugin for Spark Packages is a Sbt plugin that aims to simplify the use and development of Spark Packages. Blog IntelliJ plugin for Spark \u00b6 Note: does not work with IntelliJ 2018.1 The IntelliJ plugin for Spark supports for deployment spark application and cluster monitoring. To install, download the plugin File > Settings, Plugins tab, browse repos... point to the zip file","title":"Setup a Spark Development Environment with IntelliJ and sbt"},{"location":"Big_Data/Spark_Development_with_sbt_and_InteliJ/#setup-a-spark-development-environment-with-intellij-and-sbt","text":"","title":"Setup a Spark Development Environment with IntelliJ and sbt"},{"location":"Big_Data/Spark_Development_with_sbt_and_InteliJ/#useful-links","text":"Hortonworks tutorial","title":"Useful Links"},{"location":"Big_Data/Spark_Development_with_sbt_and_InteliJ/#packaging-and-submission-steps-using-sbt","text":"Package a jar containing your application: $ sbt package ... [ info ] Packaging { .. } / { .. } /target/scala-2.11/simple-project_2.11-1.0.jar Don't use sbt run Then use [spark submit] ( https://spark.apache.org/docs/latest/submitting-applications.html#launching-applications-with-spark-submit ) to run your application YOUR_SPARK_HOME/bin/spark-submit \\ --class \"SimpleApp\" \\ --master local [ 4 ] \\ target/scala-2.11/simple-project_2.11-1.0.jar Open the Spark UI to monitor: http://localhost:4040","title":"Packaging and Submission Steps using sbt"},{"location":"Big_Data/Spark_Development_with_sbt_and_InteliJ/#plugins","text":"","title":"Plugins"},{"location":"Big_Data/Spark_Development_with_sbt_and_InteliJ/#sbt-spark-package","text":"The Sbt Plugin for Spark Packages is a Sbt plugin that aims to simplify the use and development of Spark Packages. Blog","title":"sbt-spark-package"},{"location":"Big_Data/Spark_Development_with_sbt_and_InteliJ/#intellij-plugin-for-spark","text":"Note: does not work with IntelliJ 2018.1 The IntelliJ plugin for Spark supports for deployment spark application and cluster monitoring. To install, download the plugin File > Settings, Plugins tab, browse repos... point to the zip file","title":"IntelliJ plugin for Spark"},{"location":"Big_Data/Spark_on_AWS_EMR/","text":"Spark on AWS EMR \u00b6 Key Links \u00b6 Spark on AWS EMR Create a EMR Cluster with Spark using the AWS Console \u00b6 The following procedure creates a cluster with Spark installed. Open the Amazon EMR console at https://console.aws.amazon.com/elasticmapreduce/ . Choose Create cluster to use Quick Create. For the Software Configuration field, choose Amazon Release Version emr-5.0.0 or later. In the Select Applications field, choose either All Applications or Spark. Select other options as necessary and then choose Create cluster Create a EMR Cluster with Spark using the AWS CLI \u00b6 Simple cluster: aws emr create-cluster --name \"Spark cluster\" --release-label emr-5.0.0 --applications Name = Spark \\ --ec2-attributes KeyName = myKey --instance-type m3.xlarge --instance-count 3 --use-default-roles Note: For Windows, replace the above Linux line continuation character () with the caret (^). When using a config file: aws emr create-cluster --release-label --applications Name = Spark \\ --instance-type m3.xlarge --instance-count 3 --configurations https://s3.amazonaws.com/mybucket/myfolder/myConfig.json Sample myConfig.json: [ { \"Classification\" : \"spark\" , \"Properties\" : { \"maximizeResourceAllocation\" : \"true\" } } ] Using Spot instances: aws emr create-cluster --name \"Spot cluster\" --release-label emr-5.0.0 --applications Name = Spark \\ --use-default-roles --ec2-attributes KeyName = myKey \\ --instance-groups InstanceGroupType = MASTER,InstanceType = m3.xlarge,InstanceCount = 1 ,BidPrice = 0 .25 \\ InstanceGroupType = CORE,BidPrice = 0 .03,InstanceType = m3.xlarge,InstanceCount = 2 # InstanceGroupType=TASK,BidPrice=0.10,InstanceType=m3.xlarge,InstanceCount=3 In Java: // start Spark on EMR in java AmazonElasticMapReduceClient emr = new AmazonElasticMapReduceClient ( credentials ); Application sparkApp = new Application () . withName ( \"Spark\" ); Applications myApps = new Applications (); myApps . add ( sparkApp ); RunJobFlowRequest request = new RunJobFlowRequest () . withName ( \"Spark Cluster\" ) . withApplications ( myApps ) . withReleaseLabel ( \"\" ) . withInstances ( new JobFlowInstancesConfig () . withEc2KeyName ( \"myKeyName\" ) . withInstanceCount ( 1 ) . withKeepJobFlowAliveWhenNoSteps ( true ) . withMasterInstanceType ( \"m3.xlarge\" ) . withSlaveInstanceType ( \"m3.xlarge\" ) ); RunJobFlowResult result = emr . runJobFlow ( request ); Connect to the Master Node using SSH \u00b6 To connect to the master node using SSH, you need the public DNS name of the master node and your Amazon EC2 key pair private key. The Amazon EC2 key pair private key is specified when you launch the cluster. To retrieve the cluster identifier / the public DNS name of the master node, type the following command: aws emr list-clusters The output lists your clusters including the cluster IDs. Note the cluster ID for the cluster to which you are connecting. \"Status\" : { \"Timeline\" : { \"ReadyDateTime\" : 1408040782.374 , \"CreationDateTime\" : 1408040501.213 }, \"State\" : \"WAITING\" , \"StateChangeReason\" : { \"Message\" : \"Waiting after step completed\" } } , \"NormalizedInstanceHours\" : 4 , \"Id\" : \"j-2AL4XXXXXX5T9\" , \"Name\" : \"My cluster\" To list the cluster instances including the master public DNS name for the cluster, type one of the following commands. Replace j-2AL4XXXXXX5T9 with the cluster ID returned by the previous command. aws emr list-instances --cluster-id j-2AL4XXXXXX5T9Or:aws emr describe-clusters --cluster-id j-2AL4XXXXXX5T9 View the Web Interfaces Hosted on Amazon EMR Clusters \u00b6 View Web Interfaces Hosted on Amazon EMR Clusters YARN ResourceManager: http://master-public-dns-name:8088 YARN NodeManager: http://slave-public-dns-name:8042 Hadoop HDFS NameNode: http://master-public-dns-name:50070 Hadoop HDFS DataNode: http://slave-public-dns-name:50075 Spark HistoryServer: http://master-public-dns-name:18080 Zeppelin: http://master-public-dns-name:8890 Hue: http://master-public-dns-name:8888 Ganglia: http://master-public-dns-name/ganglia HBase UI: http://master-public-dns-name:16010","title":"Spark on AWS EMR"},{"location":"Big_Data/Spark_on_AWS_EMR/#spark-on-aws-emr","text":"","title":"Spark on AWS EMR"},{"location":"Big_Data/Spark_on_AWS_EMR/#key-links","text":"Spark on AWS EMR","title":"Key Links"},{"location":"Big_Data/Spark_on_AWS_EMR/#create-a-emr-cluster-with-spark-using-the-aws-console","text":"The following procedure creates a cluster with Spark installed. Open the Amazon EMR console at https://console.aws.amazon.com/elasticmapreduce/ . Choose Create cluster to use Quick Create. For the Software Configuration field, choose Amazon Release Version emr-5.0.0 or later. In the Select Applications field, choose either All Applications or Spark. Select other options as necessary and then choose Create cluster","title":"Create a EMR Cluster with Spark using the AWS Console"},{"location":"Big_Data/Spark_on_AWS_EMR/#create-a-emr-cluster-with-spark-using-the-aws-cli","text":"Simple cluster: aws emr create-cluster --name \"Spark cluster\" --release-label emr-5.0.0 --applications Name = Spark \\ --ec2-attributes KeyName = myKey --instance-type m3.xlarge --instance-count 3 --use-default-roles Note: For Windows, replace the above Linux line continuation character () with the caret (^). When using a config file: aws emr create-cluster --release-label --applications Name = Spark \\ --instance-type m3.xlarge --instance-count 3 --configurations https://s3.amazonaws.com/mybucket/myfolder/myConfig.json Sample myConfig.json: [ { \"Classification\" : \"spark\" , \"Properties\" : { \"maximizeResourceAllocation\" : \"true\" } } ] Using Spot instances: aws emr create-cluster --name \"Spot cluster\" --release-label emr-5.0.0 --applications Name = Spark \\ --use-default-roles --ec2-attributes KeyName = myKey \\ --instance-groups InstanceGroupType = MASTER,InstanceType = m3.xlarge,InstanceCount = 1 ,BidPrice = 0 .25 \\ InstanceGroupType = CORE,BidPrice = 0 .03,InstanceType = m3.xlarge,InstanceCount = 2 # InstanceGroupType=TASK,BidPrice=0.10,InstanceType=m3.xlarge,InstanceCount=3 In Java: // start Spark on EMR in java AmazonElasticMapReduceClient emr = new AmazonElasticMapReduceClient ( credentials ); Application sparkApp = new Application () . withName ( \"Spark\" ); Applications myApps = new Applications (); myApps . add ( sparkApp ); RunJobFlowRequest request = new RunJobFlowRequest () . withName ( \"Spark Cluster\" ) . withApplications ( myApps ) . withReleaseLabel ( \"\" ) . withInstances ( new JobFlowInstancesConfig () . withEc2KeyName ( \"myKeyName\" ) . withInstanceCount ( 1 ) . withKeepJobFlowAliveWhenNoSteps ( true ) . withMasterInstanceType ( \"m3.xlarge\" ) . withSlaveInstanceType ( \"m3.xlarge\" ) ); RunJobFlowResult result = emr . runJobFlow ( request );","title":"Create a EMR Cluster with Spark using the AWS CLI"},{"location":"Big_Data/Spark_on_AWS_EMR/#connect-to-the-master-node-using-ssh","text":"To connect to the master node using SSH, you need the public DNS name of the master node and your Amazon EC2 key pair private key. The Amazon EC2 key pair private key is specified when you launch the cluster. To retrieve the cluster identifier / the public DNS name of the master node, type the following command: aws emr list-clusters The output lists your clusters including the cluster IDs. Note the cluster ID for the cluster to which you are connecting. \"Status\" : { \"Timeline\" : { \"ReadyDateTime\" : 1408040782.374 , \"CreationDateTime\" : 1408040501.213 }, \"State\" : \"WAITING\" , \"StateChangeReason\" : { \"Message\" : \"Waiting after step completed\" } } , \"NormalizedInstanceHours\" : 4 , \"Id\" : \"j-2AL4XXXXXX5T9\" , \"Name\" : \"My cluster\" To list the cluster instances including the master public DNS name for the cluster, type one of the following commands. Replace j-2AL4XXXXXX5T9 with the cluster ID returned by the previous command. aws emr list-instances --cluster-id j-2AL4XXXXXX5T9Or:aws emr describe-clusters --cluster-id j-2AL4XXXXXX5T9","title":"Connect to the Master Node using SSH"},{"location":"Big_Data/Spark_on_AWS_EMR/#view-the-web-interfaces-hosted-on-amazon-emr-clusters","text":"View Web Interfaces Hosted on Amazon EMR Clusters YARN ResourceManager: http://master-public-dns-name:8088 YARN NodeManager: http://slave-public-dns-name:8042 Hadoop HDFS NameNode: http://master-public-dns-name:50070 Hadoop HDFS DataNode: http://slave-public-dns-name:50075 Spark HistoryServer: http://master-public-dns-name:18080 Zeppelin: http://master-public-dns-name:8890 Hue: http://master-public-dns-name:8888 Ganglia: http://master-public-dns-name/ganglia HBase UI: http://master-public-dns-name:16010","title":"View the Web Interfaces Hosted on Amazon EMR Clusters"},{"location":"Big_Data/Spark_on_EC2/","text":"Install Spark on EC2 with Flintrock \u00b6 Key Links \u00b6 Flintrock GitHub Repo Configurable CLI Defaults \u00b6 Flintrock lets you persist your desired configuration to a YAML file so that you don't have to keep typing out the same options over and over at the command line. To setup and edit the default config file, run this: flintrock configure Sample config.yaml \u00b6 provider : ec2 services : spark : version : 2.2.0 launch : num-slaves : 1 providers : ec2 : key-name : key_name identity-file : /path/to/.ssh/key.pem instance-type : m3.medium region : us-east-1 ami : ami-97785bed user : ec2-user With a config file like that, you can now launch a cluster: flintrock launch test-cluster","title":"Install Spark on EC2 with Flintrock"},{"location":"Big_Data/Spark_on_EC2/#install-spark-on-ec2-with-flintrock","text":"","title":"Install Spark on EC2 with Flintrock"},{"location":"Big_Data/Spark_on_EC2/#key-links","text":"Flintrock GitHub Repo","title":"Key Links"},{"location":"Big_Data/Spark_on_EC2/#configurable-cli-defaults","text":"Flintrock lets you persist your desired configuration to a YAML file so that you don't have to keep typing out the same options over and over at the command line. To setup and edit the default config file, run this: flintrock configure","title":"Configurable CLI Defaults"},{"location":"Big_Data/Spark_on_EC2/#sample-configyaml","text":"provider : ec2 services : spark : version : 2.2.0 launch : num-slaves : 1 providers : ec2 : key-name : key_name identity-file : /path/to/.ssh/key.pem instance-type : m3.medium region : us-east-1 ami : ami-97785bed user : ec2-user With a config file like that, you can now launch a cluster: flintrock launch test-cluster","title":"Sample config.yaml"},{"location":"Big_Data/Spark_on_Kubernetes/","text":"Background \u00b6 Introduction to Spark on Kubernetes Running Spark on Kubernetes \u00b6 Main Page Prerequisites: A runnable distribution of Spark 2.3 or above. A running Kubernetes cluster at version >= 1.6 with access configured to it using kubectl. If you do not already have a working Kubernetes cluster, you may setup a test cluster on your local machine using minikube. We recommend using the latest release of minikube with the DNS addon enabled. Be aware that the default minikube configuration is not enough for running Spark applications. We recommend 3 CPUs and 4g of memory to be able to start a simple Spark application with a single executor. You must have appropriate permissions to list, create, edit and delete pods in your cluster. You can verify that you can list these resources by running kubectl auth can-i pods. The service account credentials used by the driver pods must be allowed to create pods, services and configmaps. You must have Kubernetes DNS configured in your cluster. Steps \u00b6 Need Kubernetes version 1.6 and above. To check the version, enter kubectl version . The cluster must be configured to use the kube-dns addon. Check with minikube addons list Kubernetes DNS Page Start minikube with the recommended configuration for Spark minikube start --cpus 3 --memory 4096 Submit a Spark job using: $ bin/spark-submit \\ --master k8s://https://<k8s-apiserver-host>:<k8s-apiserver-port> \\ --deploy-mode cluster \\ --name spark-pi \\ --class org.apache.spark.examples.SparkPi \\ --conf spark.executor.instances = 3 \\ --conf spark.kubernetes.container.image = <spark-image> \\ local:///path/to/examples.jar Use kubectl cluster-info to get the K8s API server URL Spark (starting with version 2.3) ships with a Dockerfile in the kubernetes/dockerfiles/ directory. Access logs: $ kubectl -n = <namespace> logs -f <driver-pod-name> Accessing Driver UI: $ kubectl port-forward <driver-pod-name> 4040 :4040 Then go to http://localhost:4040 Alternatives \u00b6 Helm Chart for Spark The same on KubeApps helm install --name my-spark-release --version 0 .1.12 stable/spark","title":"Spark 2.3 on Kubernetes"},{"location":"Big_Data/Spark_on_Kubernetes/#background","text":"Introduction to Spark on Kubernetes","title":"Background"},{"location":"Big_Data/Spark_on_Kubernetes/#running-spark-on-kubernetes","text":"Main Page Prerequisites: A runnable distribution of Spark 2.3 or above. A running Kubernetes cluster at version >= 1.6 with access configured to it using kubectl. If you do not already have a working Kubernetes cluster, you may setup a test cluster on your local machine using minikube. We recommend using the latest release of minikube with the DNS addon enabled. Be aware that the default minikube configuration is not enough for running Spark applications. We recommend 3 CPUs and 4g of memory to be able to start a simple Spark application with a single executor. You must have appropriate permissions to list, create, edit and delete pods in your cluster. You can verify that you can list these resources by running kubectl auth can-i pods. The service account credentials used by the driver pods must be allowed to create pods, services and configmaps. You must have Kubernetes DNS configured in your cluster.","title":"Running Spark on Kubernetes"},{"location":"Big_Data/Spark_on_Kubernetes/#steps","text":"Need Kubernetes version 1.6 and above. To check the version, enter kubectl version . The cluster must be configured to use the kube-dns addon. Check with minikube addons list Kubernetes DNS Page Start minikube with the recommended configuration for Spark minikube start --cpus 3 --memory 4096 Submit a Spark job using: $ bin/spark-submit \\ --master k8s://https://<k8s-apiserver-host>:<k8s-apiserver-port> \\ --deploy-mode cluster \\ --name spark-pi \\ --class org.apache.spark.examples.SparkPi \\ --conf spark.executor.instances = 3 \\ --conf spark.kubernetes.container.image = <spark-image> \\ local:///path/to/examples.jar Use kubectl cluster-info to get the K8s API server URL Spark (starting with version 2.3) ships with a Dockerfile in the kubernetes/dockerfiles/ directory. Access logs: $ kubectl -n = <namespace> logs -f <driver-pod-name> Accessing Driver UI: $ kubectl port-forward <driver-pod-name> 4040 :4040 Then go to http://localhost:4040","title":"Steps"},{"location":"Big_Data/Spark_on_Kubernetes/#alternatives","text":"Helm Chart for Spark The same on KubeApps helm install --name my-spark-release --version 0 .1.12 stable/spark","title":"Alternatives"},{"location":"Cloud/AWS/","text":"AWS Services Overview \u00b6 Basic Services \u00b6 Compute: EC2 (autoscaling, ELB load balancing) Networking / Security: VPC (security groups), IAM (users/groups/application roles) Storage \u00b6 S3: secure, scalable object-level storage, static web site hosting... Glacier: long-term storage EBS: block-level storage (for EC2 instances) Databases \u00b6 RDS: relational databases (MySQL, PostgreSQL, MSSQL, MariaDB, Aurora...) DynamoDB: scalable NoSQL database backed by solid-state drives Analytics \u00b6 RedShift: PostgreSQL-based columnstore OLAP database that uses SQL. MPP architecture. EMR: Hadoop cluster (Hive, Pig, HBase, Spark...). ETL / ELT / Batch Processing \u00b6 Glue Batch Data Pipeline: orchestrate data transfers between S3, DynamoDB, Redshift Application Services \u00b6 Notifications: SNS (alerts by email, SMS...), SES (bulk email) Queue: SQS (async message queues for component decoupling) Workflows, State Machine as a Service: AWS Step Functions, SWF (task-oriented workflows - complicated) Document Search: ElasticSearch, CloudSearch Monitoring \u00b6 Cloudwatch (monitor services and instances e.g. CPU utilization, etc...) CloudTrail (monitor API calls) Infrastructure Deployment / Automation \u00b6 Elastic Beanstalk (simple, mostly web or Linux worker) CloudFormation (JSON / YAML templates - more difficult, but many existing templates) OpsWork (higher level than CloudFormation, uses non-native components - Chef-based) Desktop in the Cloud \u00b6 WorkSpaces Details \u00b6 Tools \u00b6 Unix tools on Windows: Cygwin Putty SSH client for Windows doc Download and install PuTTY link . Be sure to install the entire suite. Start PuTTYgen (for example, from the Start menu, click All Programs > PuTTY > PuTTYgen). Under Type of key to generate, select SSH-2 RSA. Load the .pem file (private key) downloaded from the console (in \"credentials\" folder) Save private key AWS command line interface AWS toolkit for Visual Studio AWS tools for PowerShell AWS EC2 \u00b6 Log onto instance with Putty SSH login as: ec2-user (Amazon Linux) or: ubuntu Bash shell documentation Use a shell script to configure the instance link User data: You can specify user data to configure an instance during launch, or to run a configuration script. To attach a file, select the \"As file\" option and browse for the file to attach. AWS S3 \u00b6 GUI tools to upload / manage files: AWS Console S3 Browser CloudBerry Command-line s3 clients: AWS command line (see above) S3 command line tools Redshift \u00b6 1) Use Case Large-scale SQL analytical database Querying in Redshift is FAST Full SQL compared to HiveQL Redshift isn\u2019t a complete replacement for a Hadoop system (no streaming, no text processing) 2) Tools install SQL tool or Aginity Microsoft SSDT 3) Get data into Redshift: COPY from S3 (delimited text files) COPY from DynamoDB (NoSQL datastore) JDBC/ODBC transactions (not efficient for bulk loading) Tables have \u2018keys\u2019 that define how the data is split across slices. The recommended practice is to split based upon commonly-joined columns, so that joined data resides on the same slice, thus avoiding the need to move data between systems. 4) Examples: COPY table1 FROM 's3://bucket1/' credentials 'aws_access_key_id=abc;aws_secret_access_key=xyz' delimiter '|' gzip removequotes truncatecolumns maxerror 1000 SELECT DISTINCT field1 FROM table1 SELECT COUNT ( DISTINCT field2 ) FROM table1 EMR \u00b6 EMR FAQs Extract, Transform, and Load (ETL) Data with Amazon EMR EMR article SWF \u00b6 The Amazon Simple Workflow Service (Amazon SWF) makes it easy to build applications that coordinate work across distributed components. In Amazon SWF, a task represents a logical unit of work that is performed by a component of your application. Coordinating tasks across the application involves managing intertask dependencies, scheduling, and concurrency in accordance with the logical flow of the application. Amazon SWF gives you full control over implementing tasks and coordinating them without worrying about underlying complexities such as tracking their progress and maintaining their state. When using Amazon SWF, you implement workers to perform tasks. These workers can run either on cloud infrastructure, such as Amazon Elastic Compute Cloud (Amazon EC2), or on your own premises. You can create tasks that are long-running, or that may fail, time out, or require restarts\u2014or that may complete with varying throughput and latency. Amazon SWF stores tasks and assigns them to workers when they are ready, tracks their progress, and maintains their state, including details on their completion. To coordinate tasks, you write a program that gets the latest state of each task from Amazon SWF and uses it to initiate subsequent tasks. Amazon SWF maintains an application's execution state durably so that the application is resilient to failures in individual components. With Amazon SWF, you can implement, deploy, scale, and modify these application components independently. Amazon SWF offers capabilities to support a variety of application requirements. It is suitable for a range of use cases that require coordination of tasks, including media processing, web application back-ends, business process workflows, and analytics pipelines.","title":"AWS Services Overview"},{"location":"Cloud/AWS/#aws-services-overview","text":"","title":"AWS Services Overview"},{"location":"Cloud/AWS/#basic-services","text":"Compute: EC2 (autoscaling, ELB load balancing) Networking / Security: VPC (security groups), IAM (users/groups/application roles)","title":"Basic Services"},{"location":"Cloud/AWS/#storage","text":"S3: secure, scalable object-level storage, static web site hosting... Glacier: long-term storage EBS: block-level storage (for EC2 instances)","title":"Storage"},{"location":"Cloud/AWS/#databases","text":"RDS: relational databases (MySQL, PostgreSQL, MSSQL, MariaDB, Aurora...) DynamoDB: scalable NoSQL database backed by solid-state drives","title":"Databases"},{"location":"Cloud/AWS/#analytics","text":"RedShift: PostgreSQL-based columnstore OLAP database that uses SQL. MPP architecture. EMR: Hadoop cluster (Hive, Pig, HBase, Spark...).","title":"Analytics"},{"location":"Cloud/AWS/#etl-elt-batch-processing","text":"Glue Batch Data Pipeline: orchestrate data transfers between S3, DynamoDB, Redshift","title":"ETL / ELT / Batch Processing"},{"location":"Cloud/AWS/#application-services","text":"Notifications: SNS (alerts by email, SMS...), SES (bulk email) Queue: SQS (async message queues for component decoupling) Workflows, State Machine as a Service: AWS Step Functions, SWF (task-oriented workflows - complicated) Document Search: ElasticSearch, CloudSearch","title":"Application Services"},{"location":"Cloud/AWS/#monitoring","text":"Cloudwatch (monitor services and instances e.g. CPU utilization, etc...) CloudTrail (monitor API calls)","title":"Monitoring"},{"location":"Cloud/AWS/#infrastructure-deployment-automation","text":"Elastic Beanstalk (simple, mostly web or Linux worker) CloudFormation (JSON / YAML templates - more difficult, but many existing templates) OpsWork (higher level than CloudFormation, uses non-native components - Chef-based)","title":"Infrastructure Deployment / Automation"},{"location":"Cloud/AWS/#desktop-in-the-cloud","text":"WorkSpaces","title":"Desktop in the Cloud"},{"location":"Cloud/AWS/#details","text":"","title":"Details"},{"location":"Cloud/AWS/#tools","text":"Unix tools on Windows: Cygwin Putty SSH client for Windows doc Download and install PuTTY link . Be sure to install the entire suite. Start PuTTYgen (for example, from the Start menu, click All Programs > PuTTY > PuTTYgen). Under Type of key to generate, select SSH-2 RSA. Load the .pem file (private key) downloaded from the console (in \"credentials\" folder) Save private key AWS command line interface AWS toolkit for Visual Studio AWS tools for PowerShell","title":"Tools"},{"location":"Cloud/AWS/#aws-ec2","text":"Log onto instance with Putty SSH login as: ec2-user (Amazon Linux) or: ubuntu Bash shell documentation Use a shell script to configure the instance link User data: You can specify user data to configure an instance during launch, or to run a configuration script. To attach a file, select the \"As file\" option and browse for the file to attach.","title":"AWS EC2"},{"location":"Cloud/AWS/#aws-s3","text":"GUI tools to upload / manage files: AWS Console S3 Browser CloudBerry Command-line s3 clients: AWS command line (see above) S3 command line tools","title":"AWS S3"},{"location":"Cloud/AWS/#redshift","text":"1) Use Case Large-scale SQL analytical database Querying in Redshift is FAST Full SQL compared to HiveQL Redshift isn\u2019t a complete replacement for a Hadoop system (no streaming, no text processing) 2) Tools install SQL tool or Aginity Microsoft SSDT 3) Get data into Redshift: COPY from S3 (delimited text files) COPY from DynamoDB (NoSQL datastore) JDBC/ODBC transactions (not efficient for bulk loading) Tables have \u2018keys\u2019 that define how the data is split across slices. The recommended practice is to split based upon commonly-joined columns, so that joined data resides on the same slice, thus avoiding the need to move data between systems. 4) Examples: COPY table1 FROM 's3://bucket1/' credentials 'aws_access_key_id=abc;aws_secret_access_key=xyz' delimiter '|' gzip removequotes truncatecolumns maxerror 1000 SELECT DISTINCT field1 FROM table1 SELECT COUNT ( DISTINCT field2 ) FROM table1","title":"Redshift"},{"location":"Cloud/AWS/#emr","text":"EMR FAQs Extract, Transform, and Load (ETL) Data with Amazon EMR EMR article","title":"EMR"},{"location":"Cloud/AWS/#swf","text":"The Amazon Simple Workflow Service (Amazon SWF) makes it easy to build applications that coordinate work across distributed components. In Amazon SWF, a task represents a logical unit of work that is performed by a component of your application. Coordinating tasks across the application involves managing intertask dependencies, scheduling, and concurrency in accordance with the logical flow of the application. Amazon SWF gives you full control over implementing tasks and coordinating them without worrying about underlying complexities such as tracking their progress and maintaining their state. When using Amazon SWF, you implement workers to perform tasks. These workers can run either on cloud infrastructure, such as Amazon Elastic Compute Cloud (Amazon EC2), or on your own premises. You can create tasks that are long-running, or that may fail, time out, or require restarts\u2014or that may complete with varying throughput and latency. Amazon SWF stores tasks and assigns them to workers when they are ready, tracks their progress, and maintains their state, including details on their completion. To coordinate tasks, you write a program that gets the latest state of each task from Amazon SWF and uses it to initiate subsequent tasks. Amazon SWF maintains an application's execution state durably so that the application is resilient to failures in individual components. With Amazon SWF, you can implement, deploy, scale, and modify these application components independently. Amazon SWF offers capabilities to support a variety of application requirements. It is suitable for a range of use cases that require coordination of tasks, including media processing, web application back-ends, business process workflows, and analytics pipelines.","title":"SWF"},{"location":"Cloud/AWS_Lambda/","text":"Useful Links \u00b6 Building a Dynamic DNS for Route 53 using CloudWatch Events and Lambda Lambkin - CLI tool for generating and managing simple functions in AWS Lambda","title":"AWS Lambda"},{"location":"Cloud/AWS_Lambda/#useful-links","text":"Building a Dynamic DNS for Route 53 using CloudWatch Events and Lambda Lambkin - CLI tool for generating and managing simple functions in AWS Lambda","title":"Useful Links"},{"location":"Cloud/Serverless/","text":"Serverless Cheatsheet \u00b6 Serverless home page Install \u00b6 npm install -g serverless Examples \u00b6 Serverless Examples Serverless Starter Python example C# example Cheatsheet \u00b6 Create a Service: # NodeJS serverless create -p [ SERVICE NAME ] -t aws-nodejs # C# serverless create --path serverlessCSharp --template aws-csharp Install a Service This is a convenience method to install a pre-made Serverless Service locally by downloading the Github repo and unzipping it. serverless install -u [ GITHUB URL OF SERVICE ] Deploy All Use this when you have made changes to your Functions, Events or Resources in serverless.yml or you simply want to deploy all changes within your Service at the same time. serverless deploy -s [ STAGE NAME ] -r [ REGION NAME ] -v Deploy Function Use this to quickly overwrite your AWS Lambda code on AWS, allowing you to develop faster. serverless deploy function -f [ FUNCTION NAME ] -s [ STAGE NAME ] -r [ REGION NAME ] Invoke Function Invokes an AWS Lambda Function on AWS and returns logs. serverless invoke -f [ FUNCTION NAME ] -s [ STAGE NAME ] -r [ REGION NAME ] -l Streaming Logs Open up a separate tab in your console and stream all logs for a specific Function using this command. serverless logs -f [ FUNCTION NAME ] -s [ STAGE NAME ] -r [ REGION NAME ]","title":"Serverless Cheatsheet"},{"location":"Cloud/Serverless/#serverless-cheatsheet","text":"Serverless home page","title":"Serverless Cheatsheet"},{"location":"Cloud/Serverless/#install","text":"npm install -g serverless","title":"Install"},{"location":"Cloud/Serverless/#examples","text":"Serverless Examples Serverless Starter Python example C# example","title":"Examples"},{"location":"Cloud/Serverless/#cheatsheet","text":"Create a Service: # NodeJS serverless create -p [ SERVICE NAME ] -t aws-nodejs # C# serverless create --path serverlessCSharp --template aws-csharp Install a Service This is a convenience method to install a pre-made Serverless Service locally by downloading the Github repo and unzipping it. serverless install -u [ GITHUB URL OF SERVICE ] Deploy All Use this when you have made changes to your Functions, Events or Resources in serverless.yml or you simply want to deploy all changes within your Service at the same time. serverless deploy -s [ STAGE NAME ] -r [ REGION NAME ] -v Deploy Function Use this to quickly overwrite your AWS Lambda code on AWS, allowing you to develop faster. serverless deploy function -f [ FUNCTION NAME ] -s [ STAGE NAME ] -r [ REGION NAME ] Invoke Function Invokes an AWS Lambda Function on AWS and returns logs. serverless invoke -f [ FUNCTION NAME ] -s [ STAGE NAME ] -r [ REGION NAME ] -l Streaming Logs Open up a separate tab in your console and stream all logs for a specific Function using this command. serverless logs -f [ FUNCTION NAME ] -s [ STAGE NAME ] -r [ REGION NAME ]","title":"Cheatsheet"},{"location":"Containers/Debug_Kubernetes/","text":"Run a test Docker container \u00b6 docker run --rm -p <port>:<port> <docker image>:<tag> docker ps # cleanup docker kill <container> To override the entrypoint, use: winpty docker run --rm -p <port>:<port> -it --entrypoint bash <docker image>:<tag> The above assumes you are using cygwin / git bash on Windows. Run a test K8s pod \u00b6 kubectl run <deployment name> --image = <docker image>:<tag> Useful options: - --restart=Never - if the pod has a console: -i --tty --command -- bash Attach to the (first) container in the Pod: kubectl attach <pod name> -i -t If there are multiple containers in the pod, use: -c <container name> Get a shell to a running Container: kubectl exec -it <pod name> -- bash Delete test pod: kubectl delete pod <pod name>","title":"Debug Kubernetes"},{"location":"Containers/Debug_Kubernetes/#run-a-test-docker-container","text":"docker run --rm -p <port>:<port> <docker image>:<tag> docker ps # cleanup docker kill <container> To override the entrypoint, use: winpty docker run --rm -p <port>:<port> -it --entrypoint bash <docker image>:<tag> The above assumes you are using cygwin / git bash on Windows.","title":"Run a test Docker container"},{"location":"Containers/Debug_Kubernetes/#run-a-test-k8s-pod","text":"kubectl run <deployment name> --image = <docker image>:<tag> Useful options: - --restart=Never - if the pod has a console: -i --tty --command -- bash Attach to the (first) container in the Pod: kubectl attach <pod name> -i -t If there are multiple containers in the pod, use: -c <container name> Get a shell to a running Container: kubectl exec -it <pod name> -- bash Delete test pod: kubectl delete pod <pod name>","title":"Run a test K8s pod"},{"location":"Containers/Deploy_to_Kubernetes%20%20%28Helm%29/","text":"Useful Links \u00b6 Deploying scala sbt microservice to Kubernetes SBT Native Packager - Docker OpenSSL Helm Deployment of a sbt -built app on Kubernetes (MiniKube) \u00b6 Test packaging without Kubernetes first \u00b6 Stage all Play files in a local directory and verify sbt stage For direct deployment, create a distribution in target/universal sbt dist The dist task builds a binary version of your application that you can deploy to a server without any dependency on SBT, the only thing the server needs is a Java installation. Deploy a Helm chart to Kubernetes \u00b6 Prerequisites: minikube , kubectl , docker client and helm should be installed Generate the Dockerfile and environment prepared for creating a Docker image sbt docker:stage Verify the output under target/docker Start minikube minikube start Enable Ingress minikube addons list minikube addons enable ingress Also consider enabling heapster List available nodes to verify that kubectl is properly configured kubectl get nodes It should return one node. Connect the Docker client to the Docker daemon in the K8s VM eval $( minikube docker-env ) Just make sure you tag your Docker image with something other than \u2018latest\u2019 and use that tag while you pull the image. Otherwise, if you do not specify version of your image, it will be assumed as :latest , with pull image policy of Always correspondingly, which may eventually result in ErrImagePull as you may not have any versions of your Docker image out there in the default docker registry (usually DockerHub) yet. If needed, remove previously built images from the local Docker server with sbt docker:clean or docker rmi <image> . To view the list of Docker images, run docker images Build the Docker image and publish it to Kubernetes' Docker server. sbt docker:publishLocal Deploy the Helm chart ./helm install --dry-run --debug <helm chart folder> & > output.txt and if that looks OK ./helm install <helm chart folder> or specify a release name: ./helm install --name <release name> <helm chart folder> Verify the Helm deployment to minikube ./helm list ./helm status <release name> More details via: kubectl get ing kubectl get service kubectl get deployment kubectl get pods Test the deployment by forwarding a local port to a port on the pod kubectl get pods kubectl port-forward <pod name> 8080 :<target port on pod> curl -v http://localhost:8080/api kubectl port-forward also allows using resource name, such as a service name, to select a matching pod to port forward to kubectl port-forward svc/<service name> 8080 :<service port> curl -v http://localhost:8080/ When needed, delete the release with helm ls helm delete <release name> If you want to deploy / not deploy an Ingress \u00b6 Update values.yaml in the Helm chart root folder ingress : enabled : true # or: false; true by default If true, make sure the minikube Ingress add-on is enabled minikube addons enable ingress Deploy on Kubernetes as above See Blog SSL Termination (TO DO) \u00b6 IBM Ingress TLS tutorial Generate a x509, pem encoded, RSA 2048 certificate with OpenSSL openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout ${ KEY_FILE } -out ${ CERT_FILE } -subj \"/CN= ${ HOST } /O= ${ HOST } \" openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout tls.key -out tls.crt -subj \"/CN=john-cd.com\" Note: To find myhost.com for minikube, run the following commands: $ minikube ssh $ echo $HOSTNAME minikube Create a Kubernetes secret kubectl create secret tls ${ CERT_NAME } --key ${ KEY_FILE } --cert ${ CERT_FILE } kubectl create secret tls my-secret --key tls.key --cert tls.crt Add under spec: in tls : - hosts : - myhost.com secretName : my-secret Find and delete all nginx pods to force the nginx.conf to update and reflect the ingress changes. Find the ingress pods with the following: kubectl get pods --all-namespaces kubectl delete pods --namespace = kube-system [ ingress pod ]","title":"Deploy to Kubernetes (Helm)"},{"location":"Containers/Deploy_to_Kubernetes%20%20%28Helm%29/#useful-links","text":"Deploying scala sbt microservice to Kubernetes SBT Native Packager - Docker OpenSSL Helm","title":"Useful Links"},{"location":"Containers/Deploy_to_Kubernetes%20%20%28Helm%29/#deployment-of-a-sbt-built-app-on-kubernetes-minikube","text":"","title":"Deployment of a sbt-built app on Kubernetes (MiniKube)"},{"location":"Containers/Deploy_to_Kubernetes%20%20%28Helm%29/#test-packaging-without-kubernetes-first","text":"Stage all Play files in a local directory and verify sbt stage For direct deployment, create a distribution in target/universal sbt dist The dist task builds a binary version of your application that you can deploy to a server without any dependency on SBT, the only thing the server needs is a Java installation.","title":"Test packaging without Kubernetes first"},{"location":"Containers/Deploy_to_Kubernetes%20%20%28Helm%29/#deploy-a-helm-chart-to-kubernetes","text":"Prerequisites: minikube , kubectl , docker client and helm should be installed Generate the Dockerfile and environment prepared for creating a Docker image sbt docker:stage Verify the output under target/docker Start minikube minikube start Enable Ingress minikube addons list minikube addons enable ingress Also consider enabling heapster List available nodes to verify that kubectl is properly configured kubectl get nodes It should return one node. Connect the Docker client to the Docker daemon in the K8s VM eval $( minikube docker-env ) Just make sure you tag your Docker image with something other than \u2018latest\u2019 and use that tag while you pull the image. Otherwise, if you do not specify version of your image, it will be assumed as :latest , with pull image policy of Always correspondingly, which may eventually result in ErrImagePull as you may not have any versions of your Docker image out there in the default docker registry (usually DockerHub) yet. If needed, remove previously built images from the local Docker server with sbt docker:clean or docker rmi <image> . To view the list of Docker images, run docker images Build the Docker image and publish it to Kubernetes' Docker server. sbt docker:publishLocal Deploy the Helm chart ./helm install --dry-run --debug <helm chart folder> & > output.txt and if that looks OK ./helm install <helm chart folder> or specify a release name: ./helm install --name <release name> <helm chart folder> Verify the Helm deployment to minikube ./helm list ./helm status <release name> More details via: kubectl get ing kubectl get service kubectl get deployment kubectl get pods Test the deployment by forwarding a local port to a port on the pod kubectl get pods kubectl port-forward <pod name> 8080 :<target port on pod> curl -v http://localhost:8080/api kubectl port-forward also allows using resource name, such as a service name, to select a matching pod to port forward to kubectl port-forward svc/<service name> 8080 :<service port> curl -v http://localhost:8080/ When needed, delete the release with helm ls helm delete <release name>","title":"Deploy a Helm chart to Kubernetes"},{"location":"Containers/Deploy_to_Kubernetes%20%20%28Helm%29/#if-you-want-to-deploy-not-deploy-an-ingress","text":"Update values.yaml in the Helm chart root folder ingress : enabled : true # or: false; true by default If true, make sure the minikube Ingress add-on is enabled minikube addons enable ingress Deploy on Kubernetes as above See Blog","title":"If you want to deploy / not deploy an Ingress"},{"location":"Containers/Deploy_to_Kubernetes%20%20%28Helm%29/#ssl-termination-to-do","text":"IBM Ingress TLS tutorial Generate a x509, pem encoded, RSA 2048 certificate with OpenSSL openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout ${ KEY_FILE } -out ${ CERT_FILE } -subj \"/CN= ${ HOST } /O= ${ HOST } \" openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout tls.key -out tls.crt -subj \"/CN=john-cd.com\" Note: To find myhost.com for minikube, run the following commands: $ minikube ssh $ echo $HOSTNAME minikube Create a Kubernetes secret kubectl create secret tls ${ CERT_NAME } --key ${ KEY_FILE } --cert ${ CERT_FILE } kubectl create secret tls my-secret --key tls.key --cert tls.crt Add under spec: in tls : - hosts : - myhost.com secretName : my-secret Find and delete all nginx pods to force the nginx.conf to update and reflect the ingress changes. Find the ingress pods with the following: kubectl get pods --all-namespaces kubectl delete pods --namespace = kube-system [ ingress pod ]","title":"SSL Termination (TO DO)"},{"location":"Containers/Deploy_to_Kubernetes/","text":"using kubectl Create ConfigMap from config files \u00b6 A ConfigMap stores K8s-specific configuration that can be mounted as volume or used in env variables. It is often used to provide production configuration: application configuration, log settings, etc... kubectl create configmap app-conf --from-file = <path to config files> # create a ConfigMap from multiple files in the same directory. Check the ConfigMap kubectl get configmap app-conf -o yaml # or kubectl describe configmaps app-conf Create a Kubernetes secret \u00b6 You may need a Secret to store database passwords and secret keys. For applications using the Play Framework , generate a secret using: secretText = $( sbt playGenerateSecret ) regex = \"Generated new secret: (.+) $ \" f [[ $secretText = ~ $regex ]] then secret = \" ${ BASH_REMATCH [1] } \" echo $secret kubectl create secret generic application-secret --from-literal = application_secret = $secret kubectl get secrets else echo \" $secretText doesn't match\" > & 2 fi done Create Resources (Deployment, Service, Ingress, etc...) \u00b6 kubectl create -f <path/to/resource_config.yaml> Verify proper resource creation kubectl get ing kubectl get service kubectl get deployment kubectl get pods To delete a resource later, in this case a Deployment: kubectl delete deployment <deployment name> Install an Ingress Controller \u00b6 An ingress controller is necessary to make Ingresses work. See the Ingress doc Minikube \u00b6 minikube provides its own ingress controller via the Ingress add-on: minikube addons enable ingress Enabling the add-on provisions the following: a configMap for the Nginx loadbalancer the Nginx ingress controller a service that exposes a default Nginx backend pod for handling unmapped requests. Install the nginx ingress controller (non-minikube Kubernetes) \u00b6 Install via this helm chart #./helm install stable/nginx-ingress or with stat collection enabled for Prometheus helm install --name nginx-ingress-release stable/nginx-ingress \\ --set controller.stats.enabled = true \\ --set controller.metrics.enabled = true Verify that the Ingress exists kubectl get ing See explanations and documentation The nginx ingress controller requires a 404-server like this Alternative ingress controllers \u00b6 Install https://traefik.io/","title":"Deploy to Kubernetes"},{"location":"Containers/Deploy_to_Kubernetes/#create-configmap-from-config-files","text":"A ConfigMap stores K8s-specific configuration that can be mounted as volume or used in env variables. It is often used to provide production configuration: application configuration, log settings, etc... kubectl create configmap app-conf --from-file = <path to config files> # create a ConfigMap from multiple files in the same directory. Check the ConfigMap kubectl get configmap app-conf -o yaml # or kubectl describe configmaps app-conf","title":"Create ConfigMap from config files"},{"location":"Containers/Deploy_to_Kubernetes/#create-a-kubernetes-secret","text":"You may need a Secret to store database passwords and secret keys. For applications using the Play Framework , generate a secret using: secretText = $( sbt playGenerateSecret ) regex = \"Generated new secret: (.+) $ \" f [[ $secretText = ~ $regex ]] then secret = \" ${ BASH_REMATCH [1] } \" echo $secret kubectl create secret generic application-secret --from-literal = application_secret = $secret kubectl get secrets else echo \" $secretText doesn't match\" > & 2 fi done","title":"Create a Kubernetes secret"},{"location":"Containers/Deploy_to_Kubernetes/#create-resources-deployment-service-ingress-etc","text":"kubectl create -f <path/to/resource_config.yaml> Verify proper resource creation kubectl get ing kubectl get service kubectl get deployment kubectl get pods To delete a resource later, in this case a Deployment: kubectl delete deployment <deployment name>","title":"Create Resources (Deployment, Service, Ingress, etc...)"},{"location":"Containers/Deploy_to_Kubernetes/#install-an-ingress-controller","text":"An ingress controller is necessary to make Ingresses work. See the Ingress doc","title":"Install an Ingress Controller"},{"location":"Containers/Deploy_to_Kubernetes/#minikube","text":"minikube provides its own ingress controller via the Ingress add-on: minikube addons enable ingress Enabling the add-on provisions the following: a configMap for the Nginx loadbalancer the Nginx ingress controller a service that exposes a default Nginx backend pod for handling unmapped requests.","title":"Minikube"},{"location":"Containers/Deploy_to_Kubernetes/#install-the-nginx-ingress-controller-non-minikube-kubernetes","text":"Install via this helm chart #./helm install stable/nginx-ingress or with stat collection enabled for Prometheus helm install --name nginx-ingress-release stable/nginx-ingress \\ --set controller.stats.enabled = true \\ --set controller.metrics.enabled = true Verify that the Ingress exists kubectl get ing See explanations and documentation The nginx ingress controller requires a 404-server like this","title":"Install the nginx ingress controller (non-minikube Kubernetes)"},{"location":"Containers/Deploy_to_Kubernetes/#alternative-ingress-controllers","text":"Install https://traefik.io/","title":"Alternative ingress controllers"},{"location":"Containers/Docker/","text":"Docker Cheatsheet \u00b6 Useful Links \u00b6 Docker Cheat Sheet Docker Documentation Docker Tutorials and Labs Docker + Jenkins Docker Hub Concepts \u00b6 A Docker image is a read-only template. For example, an image could contain an Ubuntu operating system with Apache and your web application installed. Images are used to create Docker containers. Docker provides a simple way to build new images or update existing images, or you can download Docker images that other people have already created. Docker images are the buildcomponent of Docker. Docker registries hold images. Cheatsheet \u00b6 To show only running containers use: $ docker ps To show all containers use: $ docker ps -a Show last started container: $ docker ps -l Download an image: $ docker pull centos Create then start a container: docker run [OPTIONS] IMAGE [COMMAND] [ARG...] * Docker run reference $ docker run hello-world Run with interactive terminal (i = interactive t = terminal): $ docker run -it ubuntu bash Start then detach the container (daemonize): $ docker run -d -p8088:80 --name webserver nginx If you want a transient container, docker run --rm will remove the container after it stops. Looks inside the container (use -f to act like tail -f ): $ docker logs <container name> Stop container: $ docker stop <container name> # container ID or name Delete container: $ docker rm <container name> To check the environment: $ docker run -it alpine env Docker version / info: $ docker version $ docker info Port Mapping \u00b6 -p 80:5000 would map port 80 on our local host to port 5000 inside our container. $ docker run -d -p 80 :5000 training/webapp python app.py Full format: ip:hostPort:containerPort | ip::containerPort | hostPort:containerPort | containerPort $ docker run -d -p 127 .0.0.1:80:5000 training/webapp python app.py Both hostPort and containerPort can be specified as a range of ports. When specifying ranges for both, the number of container ports in the range must match the number of host ports in the range, for example: -p1234-1236:1234-1236/tcp The -P flag tells Docker to map any required network ports inside our container to our host (using random ports). $ docker run -d -P training/webapp python app.py Linking \u00b6 --link <name or id>:alias where name is the name of the container we\u2019re linking to and alias is an alias for the link name. The --link flag also takes the form: --link <name or id> $ docker run -d --name myES -p 9200 :9200 -p 9300 :9300 elasticsearch $ docker run --name myK --link myES:elasticsearch -p 5601 :5601 -d docker-kibana-sense Networks \u00b6 $ docker network ls Find out the container\u2019s IP address: $ docker network inspect bridge Data Volumes \u00b6 Create a new volume inside a container at /webapp: $ docker run -d -P --name web -v /webapp training/webapp python app.py $ docker inspect web You can also use the VOLUME instruction in a Dockerfile to add one or more new volumes to any container created from that image. Mount the host directory, /src/webapp , into the container at /opt/webapp . $ docker run -d -P --name web -v /src/webapp:/opt/webapp training/webapp python app.py On Windows, use: docker run -v /c/Users/<path>:/<container path> ... Example Dockerfile \u00b6 $ vim Dockerfile FROM docker/whalesay:latest RUN apt-get -y update && apt-get install -y fortunes CMD /usr/games/fortune -a | cowsay $ docker build -t docker-whale .","title":"Docker Cheatsheet"},{"location":"Containers/Docker/#docker-cheatsheet","text":"","title":"Docker Cheatsheet"},{"location":"Containers/Docker/#useful-links","text":"Docker Cheat Sheet Docker Documentation Docker Tutorials and Labs Docker + Jenkins Docker Hub","title":"Useful Links"},{"location":"Containers/Docker/#concepts","text":"A Docker image is a read-only template. For example, an image could contain an Ubuntu operating system with Apache and your web application installed. Images are used to create Docker containers. Docker provides a simple way to build new images or update existing images, or you can download Docker images that other people have already created. Docker images are the buildcomponent of Docker. Docker registries hold images.","title":"Concepts"},{"location":"Containers/Docker/#cheatsheet","text":"To show only running containers use: $ docker ps To show all containers use: $ docker ps -a Show last started container: $ docker ps -l Download an image: $ docker pull centos Create then start a container: docker run [OPTIONS] IMAGE [COMMAND] [ARG...] * Docker run reference $ docker run hello-world Run with interactive terminal (i = interactive t = terminal): $ docker run -it ubuntu bash Start then detach the container (daemonize): $ docker run -d -p8088:80 --name webserver nginx If you want a transient container, docker run --rm will remove the container after it stops. Looks inside the container (use -f to act like tail -f ): $ docker logs <container name> Stop container: $ docker stop <container name> # container ID or name Delete container: $ docker rm <container name> To check the environment: $ docker run -it alpine env Docker version / info: $ docker version $ docker info","title":"Cheatsheet"},{"location":"Containers/Docker/#port-mapping","text":"-p 80:5000 would map port 80 on our local host to port 5000 inside our container. $ docker run -d -p 80 :5000 training/webapp python app.py Full format: ip:hostPort:containerPort | ip::containerPort | hostPort:containerPort | containerPort $ docker run -d -p 127 .0.0.1:80:5000 training/webapp python app.py Both hostPort and containerPort can be specified as a range of ports. When specifying ranges for both, the number of container ports in the range must match the number of host ports in the range, for example: -p1234-1236:1234-1236/tcp The -P flag tells Docker to map any required network ports inside our container to our host (using random ports). $ docker run -d -P training/webapp python app.py","title":"Port Mapping"},{"location":"Containers/Docker/#linking","text":"--link <name or id>:alias where name is the name of the container we\u2019re linking to and alias is an alias for the link name. The --link flag also takes the form: --link <name or id> $ docker run -d --name myES -p 9200 :9200 -p 9300 :9300 elasticsearch $ docker run --name myK --link myES:elasticsearch -p 5601 :5601 -d docker-kibana-sense","title":"Linking"},{"location":"Containers/Docker/#networks","text":"$ docker network ls Find out the container\u2019s IP address: $ docker network inspect bridge","title":"Networks"},{"location":"Containers/Docker/#data-volumes","text":"Create a new volume inside a container at /webapp: $ docker run -d -P --name web -v /webapp training/webapp python app.py $ docker inspect web You can also use the VOLUME instruction in a Dockerfile to add one or more new volumes to any container created from that image. Mount the host directory, /src/webapp , into the container at /opt/webapp . $ docker run -d -P --name web -v /src/webapp:/opt/webapp training/webapp python app.py On Windows, use: docker run -v /c/Users/<path>:/<container path> ...","title":"Data Volumes"},{"location":"Containers/Docker/#example-dockerfile","text":"$ vim Dockerfile FROM docker/whalesay:latest RUN apt-get -y update && apt-get install -y fortunes CMD /usr/games/fortune -a | cowsay $ docker build -t docker-whale .","title":"Example Dockerfile"},{"location":"Containers/Helm_Chart_Creation/","text":"Links \u00b6 How to create your first Helm chart Steps \u00b6 ./helm create <folder containing chart> ./helm lint <folder> ./helm install --dry-run --debug <folder> To create dependencies between charts \u00b6 Create requirements.yaml Add a remote repo ./helm repo add stable https://kubernetes-charts.storage.googleapis.com and, from the chart directory, run: ../helm dependency update","title":"Helm Chart Creation"},{"location":"Containers/Helm_Chart_Creation/#links","text":"How to create your first Helm chart","title":"Links"},{"location":"Containers/Helm_Chart_Creation/#steps","text":"./helm create <folder containing chart> ./helm lint <folder> ./helm install --dry-run --debug <folder>","title":"Steps"},{"location":"Containers/Helm_Chart_Creation/#to-create-dependencies-between-charts","text":"Create requirements.yaml Add a remote repo ./helm repo add stable https://kubernetes-charts.storage.googleapis.com and, from the chart directory, run: ../helm dependency update","title":"To create dependencies between charts"},{"location":"Containers/Kubernetes_Cheatsheet/","text":"minikube \u00b6 To access the Kubernetes Dashboard, run this command in a shell after starting Minikube to get the address: minikube dashboard The minikube VM is exposed to the host system via a host-only IP address, that can be obtained with the minikube ip command kubectl \u00b6 kubectl Cheat Sheet Run a particular image on the cluster (creates a deployment automatically) kubectl run hello-world --replicas = 2 --labels = \"run=load-balancer-example\" --image = gcr.io/google-samples/node-hello:1.0 --port = 8080 kubectl get - list resources. kubectl get deployment to get all deployments kubectl get pods -l app=nginx to get pods with label \"app: nginx\" kubectl describe - show detailed information about a resource kubectl logs - print the logs from a container in a pod kubectl exec - execute a command on a container in a pod Using the Docker daemon in the Minikube VM \u00b6 When using a single VM of Kubernetes, it\u2019s really handy to reuse the minikube\u2019s built-in Docker daemon eval $( minikube docker-env ) # test with docker ps Just make sure you tag your Docker image with something other than \u2018latest\u2019 and use that tag while you pull the image. Otherwise, if you do not specify version of your image, it will be assumed as :latest , with pull image policy of Always correspondingly, which may eventually result in ErrImagePull as you may not have any versions of your Docker image out there in the default docker registry (usually DockerHub) yet. A Docker client is required to publish built docker images to the Docker daemon running inside of minikube. See installing Docker for instructions for your platform.","title":"Kubernetes Cheatsheet"},{"location":"Containers/Kubernetes_Cheatsheet/#minikube","text":"To access the Kubernetes Dashboard, run this command in a shell after starting Minikube to get the address: minikube dashboard The minikube VM is exposed to the host system via a host-only IP address, that can be obtained with the minikube ip command","title":"minikube"},{"location":"Containers/Kubernetes_Cheatsheet/#kubectl","text":"kubectl Cheat Sheet Run a particular image on the cluster (creates a deployment automatically) kubectl run hello-world --replicas = 2 --labels = \"run=load-balancer-example\" --image = gcr.io/google-samples/node-hello:1.0 --port = 8080 kubectl get - list resources. kubectl get deployment to get all deployments kubectl get pods -l app=nginx to get pods with label \"app: nginx\" kubectl describe - show detailed information about a resource kubectl logs - print the logs from a container in a pod kubectl exec - execute a command on a container in a pod","title":"kubectl"},{"location":"Containers/Kubernetes_Cheatsheet/#using-the-docker-daemon-in-the-minikube-vm","text":"When using a single VM of Kubernetes, it\u2019s really handy to reuse the minikube\u2019s built-in Docker daemon eval $( minikube docker-env ) # test with docker ps Just make sure you tag your Docker image with something other than \u2018latest\u2019 and use that tag while you pull the image. Otherwise, if you do not specify version of your image, it will be assumed as :latest , with pull image policy of Always correspondingly, which may eventually result in ErrImagePull as you may not have any versions of your Docker image out there in the default docker registry (usually DockerHub) yet. A Docker client is required to publish built docker images to the Docker daemon running inside of minikube. See installing Docker for instructions for your platform.","title":"Using the Docker daemon in the Minikube VM"},{"location":"Containers/Kubernetes_Concepts/","text":"Pods - A Pod is a Kubernetes abstraction that represents a group of one or more application containers (such as Docker or rkt), and some shared resources for those containers. Kubernetes workloads, such as Deployments and Jobs, are composed of one or more Pods. Shared pod resources include: Shared storage, as Volumes Networking, as a unique cluster IP address Information about how to run each container, such as the container image version or specific ports to use Nodes: A Pod always runs on a Node. A Node is a worker machine in Kubernetes and may be either a virtual or a physical machine, depending on the cluster. Each Node is managed by the Master. A Node can have multiple pods, and the Kubernetes master automatically handles scheduling the pods across the Nodes in the cluster. Deployment - The most common way of running X copies (Pods) of your application. Supports rolling updates to your container images. Service - By itself, a Deployment can\u2019t receive traffic. Setting up a Service is one of the simplest ways to configure a Deployment to receive and loadbalance requests. Depending on the type of Service used, these requests can come from external client apps or be limited to apps within the same cluster. A Service is tied to a specific Deployment using label selection. Labels - Identifying metadata that you can use to sort and select sets of API objects. Labels have many applications, including the following: To keep the right number of replicas (Pods) running in a Deployment. The specified label is used to stamp the Deployment\u2019s newly created Pods (as the value of the spec.template.labels configuration field), and to query which Pods it already manages (as the value of spec.selector.matchLabels ). To tie a Service to a Deployment using the selector field. To look for specific subset of Kubernetes objects, when you are using kubectl. For instance, the command kubectl get deployments --selector=app=nginx only displays Deployments from the nginx app.","title":"Kubernetes Concepts"},{"location":"Containers/Kubernetes_Examples/","text":"Service + Deployment example \u00b6 apiVersion : v1 kind : Service metadata : name : p2p-robot-service spec : selector : app : p2p-robot ports : - name : http protocol : TCP port : 80 targetPort : http # can a text label (port name) or port number --- apiVersion : apps/v1 kind : Deployment metadata : name : p2p-robot-deployment spec : selector : matchLabels : app : p2p-robot replicas : 2 # tells deployment to run 2 pods matching the template template : # create pods using pod definition in this template metadata : # the name is not included in the meta data as a unique name is # generated from the deployment name labels : app : p2p-robot # label used above in matchLabels spec : containers : - name : p2p-robot image : \"johncd/p2p-robot:1.0.0\" imagePullPolicy : IfNotPresent ports : - containerPort : 9000 name : http env : - name : APPLICATION_SECRET # Place the application secret in an environment variable, which is read in application.conf valueFrom : secretKeyRef : name : application-secret key : application_secret volumeMounts : - name : conf-volume mountPath : /usr/local/etc volumes : - name : conf-volume configMap : # The configMap resource provides a way to inject configuration data into Pods. name : app-conf Ingress example \u00b6 # Ingress # https://kubernetes.io/docs/concepts/services-networking/ingress/ apiVersion : extensions/v1beta1 kind : Ingress metadata : name : test annotations : ingress.kubernetes.io/rewrite-target : / kubernetes.io/ingress.class : nginx # Use the nginx-ingress Ingress controller spec : tls : - secretName : ingresssecret # Referencing this secret in an Ingress will tell the Ingress controller to secure the channel from the client to the loadbalancer using TLS rules : - http : paths : - path : /api backend : serviceName : s1 servicePort : 80 --- # Secure the Ingress by specifying a secret that contains a TLS private key and certificate. apiVersion : v1 data : tls.crt : base64 encoded cert tls.key : base64 encoded key kind : Secret metadata : name : ingresssecret namespace : default type : Opaque","title":"Kubernetes Examples"},{"location":"Containers/Kubernetes_Examples/#service-deployment-example","text":"apiVersion : v1 kind : Service metadata : name : p2p-robot-service spec : selector : app : p2p-robot ports : - name : http protocol : TCP port : 80 targetPort : http # can a text label (port name) or port number --- apiVersion : apps/v1 kind : Deployment metadata : name : p2p-robot-deployment spec : selector : matchLabels : app : p2p-robot replicas : 2 # tells deployment to run 2 pods matching the template template : # create pods using pod definition in this template metadata : # the name is not included in the meta data as a unique name is # generated from the deployment name labels : app : p2p-robot # label used above in matchLabels spec : containers : - name : p2p-robot image : \"johncd/p2p-robot:1.0.0\" imagePullPolicy : IfNotPresent ports : - containerPort : 9000 name : http env : - name : APPLICATION_SECRET # Place the application secret in an environment variable, which is read in application.conf valueFrom : secretKeyRef : name : application-secret key : application_secret volumeMounts : - name : conf-volume mountPath : /usr/local/etc volumes : - name : conf-volume configMap : # The configMap resource provides a way to inject configuration data into Pods. name : app-conf","title":"Service + Deployment example"},{"location":"Containers/Kubernetes_Examples/#ingress-example","text":"# Ingress # https://kubernetes.io/docs/concepts/services-networking/ingress/ apiVersion : extensions/v1beta1 kind : Ingress metadata : name : test annotations : ingress.kubernetes.io/rewrite-target : / kubernetes.io/ingress.class : nginx # Use the nginx-ingress Ingress controller spec : tls : - secretName : ingresssecret # Referencing this secret in an Ingress will tell the Ingress controller to secure the channel from the client to the loadbalancer using TLS rules : - http : paths : - path : /api backend : serviceName : s1 servicePort : 80 --- # Secure the Ingress by specifying a secret that contains a TLS private key and certificate. apiVersion : v1 data : tls.crt : base64 encoded cert tls.key : base64 encoded key kind : Secret metadata : name : ingresssecret namespace : default type : Opaque","title":"Ingress example"},{"location":"Containers/Minikube_Install_in_Ubuntu_on_Windows/","text":"Install kubectl in Ubuntu on Windows \u00b6 cd ~ mkdir bin curl -LO https://storage.googleapis.com/kubernetes-release/release/ $( curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt ) /bin/linux/amd64/kubectl chmod +x ./kubectl # optionally sudo mv ./kubectl /usr/local/bin/kubectl # then test kubectl get all # enable autocompletion source < ( kubectl completion bash ) echo \"source <(kubectl completion bash)\" >> ~/.bashrc If necessary, install socat sudo apt-get update && sudo apt-get install socat In order for kubectl to find and access a Kubernetes cluster, it needs a kubeconfig file, which is created automatically when you create a cluster using kube-up.sh or successfully deploy a Minikube cluster. Check that kubectl is properly configured by getting the cluster state: kubectl cluster-info Beware that you may have two different config files in ~/.kube/ and /mnt/c/Users/<user name>/.kube if you installed minikube in Windows.","title":"Minikube Install in Ubuntu on Windows"},{"location":"Containers/Minikube_Install_in_Ubuntu_on_Windows/#install-kubectl-in-ubuntu-on-windows","text":"cd ~ mkdir bin curl -LO https://storage.googleapis.com/kubernetes-release/release/ $( curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt ) /bin/linux/amd64/kubectl chmod +x ./kubectl # optionally sudo mv ./kubectl /usr/local/bin/kubectl # then test kubectl get all # enable autocompletion source < ( kubectl completion bash ) echo \"source <(kubectl completion bash)\" >> ~/.bashrc If necessary, install socat sudo apt-get update && sudo apt-get install socat In order for kubectl to find and access a Kubernetes cluster, it needs a kubeconfig file, which is created automatically when you create a cluster using kube-up.sh or successfully deploy a Minikube cluster. Check that kubectl is properly configured by getting the cluster state: kubectl cluster-info Beware that you may have two different config files in ~/.kube/ and /mnt/c/Users/<user name>/.kube if you installed minikube in Windows.","title":"Install kubectl in Ubuntu on Windows"},{"location":"Containers/Minikube_Install_on_Windows/","text":"Install minikube on Windows \u00b6 Minikube runs a single-node Kubernetes cluster inside a VM on your laptop for users looking to try out Kubernetes or develop with it day-to-day. For Windows, install VirtualBox or Hyper-V first. Minikube is distributed in binary form: GitHub Repo . Download the minikube-installer.exe file, and execute the installer. This should automatically add minikube.exe to your path with an uninstaller available as well. If needed, add C:\\Program Files (x86)\\Kubernetes\\minikube or similar to the PATH (in System Settings > Environment Variables ) Test that minikube works: $ minikube More info at Getting Started Install kubectl \u00b6 Use a version of kubectl that is the same version as your server or later. Using an older kubectl with a newer server might produce validation errors. On Windows 10 (using Git Bash): curl -LO https://storage.googleapis.com/kubernetes-release/release/v1.10.0/bin/windows/amd64/kubectl.exe OR Install Choco Then choco install kubernetes-cli Run kubectl version to verify that the version you\u2019ve installed is sufficiently up-to-date. kubectl version Configure kubectl \u00b6 Configure kubectl to use a remote Kubernetes cluster If ~/.kube config does not exist (it should have been created by minikube ), enter the following in Powershell: cd C: \\u sers \\< yourusername> mkdir .kube cd .kube New-Item config -type file Edit the config file with a text editor of your choice. Check that kubectl is properly configured by getting the cluster state: kubectl cluster-info Enable auto-completion (if you use Git Bash) echo \"source <(kubectl completion bash)\" >> ~/.bashrc You must have appropriate permissions to list, create, edit and delete pods in your cluster: kubectl auth can-i list pods kubectl auth can-i create pods kubectl auth can-i edit pods kubectl auth can-i delete pods Run kubectl from the Ubuntu on Windows command line \u00b6 If installed by choco export PATH = $PATH :/mnt/c/ProgramData/chocolatey/bin/kubectl # then use: kubectl.exe Run minikube \u00b6 Running Kubernetes Locally via Minikube Install curl choco install curl Test curl curl http://google.com Start minikube minikube start List hosts kubectl get nodes Test by deploying a container (creates a deployment / pod automatically) kubectl run hello-minikube --image = k8s.gcr.io/echoserver:1.4 --port = 8080 Provide a dynamic port to the container (creates a service automatically) kubectl expose deployment hello-minikube --type = NodePort We have now launched an echoserver pod but we have to wait until the pod is up before curling/accessing it via the exposed service. To check whether the pod is up and running we can use the following: kubectl get pod Once the pod is running, curl it: curl $( minikube service hello-minikube --url ) Cleanup: kubectl delete deployment hello-minikube kubectl delete service hello-minikube minikube stop Install Helm \u00b6 Helm is a package manager for Kubernetes. Download a binary release of the Helm client from here Once you have Helm ready, you can initialize the local CLI and also install Tiller into your Kubernetes cluster in one step: $ helm init This will install Tiller (the helm server) into the current Kubernetes cluster (as listed in kubectl config current-context ). Install a test Helm chart, then clean up helm repo update # Make sure we get the latest list of charts helm install stable/mysql helm ls helm status <release name> helm delete <release name>","title":"Minikube Install on Windows"},{"location":"Containers/Minikube_Install_on_Windows/#install-minikube-on-windows","text":"Minikube runs a single-node Kubernetes cluster inside a VM on your laptop for users looking to try out Kubernetes or develop with it day-to-day. For Windows, install VirtualBox or Hyper-V first. Minikube is distributed in binary form: GitHub Repo . Download the minikube-installer.exe file, and execute the installer. This should automatically add minikube.exe to your path with an uninstaller available as well. If needed, add C:\\Program Files (x86)\\Kubernetes\\minikube or similar to the PATH (in System Settings > Environment Variables ) Test that minikube works: $ minikube More info at Getting Started","title":"Install minikube on Windows"},{"location":"Containers/Minikube_Install_on_Windows/#install-kubectl","text":"Use a version of kubectl that is the same version as your server or later. Using an older kubectl with a newer server might produce validation errors. On Windows 10 (using Git Bash): curl -LO https://storage.googleapis.com/kubernetes-release/release/v1.10.0/bin/windows/amd64/kubectl.exe OR Install Choco Then choco install kubernetes-cli Run kubectl version to verify that the version you\u2019ve installed is sufficiently up-to-date. kubectl version","title":"Install kubectl"},{"location":"Containers/Minikube_Install_on_Windows/#configure-kubectl","text":"Configure kubectl to use a remote Kubernetes cluster If ~/.kube config does not exist (it should have been created by minikube ), enter the following in Powershell: cd C: \\u sers \\< yourusername> mkdir .kube cd .kube New-Item config -type file Edit the config file with a text editor of your choice. Check that kubectl is properly configured by getting the cluster state: kubectl cluster-info Enable auto-completion (if you use Git Bash) echo \"source <(kubectl completion bash)\" >> ~/.bashrc You must have appropriate permissions to list, create, edit and delete pods in your cluster: kubectl auth can-i list pods kubectl auth can-i create pods kubectl auth can-i edit pods kubectl auth can-i delete pods","title":"Configure kubectl"},{"location":"Containers/Minikube_Install_on_Windows/#run-kubectl-from-the-ubuntu-on-windows-command-line","text":"If installed by choco export PATH = $PATH :/mnt/c/ProgramData/chocolatey/bin/kubectl # then use: kubectl.exe","title":"Run kubectl from the Ubuntu on Windows command line"},{"location":"Containers/Minikube_Install_on_Windows/#run-minikube","text":"Running Kubernetes Locally via Minikube Install curl choco install curl Test curl curl http://google.com Start minikube minikube start List hosts kubectl get nodes Test by deploying a container (creates a deployment / pod automatically) kubectl run hello-minikube --image = k8s.gcr.io/echoserver:1.4 --port = 8080 Provide a dynamic port to the container (creates a service automatically) kubectl expose deployment hello-minikube --type = NodePort We have now launched an echoserver pod but we have to wait until the pod is up before curling/accessing it via the exposed service. To check whether the pod is up and running we can use the following: kubectl get pod Once the pod is running, curl it: curl $( minikube service hello-minikube --url ) Cleanup: kubectl delete deployment hello-minikube kubectl delete service hello-minikube minikube stop","title":"Run minikube"},{"location":"Containers/Minikube_Install_on_Windows/#install-helm","text":"Helm is a package manager for Kubernetes. Download a binary release of the Helm client from here Once you have Helm ready, you can initialize the local CLI and also install Tiller into your Kubernetes cluster in one step: $ helm init This will install Tiller (the helm server) into the current Kubernetes cluster (as listed in kubectl config current-context ). Install a test Helm chart, then clean up helm repo update # Make sure we get the latest list of charts helm install stable/mysql helm ls helm status <release name> helm delete <release name>","title":"Install Helm"},{"location":"Data_Science/Data_Manipulation/","text":"Pandas \u00b6 Pandas cheatsheet Pandas cheatsheet 2 Data Wrangling with .NET \u00b6 Quora: Which is the best machine learning library for .NET? Deedle- Exploratory data library for .NET Deedle is an easy to use library for data and time series manipulation and for scientific programming. It supports working with structured data frames, ordered and unordered data, as well as time series. Deedle is designed to work well for exploratory programming using F# and C# interactive console, but can be also used in efficient compiled .NET code. The library implements a wide range of operations for data manipulation including advanced indexing and slicing, joining and aligning data, handling of missing values, grouping and aggregation, statistics and more. Accord.NET Framework Accord.NET provides statistical analysis, machine learning, image processing and computer vision methods for .NET applications. The Accord.NET Framework extends the popular AForge.NET with new features, adding to a more complete environment for scientific computing in .NET.","title":"Data Manipulation"},{"location":"Data_Science/Data_Manipulation/#pandas","text":"Pandas cheatsheet Pandas cheatsheet 2","title":"Pandas"},{"location":"Data_Science/Data_Manipulation/#data-wrangling-with-net","text":"Quora: Which is the best machine learning library for .NET? Deedle- Exploratory data library for .NET Deedle is an easy to use library for data and time series manipulation and for scientific programming. It supports working with structured data frames, ordered and unordered data, as well as time series. Deedle is designed to work well for exploratory programming using F# and C# interactive console, but can be also used in efficient compiled .NET code. The library implements a wide range of operations for data manipulation including advanced indexing and slicing, joining and aligning data, handling of missing values, grouping and aggregation, statistics and more. Accord.NET Framework Accord.NET provides statistical analysis, machine learning, image processing and computer vision methods for .NET applications. The Accord.NET Framework extends the popular AForge.NET with new features, adding to a more complete environment for scientific computing in .NET.","title":"Data Wrangling with .NET"},{"location":"Data_Science/Data_Visualization/","text":"Basics \u00b6 Data visualization - Wikipedia 19 Tools for Data Visualization Projects 22 free tools for data visualization and analysis - Computerworld 22 free tools for data visualization and analysis JavaScript libraries / APIs \u00b6 D3.js - Data-Driven Documents D3 provides many built-in reusable functions and function factories, such as graphical primitives for area, line and pie charts. D3 building blocks C3.js Google Charts Tools \u00b6 plot.ly Tableau Qlik Quadrigram Open-source data viz \u00b6 Superset vs Metabase vs ReDash AirPal","title":"Data Visualization"},{"location":"Data_Science/Data_Visualization/#basics","text":"Data visualization - Wikipedia 19 Tools for Data Visualization Projects 22 free tools for data visualization and analysis - Computerworld 22 free tools for data visualization and analysis","title":"Basics"},{"location":"Data_Science/Data_Visualization/#javascript-libraries-apis","text":"D3.js - Data-Driven Documents D3 provides many built-in reusable functions and function factories, such as graphical primitives for area, line and pie charts. D3 building blocks C3.js Google Charts","title":"JavaScript libraries / APIs"},{"location":"Data_Science/Data_Visualization/#tools","text":"plot.ly Tableau Qlik Quadrigram","title":"Tools"},{"location":"Data_Science/Data_Visualization/#open-source-data-viz","text":"Superset vs Metabase vs ReDash AirPal","title":"Open-source data viz"},{"location":"Data_Science/Deep_Learning/","text":"Libraries \u00b6 DeepLearning4j TensorFlow Caffe - Deep Learning Framework MXnet Keras Useful Links \u00b6 What is deep learning Why-is-Deep-Learning-so-popular-and-in-demand-these-days Deep Learning for beginners (deeplearning4j) The best answers to your most crucial deep learning questions Colah's blog - Neural Networks Neural Networks - Dropout Deep Learning Glossary Deep Learning for Visual Question Answering Deep Learning - RNNs \u00b6 RNNs Embed, encode, attend, predict- The new deep learning formula for state-of-the-art NLP models - Blog - Explosion AI RNN Tutorial Part 1 Part 2 Part 3 Part 4","title":"Deep Learning"},{"location":"Data_Science/Deep_Learning/#libraries","text":"DeepLearning4j TensorFlow Caffe - Deep Learning Framework MXnet Keras","title":"Libraries"},{"location":"Data_Science/Deep_Learning/#useful-links","text":"What is deep learning Why-is-Deep-Learning-so-popular-and-in-demand-these-days Deep Learning for beginners (deeplearning4j) The best answers to your most crucial deep learning questions Colah's blog - Neural Networks Neural Networks - Dropout Deep Learning Glossary Deep Learning for Visual Question Answering","title":"Useful Links"},{"location":"Data_Science/Deep_Learning/#deep-learning-rnns","text":"RNNs Embed, encode, attend, predict- The new deep learning formula for state-of-the-art NLP models - Blog - Explosion AI RNN Tutorial Part 1 Part 2 Part 3 Part 4","title":"Deep Learning - RNNs"},{"location":"Data_Science/Dimensionality_Reduction/","text":"Useful Links \u00b6 Dimensionality Reduction Visualizing MNIST- An Exploration of Dimensionality Reduction - colah's blog Seven Techniques for Dimensionality Reduction: Missing Values, Low Variance Filter, High Correlation Filter, PCA, Random Forests, Backward Feature Elimination, and Forward Feature Construction","title":"Dimensionality Reduction"},{"location":"Data_Science/Dimensionality_Reduction/#useful-links","text":"Dimensionality Reduction Visualizing MNIST- An Exploration of Dimensionality Reduction - colah's blog Seven Techniques for Dimensionality Reduction: Missing Values, Low Variance Filter, High Correlation Filter, PCA, Random Forests, Backward Feature Elimination, and Forward Feature Construction","title":"Useful Links"},{"location":"Data_Science/Keras/","text":"Useful Links \u00b6 Keras Keras Blog Building autoencoders in Keras manifold-learning-and-autoencoders Keras tutorial for Kaggle 2nd Annual Data Science Bowl Supervised Sequence Labelling with Recurrent Neural Networks Sequence Classification with LSTM Recurrent Neural Networks in Python with Keras","title":"Keras Cheatsheet"},{"location":"Data_Science/Keras/#useful-links","text":"Keras Keras Blog Building autoencoders in Keras manifold-learning-and-autoencoders Keras tutorial for Kaggle 2nd Annual Data Science Bowl Supervised Sequence Labelling with Recurrent Neural Networks Sequence Classification with LSTM Recurrent Neural Networks in Python with Keras","title":"Useful Links"},{"location":"Data_Science/Machine_Learning/","text":"Useful Links \u00b6 DataTau Datasets \u00b6 UC Irvine Machine Learning Repository Quandl Kaggle Algorithms \u00b6 Restricted Boltzmann Machines \u00b6 A Beginner's Tutorial for Restricted Boltzmann Machines - Deeplearning4j- Open-source, Distributed Deep Learning for the JVM Embedding \u00b6 t-distributed stochastic neighbor embedding - Wikipedia Reinforcement Learning \u00b6 Lecture 10 Reinforcement Learning I PyBrain PyBrain - a simple neural networks library in Python CyBrain ML Plaforms \u00b6 Palladium GUI tools \u00b6 Orange Provides a design tool for visual programming allowing you to connect together data preparation, algorithms, and result evaluation together to create machine learning \u201cprograms\u201d. Provides over 100 widgets for the environment and also provides a Python API and library for integrating into your application. Weka explorer A graphical machine learning workbench. It provides an explorer that you can use to prepare data, run algorithms and review results. It also provides an experimenter where you can perform the same tasks in a controlled environment and design a batch of algorithm runs that could run for an extended period of time and then review the results. Finally, it also provides a data flow interface where you can plug algorithms together like a flow diagram. Under the covers you can use Weka as a Java library and write programs that make use of the algorithms. BigML A web service where you can upload your data, prepare it and run algorithms on it. It provides clean and easy to use interfaces for configuring algorithms (decision trees) and reviewing the results. The best feature of this service is that it is all in the cloud, meaning that all you need is a web browser to get started. It also provides an API so that if you like it you can build an application around it. Tutorials \u00b6 machinelearningmastery.com Kaggle Books \u00b6 The Elements of Statistical Learning: Data Mining, Inference, and Prediction Boosting Foundations and Algorithms","title":"Machine Learning"},{"location":"Data_Science/Machine_Learning/#useful-links","text":"DataTau","title":"Useful Links"},{"location":"Data_Science/Machine_Learning/#datasets","text":"UC Irvine Machine Learning Repository Quandl Kaggle","title":"Datasets"},{"location":"Data_Science/Machine_Learning/#algorithms","text":"","title":"Algorithms"},{"location":"Data_Science/Machine_Learning/#restricted-boltzmann-machines","text":"A Beginner's Tutorial for Restricted Boltzmann Machines - Deeplearning4j- Open-source, Distributed Deep Learning for the JVM","title":"Restricted Boltzmann Machines"},{"location":"Data_Science/Machine_Learning/#embedding","text":"t-distributed stochastic neighbor embedding - Wikipedia","title":"Embedding"},{"location":"Data_Science/Machine_Learning/#reinforcement-learning","text":"Lecture 10 Reinforcement Learning I PyBrain PyBrain - a simple neural networks library in Python CyBrain","title":"Reinforcement Learning"},{"location":"Data_Science/Machine_Learning/#ml-plaforms","text":"Palladium","title":"ML Plaforms"},{"location":"Data_Science/Machine_Learning/#gui-tools","text":"Orange Provides a design tool for visual programming allowing you to connect together data preparation, algorithms, and result evaluation together to create machine learning \u201cprograms\u201d. Provides over 100 widgets for the environment and also provides a Python API and library for integrating into your application. Weka explorer A graphical machine learning workbench. It provides an explorer that you can use to prepare data, run algorithms and review results. It also provides an experimenter where you can perform the same tasks in a controlled environment and design a batch of algorithm runs that could run for an extended period of time and then review the results. Finally, it also provides a data flow interface where you can plug algorithms together like a flow diagram. Under the covers you can use Weka as a Java library and write programs that make use of the algorithms. BigML A web service where you can upload your data, prepare it and run algorithms on it. It provides clean and easy to use interfaces for configuring algorithms (decision trees) and reviewing the results. The best feature of this service is that it is all in the cloud, meaning that all you need is a web browser to get started. It also provides an API so that if you like it you can build an application around it.","title":"GUI tools"},{"location":"Data_Science/Machine_Learning/#tutorials","text":"machinelearningmastery.com Kaggle","title":"Tutorials"},{"location":"Data_Science/Machine_Learning/#books","text":"The Elements of Statistical Learning: Data Mining, Inference, and Prediction Boosting Foundations and Algorithms","title":"Books"},{"location":"Data_Science/Recommenders/","text":"Useful Links \u00b6 Recommender System Building a Recommendation Engine- An Algorithm Tutorial - Toptal Music Recognition Algorithm- How Shazam Works - Toptal","title":"Recommender Systems"},{"location":"Data_Science/Recommenders/#useful-links","text":"Recommender System Building a Recommendation Engine- An Algorithm Tutorial - Toptal Music Recognition Algorithm- How Shazam Works - Toptal","title":"Useful Links"},{"location":"Data_Science/scikit-learn/","text":"Useful Links \u00b6 Cheatsheet- Scikit-Learn & Caret Package for Python & R respectively","title":"Scikit-Learn"},{"location":"Data_Science/scikit-learn/#useful-links","text":"Cheatsheet- Scikit-Learn & Caret Package for Python & R respectively","title":"Useful Links"},{"location":"Databases/Mongodb/","text":"Import from CSV \u00b6 mongoimport --db users --collection contacts --type csv --headerline --file contacts.csv Specifying --headerline instructs mongoimport to determine the name of the fields using the first line in the CSV file. Use the --ignoreBlanks option to ignore blank fields. For CSV and TSV imports, this option provides the desired functionality in most cases, because it avoids inserting fields with null values into your collection. MongoImport documentation Print from a Cursor \u00b6 myCursor . forEach ( printjson ); // or while ( myCollection . hasNext ()) { printjson ( myCollection . next ()); } Aggregation Tips \u00b6 // lowercase a string { $project : { \"address\" : { $toLower : \"$address\" } } }, // extract field within embedded document { $project : { \"experience.location\" : 1 } }, // flatten { $unwind : \"$experience\" }, { $group : { _id : \"$_id\" , locs : { $push : { $ifNull : [ \"$experience.location\" , \"undefined\" ] } } } } // output a collection { $out : \"myCollection2\" } // get unique values { $group : { _id : \"$fulladdress\" } } Make a Copy \u00b6 Don't use copyTo - it is fully blocking... and deprecated in 3.x Use the Aggregation framework: db = db . getSiblingDB ( \"myDB\" ); // set current db for $out var myCollection = db . getCollection ( \"myCollection\" ); // project if needed, get uniques if needed, create a new collection myCollection . aggregate ([{ $project : { \"fulladdress\" : 1 } },{ $group : { _id : \"$fulladdress\" } },{ $out : \"outputCollection\" }], { allowDiskUse : true }); Or use bulk update: var outputColl = db . getCollection ( \"outputCollection\" ); var outputBulk = outputColl . initializeUnorderedBulkOp (); myCollection . find ( {}, { \"fulladdress\" : 1 } ). forEach ( function ( doc ) { outputBulk . insert ( doc ); }); outputBulk . execute (); Longer Example \u00b6 Add a count field to all records function gatherStats () { var start = Date . now (); var inputDB = db . getSiblingDB ( \"inputDB\" ); var inputColl = inputDB . getCollection ( \"inputColl\" ); // debug: inputColl.find( {} ).limit(2).forEach(printjson); outputDB = db . getSiblingDB ( \"outputDB\" ); db = outputDB ; // set current database for the next aggregate step // create temporary collection with count inputColl . aggregate ( [ { $group : { _id : { $toLower : \"$address\" }, count : { $sum : 1 } } }, { $sort : { \"count\" : - 1 } }, { $limit : 100000 }, // limit to 100k addresses with highest count { $out : \"stats\" } ], { allowDiskUse : true } ); // returns { _id, count } where _id is the address var statsColl = outputDB . getCollection ( \"stats\" ); // create output collection var outputColl = outputDB . getCollection ( \"outputColl\" ); var outputBulk = outputColl . initializeUnorderedBulkOp (); var counter = 0 ; var inputCursor = inputColl . find ( {}, {} ); inputCursor . forEach ( function ( doc ) { var statDoc = statsColl . findOne ( { _id : doc . address } ); if ( statDoc ) { doc . count = statDoc . count ; outputBulk . insert ( doc ); counter ++ ; if ( counter % 1000 == 0 ) { outputBulk . execute (); // you have to reset outputBulk = outputColl . initializeUnorderedBulkOp (); } } } ); if ( counter % 1000 > 0 ) outputBulk . execute (); // print the results outputColl . find ({}). sort ({ count : - 1 }). forEach ( printjson ); var end = Date . now (); var duration = ( end - start ) / 1000 ; printjson ( \"Duration: \" + duration + \" seconds\" ); printjson ( \" | DONE | \" ); } gatherStats (); Alternatively move data to memory: var statsDict = {}; // or better Object.create(null); statsColl . find ({}). forEach ( function ( doc ) { statsDict [ doc . _id ] = doc . count } ); // could also use: var statsArray = statsCursor.toArray(); inputCursor . forEach ( function ( doc ) { if ( doc . address in statsDict ) { doc [ \"count\" ] = statsDict [ doc . address ]; outputBulk . insert ( doc ); } }); outputBulk . execute ();","title":"MongoDB Cheatsheet"},{"location":"Databases/Mongodb/#import-from-csv","text":"mongoimport --db users --collection contacts --type csv --headerline --file contacts.csv Specifying --headerline instructs mongoimport to determine the name of the fields using the first line in the CSV file. Use the --ignoreBlanks option to ignore blank fields. For CSV and TSV imports, this option provides the desired functionality in most cases, because it avoids inserting fields with null values into your collection. MongoImport documentation","title":"Import from CSV"},{"location":"Databases/Mongodb/#print-from-a-cursor","text":"myCursor . forEach ( printjson ); // or while ( myCollection . hasNext ()) { printjson ( myCollection . next ()); }","title":"Print from a Cursor"},{"location":"Databases/Mongodb/#aggregation-tips","text":"// lowercase a string { $project : { \"address\" : { $toLower : \"$address\" } } }, // extract field within embedded document { $project : { \"experience.location\" : 1 } }, // flatten { $unwind : \"$experience\" }, { $group : { _id : \"$_id\" , locs : { $push : { $ifNull : [ \"$experience.location\" , \"undefined\" ] } } } } // output a collection { $out : \"myCollection2\" } // get unique values { $group : { _id : \"$fulladdress\" } }","title":"Aggregation Tips"},{"location":"Databases/Mongodb/#make-a-copy","text":"Don't use copyTo - it is fully blocking... and deprecated in 3.x Use the Aggregation framework: db = db . getSiblingDB ( \"myDB\" ); // set current db for $out var myCollection = db . getCollection ( \"myCollection\" ); // project if needed, get uniques if needed, create a new collection myCollection . aggregate ([{ $project : { \"fulladdress\" : 1 } },{ $group : { _id : \"$fulladdress\" } },{ $out : \"outputCollection\" }], { allowDiskUse : true }); Or use bulk update: var outputColl = db . getCollection ( \"outputCollection\" ); var outputBulk = outputColl . initializeUnorderedBulkOp (); myCollection . find ( {}, { \"fulladdress\" : 1 } ). forEach ( function ( doc ) { outputBulk . insert ( doc ); }); outputBulk . execute ();","title":"Make a Copy"},{"location":"Databases/Mongodb/#longer-example","text":"Add a count field to all records function gatherStats () { var start = Date . now (); var inputDB = db . getSiblingDB ( \"inputDB\" ); var inputColl = inputDB . getCollection ( \"inputColl\" ); // debug: inputColl.find( {} ).limit(2).forEach(printjson); outputDB = db . getSiblingDB ( \"outputDB\" ); db = outputDB ; // set current database for the next aggregate step // create temporary collection with count inputColl . aggregate ( [ { $group : { _id : { $toLower : \"$address\" }, count : { $sum : 1 } } }, { $sort : { \"count\" : - 1 } }, { $limit : 100000 }, // limit to 100k addresses with highest count { $out : \"stats\" } ], { allowDiskUse : true } ); // returns { _id, count } where _id is the address var statsColl = outputDB . getCollection ( \"stats\" ); // create output collection var outputColl = outputDB . getCollection ( \"outputColl\" ); var outputBulk = outputColl . initializeUnorderedBulkOp (); var counter = 0 ; var inputCursor = inputColl . find ( {}, {} ); inputCursor . forEach ( function ( doc ) { var statDoc = statsColl . findOne ( { _id : doc . address } ); if ( statDoc ) { doc . count = statDoc . count ; outputBulk . insert ( doc ); counter ++ ; if ( counter % 1000 == 0 ) { outputBulk . execute (); // you have to reset outputBulk = outputColl . initializeUnorderedBulkOp (); } } } ); if ( counter % 1000 > 0 ) outputBulk . execute (); // print the results outputColl . find ({}). sort ({ count : - 1 }). forEach ( printjson ); var end = Date . now (); var duration = ( end - start ) / 1000 ; printjson ( \"Duration: \" + duration + \" seconds\" ); printjson ( \" | DONE | \" ); } gatherStats (); Alternatively move data to memory: var statsDict = {}; // or better Object.create(null); statsColl . find ({}). forEach ( function ( doc ) { statsDict [ doc . _id ] = doc . count } ); // could also use: var statsArray = statsCursor.toArray(); inputCursor . forEach ( function ( doc ) { if ( doc . address in statsDict ) { doc [ \"count\" ] = statsDict [ doc . address ]; outputBulk . insert ( doc ); } }); outputBulk . execute ();","title":"Longer Example"},{"location":"Databases/Redshift/","text":"Redshift Best Practices \u00b6 Smaller node types load data faster Best Practices for data load: 1 file in S3 per slice (instances in RedShift) Compressed using gzip compression File size: 1MB to 1GB compressed COPY from S3 is the fastest COPY from EMR HDFS may be faster, but most people don't use HDFS - they store data in S3 First column of SORTKEY should not be compressed Workflows: move from staging table to production table Make sure to wrap the entire workflow into ONE transaction COMMITs are very expensive in RedShift Disable statistics on staging tables Make sure that the distribution keys match between staging and prod tables Compress your staging tables Do ANALYZE after VACUUM","title":"RedShift"},{"location":"Databases/Redshift/#redshift-best-practices","text":"Smaller node types load data faster Best Practices for data load: 1 file in S3 per slice (instances in RedShift) Compressed using gzip compression File size: 1MB to 1GB compressed COPY from S3 is the fastest COPY from EMR HDFS may be faster, but most people don't use HDFS - they store data in S3 First column of SORTKEY should not be compressed Workflows: move from staging table to production table Make sure to wrap the entire workflow into ONE transaction COMMITs are very expensive in RedShift Disable statistics on staging tables Make sure that the distribution keys match between staging and prod tables Compress your staging tables Do ANALYZE after VACUUM","title":"Redshift Best Practices"},{"location":"Databases/SQL/","text":"SQL Cheatsheet \u00b6 DML: SELECT \u00b6 Filter : SELECT LastName , FirstName , Address FROM Persons WHERE Address IS NULL Like : SELECT * FROM Customers WHERE City LIKE 's%' ; SELECT * FROM Customers WHERE Country LIKE '%land%' ; SELECT * FROM Customers WHERE Country NOT LIKE '%land%' ; Sort : SELECT * FROM Customers ORDER BY Country DESC ; SELECT * FROM Customers ORDER BY Country , CustomerName ; Limit : SELECT TOP number | percent column_name ( s ) FROM table_name ; -- Examples: SELECT TOP 2 * FROM Customers ; SELECT TOP 50 PERCENT * FROM Customers ; Oracle Syntax : SELECT column_name ( s ) FROM table_name WHERE ROWNUM <= number ; Joins : SELECT Customers . CustomerName , Orders . OrderID FROM Customers FULL OUTER JOIN Orders ON Customers . CustomerID = Orders . CustomerID ORDER BY Customers . CustomerName ; Union : SELECT column_name ( s ) FROM table1 UNION SELECT column_name ( s ) FROM table2 ; SELECT column_name ( s ) FROM table1 UNION ALL SELECT column_name ( s ) FROM table2 ; Select Into : SELECT column_name ( s ) INTO newtable [ IN externaldb ] FROM table1 ; Formula : SELECT ProductName , UnitPrice * ( UnitsInStock + ISNULL ( UnitsOnOrder , 0 )) FROM Products DML: INSERT \u00b6 INSERT INTO table_name VALUES ( value1 , value2 , value3 ,...); INSERT INTO table_name ( column1 , column2 , column3 ,...) VALUES ( value1 , value2 , value3 ,...); -- Example: INSERT INTO Customers ( CustomerName , City , Country ) VALUES ( 'Cardinal' , 'Stavanger' , 'Norway' ); Insert from select : INSERT INTO table2 ( column_name ( s )) SELECT column_name ( s ) FROM table1 ; -- Example: INSERT INTO Customers ( CustomerName , Country ) SELECT SupplierName , Country FROM Suppliers WHERE Country = 'Germany' ; DML: UPDATE \u00b6 UPDATE table_name SET column1 = value1 , column2 = value2 ,... WHERE some_column = some_value ; -- Example: UPDATE Customers SET ContactName = 'Alfred Schmidt' , City = 'Hamburg' WHERE CustomerName = 'Alfreds Futterkiste' ; DML: DELETE \u00b6 DELETE FROM table_name WHERE some_column = some_value ; DELETE FROM Customers WHERE CustomerName = 'Alfreds Futterkiste' AND ContactName = 'Maria Anders' ; Databases \u00b6 CREATE DATABASE my_db ; DROP DATABASE my_db ; Tables \u00b6 Create : CREATE TABLE table_name ( column_name1 data_type ( size ), column_name2 data_type ( size ), column_name3 data_type ( size ), .... ); CREATE TABLE table_name ( column_name1 data_type ( size ) constraint_name , column_name2 data_type ( size ) constraint_name , column_name3 data_type ( size ) constraint_name , .... ); -- Examples CREATE TABLE Persons ( P_Id int NOT NULL UNIQUE , LastName varchar ( 255 ) NOT NULL , FirstName varchar ( 255 ), Address varchar ( 255 ), City varchar ( 255 ) ) CREATE TABLE Persons ( P_Id int NOT NULL , LastName varchar ( 255 ) NOT NULL , FirstName varchar ( 255 ), Address varchar ( 255 ), City varchar ( 255 ), CONSTRAINT uc_PersonID UNIQUE ( P_Id , LastName ) ) ALTER TABLE Persons ADD CONSTRAINT uc_PersonID UNIQUE ( P_Id , LastName ) ALTER TABLE Persons DROP CONSTRAINT uc_PersonID Temporary Table : CREATE TABLE # MyTempTable ( cola INT PRIMARY KEY ); INSERT INTO # MyTempTable VALUES ( 1 ); Drop / Truncate : DROP TABLE table_name TRUNCATE TABLE table_name PRIMARY KEY constraint \u00b6 CREATE TABLE Persons ( P_Id int NOT NULL PRIMARY KEY , LastName varchar ( 255 ) NOT NULL , FirstName varchar ( 255 ), Address varchar ( 255 ), City varchar ( 255 ) ) CREATE TABLE Persons ( P_Id int NOT NULL , LastName varchar ( 255 ) NOT NULL , FirstName varchar ( 255 ), Address varchar ( 255 ), City varchar ( 255 ), CONSTRAINT PK_PersonID PRIMARY KEY ( P_Id , LastName ) ) ALTER TABLE Persons ADD CONSTRAINT PK_PersonID PRIMARY KEY ( P_Id , LastName ) ALTER TABLE Persons DROP CONSTRAINT PK_PersonID FOREIGN KEY constraints \u00b6 CREATE TABLE Orders ( O_Id int NOT NULL PRIMARY KEY , OrderNo int NOT NULL , P_Id int FOREIGN KEY REFERENCES Persons ( P_Id ) ) CREATE TABLE Orders ( O_Id int NOT NULL , OrderNo int NOT NULL , P_Id int , PRIMARY KEY ( O_Id ), CONSTRAINT FK_PerOrders FOREIGN KEY ( P_Id ) REFERENCES Persons ( P_Id ) ) ALTER TABLE Orders ADD FOREIGN KEY ( P_Id ) REFERENCES Persons ( P_Id ) ALTER TABLE Orders ADD CONSTRAINT fk_PerOrders FOREIGN KEY ( P_Id ) REFERENCES Persons ( P_Id ) ALTER TABLE Orders DROP CONSTRAINT fk_PerOrders CHECK Constraints \u00b6 CREATE TABLE Persons ( P_Id int NOT NULL CHECK ( P_Id > 0 ), LastName varchar ( 255 ) NOT NULL , FirstName varchar ( 255 ), Address varchar ( 255 ), City varchar ( 255 ) ) CREATE TABLE Persons ( P_Id int NOT NULL , LastName varchar ( 255 ) NOT NULL , FirstName varchar ( 255 ), Address varchar ( 255 ), City varchar ( 255 ), CONSTRAINT chk_Person CHECK ( P_Id > 0 AND City = 'Sandnes' ) ) ALTER TABLE Persons ADD CONSTRAINT CHK_Person CHECK ( P_Id > 0 AND City = 'Sandnes' ) ALTER TABLE Persons DROP CONSTRAINT CHK_Person DEFAULT Constraints \u00b6 CREATE TABLE Orders ( O_Id int NOT NULL , OrderNo int NOT NULL , P_Id int , OrderDate date DEFAULT GETDATE () ) ALTER TABLE Persons ALTER COLUMN City SET DEFAULT 'SEATTLE' ALTER TABLE Persons ALTER COLUMN City DROP DEFAULT Index \u00b6 CREATE UNIQUE INDEX index_name ON table_name ( column_name ) CREATE INDEX index_name ON table_name ( column_name1 , col_name2 ) -- Example: CREATE INDEX PIndex ON Persons ( LastName , FirstName ) DROP INDEX table_name . index_name -- Example: DROP INDEX IX_ProductVendor_BusinessEntityID ON Purchasing . ProductVendor ; Add / drop / alter column in table \u00b6 ALTER TABLE table_name ADD column_name datatype ALTER TABLE table_name DROP COLUMN column_name ALTER TABLE table_name ALTER COLUMN column_name datatype Autoincrement \u00b6 CREATE TABLE Persons ( ID int IDENTITY ( 1 , 1 ) PRIMARY KEY , LastName varchar ( 255 ) NOT NULL , FirstName varchar ( 255 ), Address varchar ( 255 ), City varchar ( 255 ) ) Example: CREATE TABLE dbo . PurchaseOrderDetail ( PurchaseOrderID int NOT NULL REFERENCES Purchasing . PurchaseOrderHeader ( PurchaseOrderID ), LineNumber smallint NOT NULL , ProductID int NULL REFERENCES Production . Product ( ProductID ), UnitPrice money NULL , OrderQty smallint NULL , ReceivedQty float NULL , RejectedQty float NULL , DueDate datetime NULL , rowguid uniqueidentifier ROWGUIDCOL NOT NULL CONSTRAINT DF_PurchaseOrderDetail_rowguid DEFAULT ( newid ()), ModifiedDate datetime NOT NULL CONSTRAINT DF_PurchaseOrderDetail_ModifiedDate DEFAULT ( getdate ()), LineTotal AS (( UnitPrice * OrderQty )), StockedQty AS (( ReceivedQty - RejectedQty )), CONSTRAINT PK_PurchaseOrderDetail_PurchaseOrderID_LineNumber PRIMARY KEY CLUSTERED ( PurchaseOrderID , LineNumber ) WITH ( IGNORE_DUP_KEY = OFF ) ) ON PRIMARY ; Views \u00b6 CREATE VIEW view_name AS SELECT column_name ( s ) FROM table_name WHERE condition CREATE OR REPLACE VIEW view_name AS SELECT column_name ( s ) FROM table_name WHERE condition DROP VIEW view_name Examples : CREATE VIEW [ Products Above Average Price ] AS SELECT ProductName , UnitPrice FROM Products WHERE UnitPrice > ( SELECT AVG ( UnitPrice ) FROM Products ) SELECT * FROM [ Products Above Average Price ] CREATE VIEW [ Category Sales For 1997 ] AS SELECT DISTINCT CategoryName , Sum ( ProductSales ) AS CategorySales FROM [ Product Sales for 1997 ] GROUP BY CategoryName Dates \u00b6 GETDATE () -- Returns the current date and time DATEPART () -- Returns a single part of a date/time DATEADD () -- Adds or subtracts a specified time interval from a date DATEDIFF () -- Returns the time between two dates CONVERT () -- Displays date/time data in different formats Example : CREATE TABLE Orders ( OrderId int NOT NULL PRIMARY KEY , ProductName varchar ( 50 ) NOT NULL , OrderDate datetime NOT NULL DEFAULT GETDATE () ) SELECT DATEPART ( yyyy , OrderDate ) AS OrderYear , DATEPART ( mm , OrderDate ) AS OrderMonth , DATEPART ( dd , OrderDate ) AS OrderDay , FROM Orders WHERE OrderId = 1 SELECT OrderId , DATEADD ( day , 45 , OrderDate ) AS OrderPayDate FROM Orders SELECT DATEDIFF ( day , '2008-06-05' , '2008-08-05' ) AS DiffDate CONVERT ( VARCHAR ( 19 ), GETDATE ()) CONVERT ( VARCHAR ( 10 ), GETDATE (), 10 ) CONVERT ( VARCHAR ( 10 ), GETDATE (), 110 ) SQL Server Data Types \u00b6 Data type / Description / Storage char(n) Fixed width character string. Maximum 8,000 characters Defined width varchar(n) Variable width character string. Maximum 8,000 characters 2 bytes + number of chars varchar(max) Variable width character string. Maximum 1,073,741,824 characters 2 bytes + number of chars text Variable width character string. Maximum 2GB of text data 4 bytes + number of chars nchar Fixed width Unicode string. Maximum 4,000 characters Defined width x 2 nvarchar Variable width Unicode string. Maximum 4,000 characters nvarchar(max) Variable width Unicode string. Maximum 536,870,912 characters ntext Variable width Unicode string. Maximum 2GB of text data bit Allows 0, 1, or NULL binary(n) Fixed width binary string. Maximum 8,000 bytes varbinary Variable width binary string. Maximum 8,000 bytes varbinary(max) Variable width binary string. Maximum 2GB image Variable width binary string. Maximum 2GB Number types \u00b6 tinyint Allows whole numbers from 0 to 255 1 byte smallint Allows whole numbers between -32,768 and 32,767 2 bytes int Allows whole numbers between -2,147,483,648 and 2,147,483,647 4 bytes bigint Allows whole numbers between -9,223,372,036,854,775,808 and 9,223,372,036,854,775,807 8 bytes decimal(p,s) Fixed precision and scale numbers. Allows numbers from -10^38 +1 to 10^38. The p parameter indicates the maximum total number of digits that can be stored (both to the left and to the right of the decimal point). p must be a value from 1 to 38. Default is 18. The s parameter indicates the maximum number of digits stored to the right of the decimal point. s must be a value from 0 to p. Default value is 0. 5-17 bytes numeric(p,s) Fixed precision and scale numbers. Allows numbers from -10^38 +1 to 10^38. The p parameter indicates the maximum total number of digits that can be stored (both to the left and to the right of the decimal point). p must be a value from 1 to 38. Default is 18. The s parameter indicates the maximum number of digits stored to the right of the decimal point. s must be a value from 0 to p. Default value is 0. 5-17 bytes smallmoney Monetary data from -214,748.3648 to 214,748.3647 4 bytes money Monetary data from -922,337,203,685,477.5808 to 922,337,203,685,477.5807 8 bytes float(n) Floating precision number data from -1.79E + 308 to 1.79E + 308. The n parameter indicates whether the field should hold 4 or 8 bytes. float(24) holds a 4-byte field and float(53) holds an 8-byte field. Default value of n is 53. 4 or 8 bytes real Floating precision number data from -3.40E + 38 to 3.40E + 38 4 bytes Date types \u00b6 datetime From January 1, 1753 to December 31, 9999 with an accuracy of 3.33 milliseconds 8 bytes datetime2 From January 1, 0001 to December 31, 9999 with an accuracy of 100 nanoseconds 6-8 bytes smalldatetime From January 1, 1900 to June 6, 2079 with an accuracy of 1 minute 4 bytes date Store a date only. From January 1, 0001 to December 31, 9999 3 bytes time Store a time only to an accuracy of 100 nanoseconds 3-5 bytes datetimeoffset The same as datetime2 with the addition of a time zone offset 8-10 bytes timestamp Stores a unique number that gets updated every time a row gets created or modified. The timestamp value is based upon an internal clock and does not correspond to real time. Each table may have only one timestamp variable Other data types \u00b6 sql_variant Stores up to 8,000 bytes of data of various data types, except text, ntext, and timestamp uniqueidentifier Stores a globally unique identifier (GUID) xml Stores XML formatted data. Maximum 2GB cursor Stores a reference to a cursor used for database operations table Stores a result-set for later processing SQL Aggregate Functions \u00b6 SQL aggregate functions return a single value, calculated from values in a column. Useful aggregate functions: AVG() - Returns the average value COUNT() - Returns the number of rows TOP 1 - Single sample MAX() - Returns the largest value MIN() - Returns the smallest value SUM() - Returns the sum Examples: SELECT COUNT ( DISTINCT column_name ) FROM table_name ; SELECT TOP 1 column_name FROM table_name ORDER BY column_name DESC ; SELECT column_name , aggregate_function ( column_name ) FROM table_name WHERE column_name operator value GROUP BY column_name ; SELECT Shippers . ShipperName , COUNT ( Orders . OrderID ) AS NumberOfOrders FROM Orders LEFT JOIN Shippers ON Orders . ShipperID = Shippers . ShipperID GROUP BY ShipperName ; SELECT column_name , aggregate_function ( column_name ) FROM table_name WHERE column_name operator value GROUP BY column_name HAVING aggregate_function ( column_name ) operator value ; SELECT Employees . LastName , COUNT ( Orders . OrderID ) AS NumberOfOrders FROM Orders INNER JOIN Employees ON Orders . EmployeeID = Employees . EmployeeID ) GROUP BY LastName HAVING COUNT ( Orders . OrderID ) > 10 ; SQL Scalar functions \u00b6 Converts a field to upper case: SELECT UPPER(column_name) FROM table_name; Converts a field to lower case: SELECT LOWER(column_name) FROM table_name; MID() - Extract characters from a text field LEN() - Returns the length of a text field ROUND() - Rounds a numeric field to the number of decimals specified NOW() - Returns the current system date and time FORMAT() - Formats how a field is to be displayed SELECT ProductName , ROUND ( Price , 0 ) AS RoundedPrice FROM Products ; Variables \u00b6 DECLARE @ myvar char ( 20 ); SET @ myvar = 'This is a test' ; SELECT @ myvar ; Scalar Function \u00b6 CREATE FUNCTION FunctionName ( -- Add the parameters for the function here @ p1 int ) RETURNS int AS BEGIN -- Declare the return variable here DECLARE @ Result int -- Add the T-SQL statements to compute the return value here SELECT @ Result = @ p1 -- Return the result of the function RETURN @ Result END Table Value Function \u00b6 IF OBJECT_ID ( N 'dbo.EmployeeByID' ) IS NOT NULL DROP FUNCTION dbo . EmployeeByID GO CREATE FUNCTION dbo . EmployeeByID ( @ InEmpID int ) RETURNS @ retFindReports TABLE ( -- columns returned by the function EmployeeID int NOT NULL , Name nvarchar ( 255 ) NOT NULL , Title nvarchar ( 50 ) NOT NULL , EmployeeLevel int NOT NULL ) AS -- body of the function BEGIN WITH DirectReports ( Name , Title , EmployeeID , EmployeeLevel , Sort ) AS ( SELECT CONVERT ( varchar ( 255 ), c . FirstName + ' ' + c . LastName ), e . Title , e . EmployeeID , 1 , CONVERT ( varchar ( 255 ), c . FirstName + ' ' + c . LastName ) FROM HumanResources . Employee AS e JOIN Person . Contact AS c ON e . ContactID = c . ContactID WHERE e . EmployeeID = @ InEmpID UNION ALL SELECT CONVERT ( varchar ( 255 ), REPLICATE ( '| ' , EmployeeLevel ) + c . FirstName + ' ' + c . LastName ), e . Title , e . EmployeeID , EmployeeLevel + 1 , CONVERT ( varchar ( 255 ), RTRIM ( Sort ) + '| ' + FirstName + ' ' + LastName ) FROM HumanResources . Employee as e JOIN Person . Contact AS c ON e . ContactID = c . ContactID JOIN DirectReports AS d ON e . ManagerID = d . EmployeeID ) -- copy the required columns to the result of the function INSERT @ retFindReports SELECT EmployeeID , Name , Title , EmployeeLevel FROM DirectReports ORDER BY Sort RETURN END GO Stored Procedure \u00b6 CREATE PROCEDURE ProcedureName -- Add the parameters for the stored procedure here @ p1 int = 0 , @ p2 int = 0 AS BEGIN -- SET NOCOUNT ON added to prevent extra result sets from -- interfering with SELECT statements. SET NOCOUNT ON ; -- Insert statements for procedure here SELECT @ p1 , @ p2 END GO Self-join \u00b6 Q. Here's the data in a table 'orders' customer_id order_id order_day 123 27424624 25Dec2011 123 89690900 25Dec2010 797 12131323 25Dec2010 876 67145419 15Dec2011 Could you give me SQL for customers who placed orders on both the days, 25th Dec 2010 and 25th Dec 2011? SELECT o . customer_id , o . order_day FROM orders AS o INNER JOIN orders AS o1 ON o . customer_id = o1 . customer_id WHERE ...","title":"SQL Cheatsheet"},{"location":"Databases/SQL/#sql-cheatsheet","text":"","title":"SQL Cheatsheet"},{"location":"Databases/SQL/#dml-select","text":"Filter : SELECT LastName , FirstName , Address FROM Persons WHERE Address IS NULL Like : SELECT * FROM Customers WHERE City LIKE 's%' ; SELECT * FROM Customers WHERE Country LIKE '%land%' ; SELECT * FROM Customers WHERE Country NOT LIKE '%land%' ; Sort : SELECT * FROM Customers ORDER BY Country DESC ; SELECT * FROM Customers ORDER BY Country , CustomerName ; Limit : SELECT TOP number | percent column_name ( s ) FROM table_name ; -- Examples: SELECT TOP 2 * FROM Customers ; SELECT TOP 50 PERCENT * FROM Customers ; Oracle Syntax : SELECT column_name ( s ) FROM table_name WHERE ROWNUM <= number ; Joins : SELECT Customers . CustomerName , Orders . OrderID FROM Customers FULL OUTER JOIN Orders ON Customers . CustomerID = Orders . CustomerID ORDER BY Customers . CustomerName ; Union : SELECT column_name ( s ) FROM table1 UNION SELECT column_name ( s ) FROM table2 ; SELECT column_name ( s ) FROM table1 UNION ALL SELECT column_name ( s ) FROM table2 ; Select Into : SELECT column_name ( s ) INTO newtable [ IN externaldb ] FROM table1 ; Formula : SELECT ProductName , UnitPrice * ( UnitsInStock + ISNULL ( UnitsOnOrder , 0 )) FROM Products","title":"DML: SELECT"},{"location":"Databases/SQL/#dml-insert","text":"INSERT INTO table_name VALUES ( value1 , value2 , value3 ,...); INSERT INTO table_name ( column1 , column2 , column3 ,...) VALUES ( value1 , value2 , value3 ,...); -- Example: INSERT INTO Customers ( CustomerName , City , Country ) VALUES ( 'Cardinal' , 'Stavanger' , 'Norway' ); Insert from select : INSERT INTO table2 ( column_name ( s )) SELECT column_name ( s ) FROM table1 ; -- Example: INSERT INTO Customers ( CustomerName , Country ) SELECT SupplierName , Country FROM Suppliers WHERE Country = 'Germany' ;","title":"DML: INSERT"},{"location":"Databases/SQL/#dml-update","text":"UPDATE table_name SET column1 = value1 , column2 = value2 ,... WHERE some_column = some_value ; -- Example: UPDATE Customers SET ContactName = 'Alfred Schmidt' , City = 'Hamburg' WHERE CustomerName = 'Alfreds Futterkiste' ;","title":"DML: UPDATE"},{"location":"Databases/SQL/#dml-delete","text":"DELETE FROM table_name WHERE some_column = some_value ; DELETE FROM Customers WHERE CustomerName = 'Alfreds Futterkiste' AND ContactName = 'Maria Anders' ;","title":"DML: DELETE"},{"location":"Databases/SQL/#databases","text":"CREATE DATABASE my_db ; DROP DATABASE my_db ;","title":"Databases"},{"location":"Databases/SQL/#tables","text":"Create : CREATE TABLE table_name ( column_name1 data_type ( size ), column_name2 data_type ( size ), column_name3 data_type ( size ), .... ); CREATE TABLE table_name ( column_name1 data_type ( size ) constraint_name , column_name2 data_type ( size ) constraint_name , column_name3 data_type ( size ) constraint_name , .... ); -- Examples CREATE TABLE Persons ( P_Id int NOT NULL UNIQUE , LastName varchar ( 255 ) NOT NULL , FirstName varchar ( 255 ), Address varchar ( 255 ), City varchar ( 255 ) ) CREATE TABLE Persons ( P_Id int NOT NULL , LastName varchar ( 255 ) NOT NULL , FirstName varchar ( 255 ), Address varchar ( 255 ), City varchar ( 255 ), CONSTRAINT uc_PersonID UNIQUE ( P_Id , LastName ) ) ALTER TABLE Persons ADD CONSTRAINT uc_PersonID UNIQUE ( P_Id , LastName ) ALTER TABLE Persons DROP CONSTRAINT uc_PersonID Temporary Table : CREATE TABLE # MyTempTable ( cola INT PRIMARY KEY ); INSERT INTO # MyTempTable VALUES ( 1 ); Drop / Truncate : DROP TABLE table_name TRUNCATE TABLE table_name","title":"Tables"},{"location":"Databases/SQL/#primary-key-constraint","text":"CREATE TABLE Persons ( P_Id int NOT NULL PRIMARY KEY , LastName varchar ( 255 ) NOT NULL , FirstName varchar ( 255 ), Address varchar ( 255 ), City varchar ( 255 ) ) CREATE TABLE Persons ( P_Id int NOT NULL , LastName varchar ( 255 ) NOT NULL , FirstName varchar ( 255 ), Address varchar ( 255 ), City varchar ( 255 ), CONSTRAINT PK_PersonID PRIMARY KEY ( P_Id , LastName ) ) ALTER TABLE Persons ADD CONSTRAINT PK_PersonID PRIMARY KEY ( P_Id , LastName ) ALTER TABLE Persons DROP CONSTRAINT PK_PersonID","title":"PRIMARY KEY constraint"},{"location":"Databases/SQL/#foreign-key-constraints","text":"CREATE TABLE Orders ( O_Id int NOT NULL PRIMARY KEY , OrderNo int NOT NULL , P_Id int FOREIGN KEY REFERENCES Persons ( P_Id ) ) CREATE TABLE Orders ( O_Id int NOT NULL , OrderNo int NOT NULL , P_Id int , PRIMARY KEY ( O_Id ), CONSTRAINT FK_PerOrders FOREIGN KEY ( P_Id ) REFERENCES Persons ( P_Id ) ) ALTER TABLE Orders ADD FOREIGN KEY ( P_Id ) REFERENCES Persons ( P_Id ) ALTER TABLE Orders ADD CONSTRAINT fk_PerOrders FOREIGN KEY ( P_Id ) REFERENCES Persons ( P_Id ) ALTER TABLE Orders DROP CONSTRAINT fk_PerOrders","title":"FOREIGN KEY constraints"},{"location":"Databases/SQL/#check-constraints","text":"CREATE TABLE Persons ( P_Id int NOT NULL CHECK ( P_Id > 0 ), LastName varchar ( 255 ) NOT NULL , FirstName varchar ( 255 ), Address varchar ( 255 ), City varchar ( 255 ) ) CREATE TABLE Persons ( P_Id int NOT NULL , LastName varchar ( 255 ) NOT NULL , FirstName varchar ( 255 ), Address varchar ( 255 ), City varchar ( 255 ), CONSTRAINT chk_Person CHECK ( P_Id > 0 AND City = 'Sandnes' ) ) ALTER TABLE Persons ADD CONSTRAINT CHK_Person CHECK ( P_Id > 0 AND City = 'Sandnes' ) ALTER TABLE Persons DROP CONSTRAINT CHK_Person","title":"CHECK Constraints"},{"location":"Databases/SQL/#default-constraints","text":"CREATE TABLE Orders ( O_Id int NOT NULL , OrderNo int NOT NULL , P_Id int , OrderDate date DEFAULT GETDATE () ) ALTER TABLE Persons ALTER COLUMN City SET DEFAULT 'SEATTLE' ALTER TABLE Persons ALTER COLUMN City DROP DEFAULT","title":"DEFAULT Constraints"},{"location":"Databases/SQL/#index","text":"CREATE UNIQUE INDEX index_name ON table_name ( column_name ) CREATE INDEX index_name ON table_name ( column_name1 , col_name2 ) -- Example: CREATE INDEX PIndex ON Persons ( LastName , FirstName ) DROP INDEX table_name . index_name -- Example: DROP INDEX IX_ProductVendor_BusinessEntityID ON Purchasing . ProductVendor ;","title":"Index"},{"location":"Databases/SQL/#add-drop-alter-column-in-table","text":"ALTER TABLE table_name ADD column_name datatype ALTER TABLE table_name DROP COLUMN column_name ALTER TABLE table_name ALTER COLUMN column_name datatype","title":"Add / drop / alter column in table"},{"location":"Databases/SQL/#autoincrement","text":"CREATE TABLE Persons ( ID int IDENTITY ( 1 , 1 ) PRIMARY KEY , LastName varchar ( 255 ) NOT NULL , FirstName varchar ( 255 ), Address varchar ( 255 ), City varchar ( 255 ) ) Example: CREATE TABLE dbo . PurchaseOrderDetail ( PurchaseOrderID int NOT NULL REFERENCES Purchasing . PurchaseOrderHeader ( PurchaseOrderID ), LineNumber smallint NOT NULL , ProductID int NULL REFERENCES Production . Product ( ProductID ), UnitPrice money NULL , OrderQty smallint NULL , ReceivedQty float NULL , RejectedQty float NULL , DueDate datetime NULL , rowguid uniqueidentifier ROWGUIDCOL NOT NULL CONSTRAINT DF_PurchaseOrderDetail_rowguid DEFAULT ( newid ()), ModifiedDate datetime NOT NULL CONSTRAINT DF_PurchaseOrderDetail_ModifiedDate DEFAULT ( getdate ()), LineTotal AS (( UnitPrice * OrderQty )), StockedQty AS (( ReceivedQty - RejectedQty )), CONSTRAINT PK_PurchaseOrderDetail_PurchaseOrderID_LineNumber PRIMARY KEY CLUSTERED ( PurchaseOrderID , LineNumber ) WITH ( IGNORE_DUP_KEY = OFF ) ) ON PRIMARY ;","title":"Autoincrement"},{"location":"Databases/SQL/#views","text":"CREATE VIEW view_name AS SELECT column_name ( s ) FROM table_name WHERE condition CREATE OR REPLACE VIEW view_name AS SELECT column_name ( s ) FROM table_name WHERE condition DROP VIEW view_name Examples : CREATE VIEW [ Products Above Average Price ] AS SELECT ProductName , UnitPrice FROM Products WHERE UnitPrice > ( SELECT AVG ( UnitPrice ) FROM Products ) SELECT * FROM [ Products Above Average Price ] CREATE VIEW [ Category Sales For 1997 ] AS SELECT DISTINCT CategoryName , Sum ( ProductSales ) AS CategorySales FROM [ Product Sales for 1997 ] GROUP BY CategoryName","title":"Views"},{"location":"Databases/SQL/#dates","text":"GETDATE () -- Returns the current date and time DATEPART () -- Returns a single part of a date/time DATEADD () -- Adds or subtracts a specified time interval from a date DATEDIFF () -- Returns the time between two dates CONVERT () -- Displays date/time data in different formats Example : CREATE TABLE Orders ( OrderId int NOT NULL PRIMARY KEY , ProductName varchar ( 50 ) NOT NULL , OrderDate datetime NOT NULL DEFAULT GETDATE () ) SELECT DATEPART ( yyyy , OrderDate ) AS OrderYear , DATEPART ( mm , OrderDate ) AS OrderMonth , DATEPART ( dd , OrderDate ) AS OrderDay , FROM Orders WHERE OrderId = 1 SELECT OrderId , DATEADD ( day , 45 , OrderDate ) AS OrderPayDate FROM Orders SELECT DATEDIFF ( day , '2008-06-05' , '2008-08-05' ) AS DiffDate CONVERT ( VARCHAR ( 19 ), GETDATE ()) CONVERT ( VARCHAR ( 10 ), GETDATE (), 10 ) CONVERT ( VARCHAR ( 10 ), GETDATE (), 110 )","title":"Dates"},{"location":"Databases/SQL/#sql-server-data-types","text":"Data type / Description / Storage char(n) Fixed width character string. Maximum 8,000 characters Defined width varchar(n) Variable width character string. Maximum 8,000 characters 2 bytes + number of chars varchar(max) Variable width character string. Maximum 1,073,741,824 characters 2 bytes + number of chars text Variable width character string. Maximum 2GB of text data 4 bytes + number of chars nchar Fixed width Unicode string. Maximum 4,000 characters Defined width x 2 nvarchar Variable width Unicode string. Maximum 4,000 characters nvarchar(max) Variable width Unicode string. Maximum 536,870,912 characters ntext Variable width Unicode string. Maximum 2GB of text data bit Allows 0, 1, or NULL binary(n) Fixed width binary string. Maximum 8,000 bytes varbinary Variable width binary string. Maximum 8,000 bytes varbinary(max) Variable width binary string. Maximum 2GB image Variable width binary string. Maximum 2GB","title":"SQL Server Data Types"},{"location":"Databases/SQL/#number-types","text":"tinyint Allows whole numbers from 0 to 255 1 byte smallint Allows whole numbers between -32,768 and 32,767 2 bytes int Allows whole numbers between -2,147,483,648 and 2,147,483,647 4 bytes bigint Allows whole numbers between -9,223,372,036,854,775,808 and 9,223,372,036,854,775,807 8 bytes decimal(p,s) Fixed precision and scale numbers. Allows numbers from -10^38 +1 to 10^38. The p parameter indicates the maximum total number of digits that can be stored (both to the left and to the right of the decimal point). p must be a value from 1 to 38. Default is 18. The s parameter indicates the maximum number of digits stored to the right of the decimal point. s must be a value from 0 to p. Default value is 0. 5-17 bytes numeric(p,s) Fixed precision and scale numbers. Allows numbers from -10^38 +1 to 10^38. The p parameter indicates the maximum total number of digits that can be stored (both to the left and to the right of the decimal point). p must be a value from 1 to 38. Default is 18. The s parameter indicates the maximum number of digits stored to the right of the decimal point. s must be a value from 0 to p. Default value is 0. 5-17 bytes smallmoney Monetary data from -214,748.3648 to 214,748.3647 4 bytes money Monetary data from -922,337,203,685,477.5808 to 922,337,203,685,477.5807 8 bytes float(n) Floating precision number data from -1.79E + 308 to 1.79E + 308. The n parameter indicates whether the field should hold 4 or 8 bytes. float(24) holds a 4-byte field and float(53) holds an 8-byte field. Default value of n is 53. 4 or 8 bytes real Floating precision number data from -3.40E + 38 to 3.40E + 38 4 bytes","title":"Number types"},{"location":"Databases/SQL/#date-types","text":"datetime From January 1, 1753 to December 31, 9999 with an accuracy of 3.33 milliseconds 8 bytes datetime2 From January 1, 0001 to December 31, 9999 with an accuracy of 100 nanoseconds 6-8 bytes smalldatetime From January 1, 1900 to June 6, 2079 with an accuracy of 1 minute 4 bytes date Store a date only. From January 1, 0001 to December 31, 9999 3 bytes time Store a time only to an accuracy of 100 nanoseconds 3-5 bytes datetimeoffset The same as datetime2 with the addition of a time zone offset 8-10 bytes timestamp Stores a unique number that gets updated every time a row gets created or modified. The timestamp value is based upon an internal clock and does not correspond to real time. Each table may have only one timestamp variable","title":"Date types"},{"location":"Databases/SQL/#other-data-types","text":"sql_variant Stores up to 8,000 bytes of data of various data types, except text, ntext, and timestamp uniqueidentifier Stores a globally unique identifier (GUID) xml Stores XML formatted data. Maximum 2GB cursor Stores a reference to a cursor used for database operations table Stores a result-set for later processing","title":"Other data types"},{"location":"Databases/SQL/#sql-aggregate-functions","text":"SQL aggregate functions return a single value, calculated from values in a column. Useful aggregate functions: AVG() - Returns the average value COUNT() - Returns the number of rows TOP 1 - Single sample MAX() - Returns the largest value MIN() - Returns the smallest value SUM() - Returns the sum Examples: SELECT COUNT ( DISTINCT column_name ) FROM table_name ; SELECT TOP 1 column_name FROM table_name ORDER BY column_name DESC ; SELECT column_name , aggregate_function ( column_name ) FROM table_name WHERE column_name operator value GROUP BY column_name ; SELECT Shippers . ShipperName , COUNT ( Orders . OrderID ) AS NumberOfOrders FROM Orders LEFT JOIN Shippers ON Orders . ShipperID = Shippers . ShipperID GROUP BY ShipperName ; SELECT column_name , aggregate_function ( column_name ) FROM table_name WHERE column_name operator value GROUP BY column_name HAVING aggregate_function ( column_name ) operator value ; SELECT Employees . LastName , COUNT ( Orders . OrderID ) AS NumberOfOrders FROM Orders INNER JOIN Employees ON Orders . EmployeeID = Employees . EmployeeID ) GROUP BY LastName HAVING COUNT ( Orders . OrderID ) > 10 ;","title":"SQL Aggregate Functions"},{"location":"Databases/SQL/#sql-scalar-functions","text":"Converts a field to upper case: SELECT UPPER(column_name) FROM table_name; Converts a field to lower case: SELECT LOWER(column_name) FROM table_name; MID() - Extract characters from a text field LEN() - Returns the length of a text field ROUND() - Rounds a numeric field to the number of decimals specified NOW() - Returns the current system date and time FORMAT() - Formats how a field is to be displayed SELECT ProductName , ROUND ( Price , 0 ) AS RoundedPrice FROM Products ;","title":"SQL Scalar functions"},{"location":"Databases/SQL/#variables","text":"DECLARE @ myvar char ( 20 ); SET @ myvar = 'This is a test' ; SELECT @ myvar ;","title":"Variables"},{"location":"Databases/SQL/#scalar-function","text":"CREATE FUNCTION FunctionName ( -- Add the parameters for the function here @ p1 int ) RETURNS int AS BEGIN -- Declare the return variable here DECLARE @ Result int -- Add the T-SQL statements to compute the return value here SELECT @ Result = @ p1 -- Return the result of the function RETURN @ Result END","title":"Scalar Function"},{"location":"Databases/SQL/#table-value-function","text":"IF OBJECT_ID ( N 'dbo.EmployeeByID' ) IS NOT NULL DROP FUNCTION dbo . EmployeeByID GO CREATE FUNCTION dbo . EmployeeByID ( @ InEmpID int ) RETURNS @ retFindReports TABLE ( -- columns returned by the function EmployeeID int NOT NULL , Name nvarchar ( 255 ) NOT NULL , Title nvarchar ( 50 ) NOT NULL , EmployeeLevel int NOT NULL ) AS -- body of the function BEGIN WITH DirectReports ( Name , Title , EmployeeID , EmployeeLevel , Sort ) AS ( SELECT CONVERT ( varchar ( 255 ), c . FirstName + ' ' + c . LastName ), e . Title , e . EmployeeID , 1 , CONVERT ( varchar ( 255 ), c . FirstName + ' ' + c . LastName ) FROM HumanResources . Employee AS e JOIN Person . Contact AS c ON e . ContactID = c . ContactID WHERE e . EmployeeID = @ InEmpID UNION ALL SELECT CONVERT ( varchar ( 255 ), REPLICATE ( '| ' , EmployeeLevel ) + c . FirstName + ' ' + c . LastName ), e . Title , e . EmployeeID , EmployeeLevel + 1 , CONVERT ( varchar ( 255 ), RTRIM ( Sort ) + '| ' + FirstName + ' ' + LastName ) FROM HumanResources . Employee as e JOIN Person . Contact AS c ON e . ContactID = c . ContactID JOIN DirectReports AS d ON e . ManagerID = d . EmployeeID ) -- copy the required columns to the result of the function INSERT @ retFindReports SELECT EmployeeID , Name , Title , EmployeeLevel FROM DirectReports ORDER BY Sort RETURN END GO","title":"Table Value Function"},{"location":"Databases/SQL/#stored-procedure","text":"CREATE PROCEDURE ProcedureName -- Add the parameters for the stored procedure here @ p1 int = 0 , @ p2 int = 0 AS BEGIN -- SET NOCOUNT ON added to prevent extra result sets from -- interfering with SELECT statements. SET NOCOUNT ON ; -- Insert statements for procedure here SELECT @ p1 , @ p2 END GO","title":"Stored Procedure"},{"location":"Databases/SQL/#self-join","text":"Q. Here's the data in a table 'orders' customer_id order_id order_day 123 27424624 25Dec2011 123 89690900 25Dec2010 797 12131323 25Dec2010 876 67145419 15Dec2011 Could you give me SQL for customers who placed orders on both the days, 25th Dec 2010 and 25th Dec 2011? SELECT o . customer_id , o . order_day FROM orders AS o INNER JOIN orders AS o1 ON o . customer_id = o1 . customer_id WHERE ...","title":"Self-join"},{"location":"DevOps/CloudFormation/","text":"DevOps Philosophy \u00b6 Why we use Terraform and not Chef, Puppet, Ansible, SaltStack, or CloudFormation Tools \u00b6 AWS CLI cloudformation aws cloudformation validate-template boto3 cloudformation YAML \u00b6 YAML Cheatsheet YAML Cheatsheet 2 YAML notation for folded text: > data : > Wrapped text will be folded into a single paragraph Blank lines denote paragraph breaks Sample Templates \u00b6 Templates for the US East (Northern Virginia) Region AWSlabs on GitHub - Startup kit templates - AWS CloudFormation Sample Templates Cloudonaut Templates Free Templates for AWS CloudFormation (Cloudonaut) Deploying Microservices with Amazon ECS, AWS CloudFormation, and an Application Load Balancer Template Basics \u00b6 Template Basics Template Anatomy --- AWSTemplateFormatVersion : \"version date\" Description : String Metadata : template metadata Parameters : set of parameters Mappings : set of mappings Conditions : set of conditions Transform : set of transforms Resources : set of resources Outputs : set of outputs With examples: --- AWSTemplateFormatVersion : \"2010-09-09\" Description : > Here are some details about the template. Metadata : Instances : Description : \"Information about the instances\" Databases : Description : \"Information about the databases\" Parameters : InstanceTypeParameter : Type : String # String, Number, List<Number>, CommaDelimitedList e.g. \"test,dev,prod\", or an AWS-specific types such as Amazon EC2 key pair names and VPC IDs. Default : t2.micro AllowedValues : - t2.micro - m1.small Description : Enter t2.micro or m1.small. Default is t2.micro. # AllowedPattern: \"[A-Za-z0-9]+\" # A regular expression that represents the patterns you want to allow for String types. # ConstraintDescription: Malformed input-Parameter MyParameter must match pattern [A-Za-z0-9]+ # MinLength: 2 # for String # MaxLength: 10 # MinValue: 0 # for Number types. # MaxValue: 100 # NoEcho: True Mappings : RegionMap : us-east-1 : \"32\" : \"ami-6411e20d\" us-west-1 : \"32\" : \"ami-c9c7978c\" eu-west-1 : \"32\" : \"ami-37c2f643\" ap-southeast-1 : \"32\" : \"ami-66f28c34\" ap-northeast-1 : \"32\" : \"ami-9c03a89d\" Conditions : CreateProdResources : !Equals [ !Ref EnvType , prod ] Transform : set of transforms Resources : Ec2Instance : Type : AWS::EC2::Instance Properties : InstanceType : Ref : InstanceTypeParameter # reference to parameter above ImageId : ami-2f726546 Outputs : VolumeId : Condition : CreateProdResources Value : !Ref NewVolume The Ref function can refer to input parameters that are specified at stack creation time. Examples \u00b6 S3 \u00b6 Resources : HelloBucket : Type : AWS::S3::Bucket # AWS::ProductIdentifier::ResourceType EC2 \u00b6 Resources : Ec2Instance : Type : AWS::EC2::Instance Properties : SecurityGroups : - Ref : InstanceSecurityGroup KeyName : mykey ImageId : '' InstanceSecurityGroup : Type : AWS::EC2::SecurityGroup Properties : GroupDescription : Enable SSH access via port 22 SecurityGroupIngress : - IpProtocol : tcp FromPort : '22' ToPort : '22' CidrIp : 0.0.0.0/0","title":"CloudFormation Basics"},{"location":"DevOps/CloudFormation/#devops-philosophy","text":"Why we use Terraform and not Chef, Puppet, Ansible, SaltStack, or CloudFormation","title":"DevOps Philosophy"},{"location":"DevOps/CloudFormation/#tools","text":"AWS CLI cloudformation aws cloudformation validate-template boto3 cloudformation","title":"Tools"},{"location":"DevOps/CloudFormation/#yaml","text":"YAML Cheatsheet YAML Cheatsheet 2 YAML notation for folded text: > data : > Wrapped text will be folded into a single paragraph Blank lines denote paragraph breaks","title":"YAML"},{"location":"DevOps/CloudFormation/#sample-templates","text":"Templates for the US East (Northern Virginia) Region AWSlabs on GitHub - Startup kit templates - AWS CloudFormation Sample Templates Cloudonaut Templates Free Templates for AWS CloudFormation (Cloudonaut) Deploying Microservices with Amazon ECS, AWS CloudFormation, and an Application Load Balancer","title":"Sample Templates"},{"location":"DevOps/CloudFormation/#template-basics","text":"Template Basics Template Anatomy --- AWSTemplateFormatVersion : \"version date\" Description : String Metadata : template metadata Parameters : set of parameters Mappings : set of mappings Conditions : set of conditions Transform : set of transforms Resources : set of resources Outputs : set of outputs With examples: --- AWSTemplateFormatVersion : \"2010-09-09\" Description : > Here are some details about the template. Metadata : Instances : Description : \"Information about the instances\" Databases : Description : \"Information about the databases\" Parameters : InstanceTypeParameter : Type : String # String, Number, List<Number>, CommaDelimitedList e.g. \"test,dev,prod\", or an AWS-specific types such as Amazon EC2 key pair names and VPC IDs. Default : t2.micro AllowedValues : - t2.micro - m1.small Description : Enter t2.micro or m1.small. Default is t2.micro. # AllowedPattern: \"[A-Za-z0-9]+\" # A regular expression that represents the patterns you want to allow for String types. # ConstraintDescription: Malformed input-Parameter MyParameter must match pattern [A-Za-z0-9]+ # MinLength: 2 # for String # MaxLength: 10 # MinValue: 0 # for Number types. # MaxValue: 100 # NoEcho: True Mappings : RegionMap : us-east-1 : \"32\" : \"ami-6411e20d\" us-west-1 : \"32\" : \"ami-c9c7978c\" eu-west-1 : \"32\" : \"ami-37c2f643\" ap-southeast-1 : \"32\" : \"ami-66f28c34\" ap-northeast-1 : \"32\" : \"ami-9c03a89d\" Conditions : CreateProdResources : !Equals [ !Ref EnvType , prod ] Transform : set of transforms Resources : Ec2Instance : Type : AWS::EC2::Instance Properties : InstanceType : Ref : InstanceTypeParameter # reference to parameter above ImageId : ami-2f726546 Outputs : VolumeId : Condition : CreateProdResources Value : !Ref NewVolume The Ref function can refer to input parameters that are specified at stack creation time.","title":"Template Basics"},{"location":"DevOps/CloudFormation/#examples","text":"","title":"Examples"},{"location":"DevOps/CloudFormation/#s3","text":"Resources : HelloBucket : Type : AWS::S3::Bucket # AWS::ProductIdentifier::ResourceType","title":"S3"},{"location":"DevOps/CloudFormation/#ec2","text":"Resources : Ec2Instance : Type : AWS::EC2::Instance Properties : SecurityGroups : - Ref : InstanceSecurityGroup KeyName : mykey ImageId : '' InstanceSecurityGroup : Type : AWS::EC2::SecurityGroup Properties : GroupDescription : Enable SSH access via port 22 SecurityGroupIngress : - IpProtocol : tcp FromPort : '22' ToPort : '22' CidrIp : 0.0.0.0/0","title":"EC2"},{"location":"DevOps/Git/","text":"Git Cheatsheets \u00b6 Graphical git cheatsheet Git basic commands Git cheatsheet (visual) Git cheatsheet (interactive) Git full documentation Repo hosting: * BitBucket * GitHub Common Commands \u00b6 Create a new Git repository in current directory: git init Or create an empty Git repository in the specified directory: git init <directory> Or copy an existing Git repository: git clone <repo URL> Clone the repository located at into the folder called on the local machine: git clone <repo> <directory> git clone username@host:/path/to/repository Global Configuration: $ git config --global user.name \"Firstname Lastname\" $ git config --global user.email \"your_email@youremail.com\" Stage all changes in <file> for the next commit: git add <file> Or stage all changes in <directory> for the next commit: git add <directory> # usually '.' for current directory Commit the staged snapshot to the project history: git commit # interactive git commit -m \"<message>\" Or add and commit all in one: git commit -am \"message\" Fix up the most recent commit (don't do that if shared history): git commit --amend List which files are staged, unstaged, and untracked: git status git status -s # short format Show file diff: git diff # git diff by itself doesn\u2019t show all changes made since your last commit \u2013 only changes that are still unstaged. git diff --staged # Shows file differences between staging and the last file version Open GUI: git gui Displays committed snapshots: git log -n <limit> git log --graph --decorate --oneline Checking out commits, and checking out branches: git checkout <commit> # Return to commit git checkout master # Return to the master branch (or whatever branch we choose) Check out a previous version of a file: git checkout <commit> <file> # Check out the version of the file from the selected commit git checkout HEAD hello.py # Check out the most recent version Branches \u00b6 Branches are just pointers to commits. List all of the branches in your repository. Also tell you what branch you're currently in ('*' branch): git branch Create a new branch called <branch> . git branch <branch> This does not check out the new branch. You need: git checkout <existing-branch> Or direcly create-and-check out <new-branch> . git checkout -b <new-branch> Safe delete the branch: git branch -d <branch> Merge the specified branch into the current branch: git merge <branch> Undo any undesired changes Generate a new commit that undoes all of the changes introduced in <commit> , then apply it to the current branch git revert <commit> git revert undoes a single commit \u2014 it does not \u201crevert\u201d back to the previous state of a project by removing all subsequent commits. Reset (dangerous method - erases history): git reset List the remote connections you have to other repositories. git remote -v Create a new connection / delete a connection to a remote repository. git remote add <name> <url> # often \"origin\" git remote rm <name> # delete Fetch the specified remote\u2019s copy of the current branch and immediately merge it into the local copy. This is the same as git fetch <remote> followed by git merge origin/<current-branch> . git pull <remote> Put my changes on top of what everybody else has done. Ensure a linear history by preventing unnecessary merge commits. git pull --rebase <remote> Transfer commits from your local repository to a remote repo. git push <remote> <branch> Pushes the current branch to the remote server and links the local branch to the remote so next time you can do git pull or git push . git push -u origin <branch> Typical Workflows \u00b6 Clone a Repo \u00b6 $ mkdir repos $ cd ~/repos $ git clone https://<url> $ ls -al <repo dir> Add a change in the working directory to the staging area \u00b6 $ git status $ git add README -A , --all finds new files as well as staging modified content and removing files that are no longer in the working tree. $ git add -A $ git commit -m \"Add repo instructions\" $ git push -u origin master $ git pull $ ssh -p 2222 user@domain.com Short-lived topic branches \u00b6 Start a new feature: git checkout -b new-feature master Edit some files: git add <file> git commit -m \"Start a feature\" Edit some files git add <file> git commit -m \"Finish a feature\" Merge in the new-feature branch git checkout master git merge new-feature git branch -d new-feature Push and pull from a centralized repo \u00b6 To push the master branch to the central repo: git push origin master If local history has diverged from the central repository, Git will refuse the request. git pull --rebase origin master Sync my local repo with the remote repo \u00b6 git pull origin master git add filename.xyz git commit . -m \u201ccomment\u201d git push origin master Create a central Repo \u00b6 The --bare flag creates a repository that doesn\u2019t have a working directory, making it impossible to edit files and commit changes in that repository. Central repositories should always be created as bare repositories because pushing branches to a non-bare repository has the potential to overwrite changes. $ git init --bare foobar.git $ git rev-parse --show-toplevel # print top-level directory $ git rev-parse --git-dir # print .git directory name","title":"Git Cheatsheet"},{"location":"DevOps/Git/#git-cheatsheets","text":"Graphical git cheatsheet Git basic commands Git cheatsheet (visual) Git cheatsheet (interactive) Git full documentation Repo hosting: * BitBucket * GitHub","title":"Git Cheatsheets"},{"location":"DevOps/Git/#common-commands","text":"Create a new Git repository in current directory: git init Or create an empty Git repository in the specified directory: git init <directory> Or copy an existing Git repository: git clone <repo URL> Clone the repository located at into the folder called on the local machine: git clone <repo> <directory> git clone username@host:/path/to/repository Global Configuration: $ git config --global user.name \"Firstname Lastname\" $ git config --global user.email \"your_email@youremail.com\" Stage all changes in <file> for the next commit: git add <file> Or stage all changes in <directory> for the next commit: git add <directory> # usually '.' for current directory Commit the staged snapshot to the project history: git commit # interactive git commit -m \"<message>\" Or add and commit all in one: git commit -am \"message\" Fix up the most recent commit (don't do that if shared history): git commit --amend List which files are staged, unstaged, and untracked: git status git status -s # short format Show file diff: git diff # git diff by itself doesn\u2019t show all changes made since your last commit \u2013 only changes that are still unstaged. git diff --staged # Shows file differences between staging and the last file version Open GUI: git gui Displays committed snapshots: git log -n <limit> git log --graph --decorate --oneline Checking out commits, and checking out branches: git checkout <commit> # Return to commit git checkout master # Return to the master branch (or whatever branch we choose) Check out a previous version of a file: git checkout <commit> <file> # Check out the version of the file from the selected commit git checkout HEAD hello.py # Check out the most recent version","title":"Common Commands"},{"location":"DevOps/Git/#branches","text":"Branches are just pointers to commits. List all of the branches in your repository. Also tell you what branch you're currently in ('*' branch): git branch Create a new branch called <branch> . git branch <branch> This does not check out the new branch. You need: git checkout <existing-branch> Or direcly create-and-check out <new-branch> . git checkout -b <new-branch> Safe delete the branch: git branch -d <branch> Merge the specified branch into the current branch: git merge <branch> Undo any undesired changes Generate a new commit that undoes all of the changes introduced in <commit> , then apply it to the current branch git revert <commit> git revert undoes a single commit \u2014 it does not \u201crevert\u201d back to the previous state of a project by removing all subsequent commits. Reset (dangerous method - erases history): git reset List the remote connections you have to other repositories. git remote -v Create a new connection / delete a connection to a remote repository. git remote add <name> <url> # often \"origin\" git remote rm <name> # delete Fetch the specified remote\u2019s copy of the current branch and immediately merge it into the local copy. This is the same as git fetch <remote> followed by git merge origin/<current-branch> . git pull <remote> Put my changes on top of what everybody else has done. Ensure a linear history by preventing unnecessary merge commits. git pull --rebase <remote> Transfer commits from your local repository to a remote repo. git push <remote> <branch> Pushes the current branch to the remote server and links the local branch to the remote so next time you can do git pull or git push . git push -u origin <branch>","title":"Branches"},{"location":"DevOps/Git/#typical-workflows","text":"","title":"Typical Workflows"},{"location":"DevOps/Git/#clone-a-repo","text":"$ mkdir repos $ cd ~/repos $ git clone https://<url> $ ls -al <repo dir>","title":"Clone a Repo"},{"location":"DevOps/Git/#add-a-change-in-the-working-directory-to-the-staging-area","text":"$ git status $ git add README -A , --all finds new files as well as staging modified content and removing files that are no longer in the working tree. $ git add -A $ git commit -m \"Add repo instructions\" $ git push -u origin master $ git pull $ ssh -p 2222 user@domain.com","title":"Add a change in the working directory to the staging area"},{"location":"DevOps/Git/#short-lived-topic-branches","text":"Start a new feature: git checkout -b new-feature master Edit some files: git add <file> git commit -m \"Start a feature\" Edit some files git add <file> git commit -m \"Finish a feature\" Merge in the new-feature branch git checkout master git merge new-feature git branch -d new-feature","title":"Short-lived topic branches"},{"location":"DevOps/Git/#push-and-pull-from-a-centralized-repo","text":"To push the master branch to the central repo: git push origin master If local history has diverged from the central repository, Git will refuse the request. git pull --rebase origin master","title":"Push and pull from a centralized repo"},{"location":"DevOps/Git/#sync-my-local-repo-with-the-remote-repo","text":"git pull origin master git add filename.xyz git commit . -m \u201ccomment\u201d git push origin master","title":"Sync my local repo with the remote repo"},{"location":"DevOps/Git/#create-a-central-repo","text":"The --bare flag creates a repository that doesn\u2019t have a working directory, making it impossible to edit files and commit changes in that repository. Central repositories should always be created as bare repositories because pushing branches to a non-bare repository has the potential to overwrite changes. $ git init --bare foobar.git $ git rev-parse --show-toplevel # print top-level directory $ git rev-parse --git-dir # print .git directory name","title":"Create a central Repo"},{"location":"DevOps/Orchestrator_Scheduler/","text":"Orchestrators / Schedulers \u00b6 Tools to build complex pipelines of batch jobs. They handle dependency resolution, workflow management, visualization. Links \u00b6 Luigi vs Airflow vs Pinball Airflow Documentation Luigi Petabyte-Scale Data Pipelines with Docker, Luigi and Elastic Spot Instances Snowplow","title":"Orchestrators / Schedulers"},{"location":"DevOps/Orchestrator_Scheduler/#orchestrators-schedulers","text":"Tools to build complex pipelines of batch jobs. They handle dependency resolution, workflow management, visualization.","title":"Orchestrators / Schedulers"},{"location":"DevOps/Orchestrator_Scheduler/#links","text":"Luigi vs Airflow vs Pinball Airflow Documentation Luigi Petabyte-Scale Data Pipelines with Docker, Luigi and Elastic Spot Instances Snowplow","title":"Links"},{"location":"Java/Java/","text":"Install Java \u00b6 JDK download java -version Java Tools \u00b6 List 1 List 2 Eclipse IDE Maven or Graddle build tool Nexus private repository Maven public repository Phabrikator code review Phabrikator blog Jenkins CI / CD automation server JProfiler FindBugs static analysis or Checker Framework Checkstyle coding standard checker Style guidelines Java Libraries \u00b6 Libraries Log4j Spring Spring Cloud for Amazon Web Services Spring boot code generator Apache Commons Guava Jackson JSON or GSON Hibernate on the JVM. Play framework Spark web microframework Akka - actor model, to build highly concurrent, distributed, and resilient message-driven applications Random Snippets \u00b6 UUID . randomUUID (). toString ();","title":"Java Pointers"},{"location":"Java/Java/#install-java","text":"JDK download java -version","title":"Install Java"},{"location":"Java/Java/#java-tools","text":"List 1 List 2 Eclipse IDE Maven or Graddle build tool Nexus private repository Maven public repository Phabrikator code review Phabrikator blog Jenkins CI / CD automation server JProfiler FindBugs static analysis or Checker Framework Checkstyle coding standard checker Style guidelines","title":"Java Tools"},{"location":"Java/Java/#java-libraries","text":"Libraries Log4j Spring Spring Cloud for Amazon Web Services Spring boot code generator Apache Commons Guava Jackson JSON or GSON Hibernate on the JVM. Play framework Spark web microframework Akka - actor model, to build highly concurrent, distributed, and resilient message-driven applications","title":"Java Libraries"},{"location":"Java/Java/#random-snippets","text":"UUID . randomUUID (). toString ();","title":"Random Snippets"},{"location":"Java/Log4j/","text":"Apache Log4j 2 \u00b6 Log4j quick guide Key Components \u00b6 loggers: Responsible for capturing logging information. appenders: Responsible for publishing logging information to various preferred destinations. layouts: Responsible for formatting logging information in different styles. There are seven levels of logging defined within the API: OFF, DEBUG, INFO, ERROR, WARN, FATAL, and ALL. Install \u00b6 Download Log4j $ gunzip apache-log4j-1.2.15.tar.gz $ tar -xvf apache-log4j-1.2.15.tar $ pwd /usr/local/apache-log4j-1.2.15 $ export CLASSPATH = $CLASSPATH :/usr/local/apache-log4j-1.2.15/log4j-1.2.15.jar $ export PATH = $PATH :/usr/local/apache-log4j-1.2.15/ Maven Snippet \u00b6 <dependencies> <dependency> <groupId>org.apache.logging.log4j</groupId> <artifactId>log4j-api</artifactId> <version>2.6.1</version> </dependency> <dependency> <groupId>org.apache.logging.log4j</groupId> <artifactId>log4j-core</artifactId> <version>2.6.1</version> </dependency> </dependencies> log4j.properties \u00b6 All the libraries should be available in CLASSPATH and yourlog4j.properties file should be available in PATH. # Define the root logger with appender file log = /usr/home/log4j log4j.rootLogger = WARN, FILE # Define the file appender log4j.appender.FILE = org.apache.log4j.FileAppender log4j.appender.FILE.File = ${log}/log.out # Define the layout for file appender log4j.appender.FILE.layout = org.apache.log4j.PatternLayout log4j.appender.FILE.layout.conversionPattern = %m%n Snippets \u00b6 import org.apache.logging.log4j.LogManager ; import org.apache.logging.log4j.Logger ; public class MyTest { private static final Logger logger = LogManager . getLogger (); // equiv to LogManager.getLogger(MyTest.class); private static final Logger logger = LogManager . getLogger ( \"HelloWorld\" ); public static void main ( String [] args ) { logger . setLevel ( Level . WARN ); logger . info ( \"Hello, World!\" ); // string interpolation logger . debug ( \"Logging in user {} with birthday {}\" , user . getName (), user . getBirthdayCalendar ()); // pre-Java 8 style optimization: explicitly check the log level // to make sure the expensiveOperation() method is only called if necessary if ( logger . isTraceEnabled ()) { logger . trace ( \"Some long-running operation returned {}\" , expensiveOperation ()); } // Java-8 style optimization: no need to explicitly check the log level: // the lambda expression is not evaluated if the TRACE level is not enabledlogger.trace(\"Some long-running operation returned {}\", () -> expensiveOperation()); } } // FORMATTER LOGGER public static Logger logger = LogManager . getFormatterLogger ( \"Foo\" ); logger . debug ( \"Logging in user %s with birthday %s\" , user . getName (), user . getBirthdayCalendar ()); logger . debug ( \"Logging in user %1$s with birthday %2$tm %2$te,%2$tY\" , user . getName (), user . getBirthdayCalendar ()); // logger . debug ( \"Logging in user {} with birthday {}\" , user . getName (), user . getBirthdayCalendar ());","title":"Apache Log4j 2"},{"location":"Java/Log4j/#apache-log4j-2","text":"Log4j quick guide","title":"Apache Log4j 2"},{"location":"Java/Log4j/#key-components","text":"loggers: Responsible for capturing logging information. appenders: Responsible for publishing logging information to various preferred destinations. layouts: Responsible for formatting logging information in different styles. There are seven levels of logging defined within the API: OFF, DEBUG, INFO, ERROR, WARN, FATAL, and ALL.","title":"Key Components"},{"location":"Java/Log4j/#install","text":"Download Log4j $ gunzip apache-log4j-1.2.15.tar.gz $ tar -xvf apache-log4j-1.2.15.tar $ pwd /usr/local/apache-log4j-1.2.15 $ export CLASSPATH = $CLASSPATH :/usr/local/apache-log4j-1.2.15/log4j-1.2.15.jar $ export PATH = $PATH :/usr/local/apache-log4j-1.2.15/","title":"Install"},{"location":"Java/Log4j/#maven-snippet","text":"<dependencies> <dependency> <groupId>org.apache.logging.log4j</groupId> <artifactId>log4j-api</artifactId> <version>2.6.1</version> </dependency> <dependency> <groupId>org.apache.logging.log4j</groupId> <artifactId>log4j-core</artifactId> <version>2.6.1</version> </dependency> </dependencies>","title":"Maven Snippet"},{"location":"Java/Log4j/#log4jproperties","text":"All the libraries should be available in CLASSPATH and yourlog4j.properties file should be available in PATH. # Define the root logger with appender file log = /usr/home/log4j log4j.rootLogger = WARN, FILE # Define the file appender log4j.appender.FILE = org.apache.log4j.FileAppender log4j.appender.FILE.File = ${log}/log.out # Define the layout for file appender log4j.appender.FILE.layout = org.apache.log4j.PatternLayout log4j.appender.FILE.layout.conversionPattern = %m%n","title":"log4j.properties"},{"location":"Java/Log4j/#snippets","text":"import org.apache.logging.log4j.LogManager ; import org.apache.logging.log4j.Logger ; public class MyTest { private static final Logger logger = LogManager . getLogger (); // equiv to LogManager.getLogger(MyTest.class); private static final Logger logger = LogManager . getLogger ( \"HelloWorld\" ); public static void main ( String [] args ) { logger . setLevel ( Level . WARN ); logger . info ( \"Hello, World!\" ); // string interpolation logger . debug ( \"Logging in user {} with birthday {}\" , user . getName (), user . getBirthdayCalendar ()); // pre-Java 8 style optimization: explicitly check the log level // to make sure the expensiveOperation() method is only called if necessary if ( logger . isTraceEnabled ()) { logger . trace ( \"Some long-running operation returned {}\" , expensiveOperation ()); } // Java-8 style optimization: no need to explicitly check the log level: // the lambda expression is not evaluated if the TRACE level is not enabledlogger.trace(\"Some long-running operation returned {}\", () -> expensiveOperation()); } } // FORMATTER LOGGER public static Logger logger = LogManager . getFormatterLogger ( \"Foo\" ); logger . debug ( \"Logging in user %s with birthday %s\" , user . getName (), user . getBirthdayCalendar ()); logger . debug ( \"Logging in user %1$s with birthday %2$tm %2$te,%2$tY\" , user . getName (), user . getBirthdayCalendar ()); // logger . debug ( \"Logging in user {} with birthday {}\" , user . getName (), user . getBirthdayCalendar ());","title":"Snippets"},{"location":"Java/Maven/","text":"Useful Links \u00b6 http://maven.apache.org/ Maven Cheatsheet Maven Quick Ref Apache-maven-2 Maven Basics Maven Download / Install Nexus Basics \u00b6 Install: cd /usr/local ln -s apache-maven-3.0.5 maven export PATH = /usr/local/maven/bin: $PATH mvn -v Settings file: ~/.m2/settings.xml It contains user-specific configuration for authentication, repositories, and other information to customize the behavior of Maven. Maven Repo: ~/.m2/repository/ This directory contains your local Maven repository. When you download a dependency from a remote Maven repository, Maven stores a copy of the dependency in your local repository. Directory Layout \u00b6 Introduction to the standard directory layout Without customization, source code is assumed to be in ${basedir}/src/main/java and resources are assumed to be in ${basedir}/src/main/resources . Tests are assumed to be in ${basedir}/src/test , and a project is assumed to produce a JAR file. Maven assumes that you want the compile bytecode to ${basedir}/target/classes and then create a distributable JAR file in ${basedir}/target For WAR files , the /WEB-INF directory contains a file named web.xml which defines the structure of the web application. See also Tomcat Deployment guide Cheatsheet \u00b6 Bring up a menu of choices mvn archetype:generate -DgroupId = com.dw -DartifactId = es-demo -DarchetypeArtifactId = maven-archetype-quickstart -DinteractiveMode = false Create a Java project mvn archetype:create -DgroupId = org.yourcompany.project -DartifactId = application Create a web project mvn archetype:create -DgroupId = org.yourcompany.project -DartifactId = application -DarchetypeArtifactId = maven-archetype-webapp Clean project (will delete target directory) mvn clean Validate project (validate the project is correct and all necessary information is available) mvn validate Compile project (compile source code, classes stored in target/classes) mvn compile Test project (run tests using a suitable unit testing framework) mvn test Package project (take the compiled code and package it in its distributable format, such as a JAR / WAR) mvn package Verify project (run any checks to verify the package is valid and meets quality criteria) mvn verify Install project (install the package into the local repository, for use as a dependency in other projects locally) mvn install mvn clean install -DskipTests -Dmaven.javadoc.skip = true Deploy project (done in an integration or release environment, copies the final package to the remote repository for sharing with other developers and projects) mvn deploy Deploy-file (can be used for deploying a external jar file to repository) mvn deploy:deploy-file -Dfile = /path/to/jar/file -DrepositoryId = repos-server -Durl = http ://repos.company.o You can run mvn site and then find an index.html file in target/site that contains links to JavaDoc and a few reports about your source code. POM files \u00b6 Use the search engine at repository.sonatype.org to find dependencies by name and get the xml necessary to paste into your pom.xml <project> <modelVersion> 4.0.0 </modelVersion> <groupId> org.sonatype.mavenbook </groupId> <artifactId> my-project </artifactId> <version> 1.0-SNAPSHOT </version> </project>","title":"Maven"},{"location":"Java/Maven/#useful-links","text":"http://maven.apache.org/ Maven Cheatsheet Maven Quick Ref Apache-maven-2 Maven Basics Maven Download / Install Nexus","title":"Useful Links"},{"location":"Java/Maven/#basics","text":"Install: cd /usr/local ln -s apache-maven-3.0.5 maven export PATH = /usr/local/maven/bin: $PATH mvn -v Settings file: ~/.m2/settings.xml It contains user-specific configuration for authentication, repositories, and other information to customize the behavior of Maven. Maven Repo: ~/.m2/repository/ This directory contains your local Maven repository. When you download a dependency from a remote Maven repository, Maven stores a copy of the dependency in your local repository.","title":"Basics"},{"location":"Java/Maven/#directory-layout","text":"Introduction to the standard directory layout Without customization, source code is assumed to be in ${basedir}/src/main/java and resources are assumed to be in ${basedir}/src/main/resources . Tests are assumed to be in ${basedir}/src/test , and a project is assumed to produce a JAR file. Maven assumes that you want the compile bytecode to ${basedir}/target/classes and then create a distributable JAR file in ${basedir}/target For WAR files , the /WEB-INF directory contains a file named web.xml which defines the structure of the web application. See also Tomcat Deployment guide","title":"Directory Layout"},{"location":"Java/Maven/#cheatsheet","text":"Bring up a menu of choices mvn archetype:generate -DgroupId = com.dw -DartifactId = es-demo -DarchetypeArtifactId = maven-archetype-quickstart -DinteractiveMode = false Create a Java project mvn archetype:create -DgroupId = org.yourcompany.project -DartifactId = application Create a web project mvn archetype:create -DgroupId = org.yourcompany.project -DartifactId = application -DarchetypeArtifactId = maven-archetype-webapp Clean project (will delete target directory) mvn clean Validate project (validate the project is correct and all necessary information is available) mvn validate Compile project (compile source code, classes stored in target/classes) mvn compile Test project (run tests using a suitable unit testing framework) mvn test Package project (take the compiled code and package it in its distributable format, such as a JAR / WAR) mvn package Verify project (run any checks to verify the package is valid and meets quality criteria) mvn verify Install project (install the package into the local repository, for use as a dependency in other projects locally) mvn install mvn clean install -DskipTests -Dmaven.javadoc.skip = true Deploy project (done in an integration or release environment, copies the final package to the remote repository for sharing with other developers and projects) mvn deploy Deploy-file (can be used for deploying a external jar file to repository) mvn deploy:deploy-file -Dfile = /path/to/jar/file -DrepositoryId = repos-server -Durl = http ://repos.company.o You can run mvn site and then find an index.html file in target/site that contains links to JavaDoc and a few reports about your source code.","title":"Cheatsheet"},{"location":"Java/Maven/#pom-files","text":"Use the search engine at repository.sonatype.org to find dependencies by name and get the xml necessary to paste into your pom.xml <project> <modelVersion> 4.0.0 </modelVersion> <groupId> org.sonatype.mavenbook </groupId> <artifactId> my-project </artifactId> <version> 1.0-SNAPSHOT </version> </project>","title":"POM files"},{"location":"Java/Spring/","text":"Spring \u00b6 All Spring beans are managed - they \"live\" inside a container, called \"application context\". Second, each application has an entry point to that context. Web applications have a Servlet, JSFuses a el-resolver, etc. Also, there is a place where the application context is bootstrapped and all beans - autowired. In web applications this can be a startup listener. Autowiring happens by placing an instance of one bean into the desired field in an instance of another bean. Both classes should be beans, i.e. they should be defined to live in the application context. What is \"living\" in the application context? This means that the context instantiates the objects, not you. I.e. - you never make new UserServiceImpl() - the container finds each injection point and sets an instance there. Spring and AWS \u00b6 A New Way of Using Email for Support Apps: An AWS Tutorial","title":"Spring"},{"location":"Java/Spring/#spring","text":"All Spring beans are managed - they \"live\" inside a container, called \"application context\". Second, each application has an entry point to that context. Web applications have a Servlet, JSFuses a el-resolver, etc. Also, there is a place where the application context is bootstrapped and all beans - autowired. In web applications this can be a startup listener. Autowiring happens by placing an instance of one bean into the desired field in an instance of another bean. Both classes should be beans, i.e. they should be defined to live in the application context. What is \"living\" in the application context? This means that the context instantiates the objects, not you. I.e. - you never make new UserServiceImpl() - the container finds each injection point and sets an instance there.","title":"Spring"},{"location":"Java/Spring/#spring-and-aws","text":"A New Way of Using Email for Support Apps: An AWS Tutorial","title":"Spring and AWS"},{"location":"Linux/Linux/","text":"Vim \u00b6 Vim Commands Cheat Sheet :q :q! :wq :wq {file} :e[dit] {file} i insert dd delete [count] lines Bash \u00b6 BASH Programming - Introduction Bash CheatSheet for UNIX Systems Bash Cheat Sheet #!/bin/bash varname = value echo $varname Don't forget chmod +x filename Amazon Linux \u00b6 Amazon Linux Basics Adding Packages \u00b6 sudo yum update -y # all packages sudo yum install -y package_name sudo yum install -y httpd24 php56 mysql55-server php56-mysqlnd Start a Service \u00b6 sudo service docker start sudo service jenkins start sudo service nginx start Autostarting a service on Amazon Linux \u00b6 Tutorial on \"Chkconfig\" Command in Linux with Examples man page # check a service is configured for startup sudo chkconfig sshd echo $? # 0 = configured for startup # or sudo chkconfig --list mysqld sudo chkconfig --list # all services # add a service sudo chkconfig --add vsftpd sudo chkconfig mysqld on sudo chkconfig --level 3 httpd on # specific runlevel Linux Boot Process \u00b6 Linux Boot Process Scripts in /etc/init.d You can also use a /etc/rc.d/rc.local script. Running Commands on your Linux Instance at Launch \u00b6 Paste a user data script into the User data field #!/bin/bash yum update -y yum install -y httpd24 php56 mysql55-server php56-mysqlnd service httpd start chkconfig httpd on groupadd www usermod -a -G www ec2-user chown -R root:www /var/www chmod 2775 /var/www find /var/www -type d -exec chmod 2775 {} + find /var/www -type f -exec chmod 0664 {} + echo \"<?php phpinfo(); ?>\" > /var/www/html/phpinfo.php Or use cloud-init cloud-init for AWS EC2 cloud-init docs File location: /etc/sysconfig/cloudinit Cloud-init output log file: /var/log/cloud-init-output.log Install the SSM Agent on EC2 Instances at Start-Up \u00b6 #!/bin/bash cd /tmp curl https://amazon-ssm-region.s3.amazonaws.com/latest/linux_amd64/amazon-ssm-agent.rpm -o amazon-ssm-agent.rpm yum install -y amazon-ssm-agent.rpm Linux desktop \u00b6 How can I connect to an Amazon EC2 Linux instance with desktop functionality from Windows?","title":"Linux Cheatsheet"},{"location":"Linux/Linux/#vim","text":"Vim Commands Cheat Sheet :q :q! :wq :wq {file} :e[dit] {file} i insert dd delete [count] lines","title":"Vim"},{"location":"Linux/Linux/#bash","text":"BASH Programming - Introduction Bash CheatSheet for UNIX Systems Bash Cheat Sheet #!/bin/bash varname = value echo $varname Don't forget chmod +x filename","title":"Bash"},{"location":"Linux/Linux/#amazon-linux","text":"Amazon Linux Basics","title":"Amazon Linux"},{"location":"Linux/Linux/#adding-packages","text":"sudo yum update -y # all packages sudo yum install -y package_name sudo yum install -y httpd24 php56 mysql55-server php56-mysqlnd","title":"Adding Packages"},{"location":"Linux/Linux/#start-a-service","text":"sudo service docker start sudo service jenkins start sudo service nginx start","title":"Start a Service"},{"location":"Linux/Linux/#autostarting-a-service-on-amazon-linux","text":"Tutorial on \"Chkconfig\" Command in Linux with Examples man page # check a service is configured for startup sudo chkconfig sshd echo $? # 0 = configured for startup # or sudo chkconfig --list mysqld sudo chkconfig --list # all services # add a service sudo chkconfig --add vsftpd sudo chkconfig mysqld on sudo chkconfig --level 3 httpd on # specific runlevel","title":"Autostarting a service on Amazon Linux"},{"location":"Linux/Linux/#linux-boot-process","text":"Linux Boot Process Scripts in /etc/init.d You can also use a /etc/rc.d/rc.local script.","title":"Linux Boot Process"},{"location":"Linux/Linux/#running-commands-on-your-linux-instance-at-launch","text":"Paste a user data script into the User data field #!/bin/bash yum update -y yum install -y httpd24 php56 mysql55-server php56-mysqlnd service httpd start chkconfig httpd on groupadd www usermod -a -G www ec2-user chown -R root:www /var/www chmod 2775 /var/www find /var/www -type d -exec chmod 2775 {} + find /var/www -type f -exec chmod 0664 {} + echo \"<?php phpinfo(); ?>\" > /var/www/html/phpinfo.php Or use cloud-init cloud-init for AWS EC2 cloud-init docs File location: /etc/sysconfig/cloudinit Cloud-init output log file: /var/log/cloud-init-output.log","title":"Running Commands on your Linux Instance at Launch"},{"location":"Linux/Linux/#install-the-ssm-agent-on-ec2-instances-at-start-up","text":"#!/bin/bash cd /tmp curl https://amazon-ssm-region.s3.amazonaws.com/latest/linux_amd64/amazon-ssm-agent.rpm -o amazon-ssm-agent.rpm yum install -y amazon-ssm-agent.rpm","title":"Install the SSM Agent on EC2 Instances at Start-Up"},{"location":"Linux/Linux/#linux-desktop","text":"How can I connect to an Amazon EC2 Linux instance with desktop functionality from Windows?","title":"Linux desktop"},{"location":"Linux/Virtualization/","text":"Virtualization \u00b6 Comparison of platform virtual machines Linux KVM Xen VMware ESX vSphere","title":"Virtualization"},{"location":"Linux/Virtualization/#virtualization","text":"Comparison of platform virtual machines Linux KVM Xen VMware ESX vSphere","title":"Virtualization"},{"location":"Markup_and_Documentation/Jekyll/","text":"Jekyll Basics \u00b6 Jekyll Home Page Check out the Jekyll docs for more info on how to get the most out of Jekyll. File all bugs/feature requests at Jekyll\u2019s GitHub repo . If you have questions, you can ask them on Jekyll Talk . Jekyll source code Guide to basic Jekyll Jekyll Install How-To \u00b6 Install Instructions Install Ruby via RubyInstaller Update RubyGems $ gem update --system Install Jekyll $ gem install jekyll Test Jekyll $ jekyll --version $ gem list jekyll Install bundler $ gem install bundler Bundler is a gem that manages other Ruby gems. It makes sure your gems and gem versions are compatible, and that you have all necessary dependencies each gem requires. Create a new site # Create a new Jekyll site at ./myblog ~ $ jekyll new myblog # Change into your new directory ~ $ cd myblog Jekyll installs a site that uses a gem-based theme called Minima. With gem-based themes, some of the site\u2019s directories (such as the assets, _layouts, _includes, and _sass directories) are stored in the theme\u2019s gem, hidden from your immediate view. Yet all of the necessary directories will be read and processed during Jekyll\u2019s build process. Build site locally # Build the site on the preview server ~/myblog $ bundle exec jekyll serve Now browse to localhost:4000 Jekyll Quickstart When you run bundle exec jekyll serve, Bundler uses the gems and versions as specified in Gemfile.lock to ensure your Jekyll site builds with no compatibility or dependency conflicts. The Gemfile and Gemfile.lock files inform Bundler about the gem requirements in your site. If your site doesn\u2019t have these Gemfiles, you can omit bundle exec and just run jekyll serve. $ jekyll build # => The current folder will be generated into ./_site $ jekyll serve # => A development server will run at http://localhost:4000/ # Auto-regeneration: enabled. Use `--no-watch` to disable. Plugins \u00b6 $ gem install jekyll-sitemap $ gem install jekyll-feed etc... Add to _config.yml gems: - jekyll-paginate - jekyll-feed - jekyll-sitemap `` # Custom Search [Adding a custom Google search](http://digitaldrummerj.me/blogging-on-github-part-7-adding-a-custom-google-search/) # Themes [Theme documentation](https://jekyllrb.com/docs/themes/) To change theme, search for jekyll theme on [RubyGems](https://rubygems.org/search?utf8=%E2%9C%93&query=jekyll-theme) to find other gem-based themes. Add the theme to your site\u2019s Gemfile: gem \"jekyll-theme-tactile\" ```bash $ bundle install # check proper install $ bundle show jekyll-theme-tactile Add the following to your site\u2019s _config.yml to activate the theme: theme: jekyll-theme-tactile Build your site: $ bundle exec jekyll serve You can find out info about customizing your Jekyll theme, as well as basic Jekyll usage documentation at jekyllrb.com You can find the source code for the Jekyll minima theme at: minima You\u2019ll find this post in your _posts directory. Go ahead and edit it and re-build the site to see your changes. You can rebuild the site in many different ways, but the most common way is to run jekyll serve , which launches a web server and auto-regenerates your site when a file is updated. To add new posts, simply add a file in the _posts directory that follows the convention YYYY-MM-DD-name-of-post.ext and includes the necessary front matter. Take a look at the source for this post to get an idea about how it works. Jekyll also offers powerful support for code snippets: {% highlight ruby %} def print_hi(name) puts \"Hi, #{name}\" end print_hi('Tom') => prints 'Hi, Tom' to STDOUT. \u00b6 {% endhighlight %}","title":"Jekyll How-To"},{"location":"Markup_and_Documentation/Jekyll/#jekyll-basics","text":"Jekyll Home Page Check out the Jekyll docs for more info on how to get the most out of Jekyll. File all bugs/feature requests at Jekyll\u2019s GitHub repo . If you have questions, you can ask them on Jekyll Talk . Jekyll source code Guide to basic Jekyll","title":"Jekyll Basics"},{"location":"Markup_and_Documentation/Jekyll/#jekyll-install-how-to","text":"Install Instructions Install Ruby via RubyInstaller Update RubyGems $ gem update --system Install Jekyll $ gem install jekyll Test Jekyll $ jekyll --version $ gem list jekyll Install bundler $ gem install bundler Bundler is a gem that manages other Ruby gems. It makes sure your gems and gem versions are compatible, and that you have all necessary dependencies each gem requires. Create a new site # Create a new Jekyll site at ./myblog ~ $ jekyll new myblog # Change into your new directory ~ $ cd myblog Jekyll installs a site that uses a gem-based theme called Minima. With gem-based themes, some of the site\u2019s directories (such as the assets, _layouts, _includes, and _sass directories) are stored in the theme\u2019s gem, hidden from your immediate view. Yet all of the necessary directories will be read and processed during Jekyll\u2019s build process. Build site locally # Build the site on the preview server ~/myblog $ bundle exec jekyll serve Now browse to localhost:4000 Jekyll Quickstart When you run bundle exec jekyll serve, Bundler uses the gems and versions as specified in Gemfile.lock to ensure your Jekyll site builds with no compatibility or dependency conflicts. The Gemfile and Gemfile.lock files inform Bundler about the gem requirements in your site. If your site doesn\u2019t have these Gemfiles, you can omit bundle exec and just run jekyll serve. $ jekyll build # => The current folder will be generated into ./_site $ jekyll serve # => A development server will run at http://localhost:4000/ # Auto-regeneration: enabled. Use `--no-watch` to disable.","title":"Jekyll Install How-To"},{"location":"Markup_and_Documentation/Jekyll/#plugins","text":"$ gem install jekyll-sitemap $ gem install jekyll-feed etc... Add to _config.yml gems: - jekyll-paginate - jekyll-feed - jekyll-sitemap `` # Custom Search [Adding a custom Google search](http://digitaldrummerj.me/blogging-on-github-part-7-adding-a-custom-google-search/) # Themes [Theme documentation](https://jekyllrb.com/docs/themes/) To change theme, search for jekyll theme on [RubyGems](https://rubygems.org/search?utf8=%E2%9C%93&query=jekyll-theme) to find other gem-based themes. Add the theme to your site\u2019s Gemfile: gem \"jekyll-theme-tactile\" ```bash $ bundle install # check proper install $ bundle show jekyll-theme-tactile Add the following to your site\u2019s _config.yml to activate the theme: theme: jekyll-theme-tactile Build your site: $ bundle exec jekyll serve You can find out info about customizing your Jekyll theme, as well as basic Jekyll usage documentation at jekyllrb.com You can find the source code for the Jekyll minima theme at: minima You\u2019ll find this post in your _posts directory. Go ahead and edit it and re-build the site to see your changes. You can rebuild the site in many different ways, but the most common way is to run jekyll serve , which launches a web server and auto-regenerates your site when a file is updated. To add new posts, simply add a file in the _posts directory that follows the convention YYYY-MM-DD-name-of-post.ext and includes the necessary front matter. Take a look at the source for this post to get an idea about how it works. Jekyll also offers powerful support for code snippets: {% highlight ruby %} def print_hi(name) puts \"Hi, #{name}\" end print_hi('Tom')","title":"Plugins"},{"location":"Markup_and_Documentation/Jekyll/#prints-hi-tom-to-stdout","text":"{% endhighlight %}","title":"=&gt; prints 'Hi, Tom' to STDOUT."},{"location":"Markup_and_Documentation/Markdown/","text":"Markdown Essentials \u00b6 Markdown main site GitHub Flavored Markdown Guide Basics \u00b6 A paragraph is one or more consecutive lines of text separated by one or more blank lines. A blank line contains nothing but spaces or tabs. Do not indent normal paragraphs with spaces or tabs. Indent at least 4 spaces or a tab for code blocks. Syntax highlighted code block # Header 1 ## Header 2 ### Header 3 - Bulleted - List 1. Numbered 2. List **Bold** and _Italic_ and `Code` text [Link](url) and ![Image](src) Emphasis \u00b6 *single asterisks* _single underscores_ **double asterisks** __double underscores__ Emphasis can be used in the mi\\*dd\\*le of a word. Headers \u00b6 # H1 ## H2 ### H3 #### H4 ##### H5 ###### H6 Alt-H1 ====== Alt-H2 ------ Links and Images \u00b6 [ Text for the link ](URL) This is [an example][id] reference-style link. [id]: http://example.com/ \"Optional Title Here\" ![Alt text](/path/to/img.jpg \"Optional title\") Code \u00b6 `span of code` ```python def wiki_rocks(text): formatter = lambda t: \"funky\"+t return formatter(text) ``` will be displayed as def wiki_rocks ( text ): formatter = lambda t : \"funky\" + t return formatter ( text ) Blockquotes \u00b6 > This is a blockquote with two paragraphs. > > Second paragraph. GitHub Pages \u00b6 GitHub Pages documentation GitHub Pages site will use the layout and styles from the Jekyll theme you have selected in your repository settings . The name of this theme is saved in the Jekyll _config.yml configuration file. Bitbucket \u00b6 Bitbucket doesn't support arbitrary HTML in Markdown, it instead uses safe mode. Safe mode requires that you replace, remove, or escape HTML tags appropriately. Code highlighting to bitbucket README.md written in Python Markdown friends = [ 'john' , 'pat' , 'gary' , 'michael' ] for i , name in enumerate ( friends ): print \"iteration {iteration} is {name} \" . format ( iteration = i , name = name ) Python markdown main site Cloning your Bitbucket Wiki \u00b6 $ git clone http://bitbucket.org/MY_USER/MY_REPO/wiki","title":"Markdown Essentials"},{"location":"Markup_and_Documentation/Markdown/#markdown-essentials","text":"Markdown main site GitHub Flavored Markdown Guide","title":"Markdown Essentials"},{"location":"Markup_and_Documentation/Markdown/#basics","text":"A paragraph is one or more consecutive lines of text separated by one or more blank lines. A blank line contains nothing but spaces or tabs. Do not indent normal paragraphs with spaces or tabs. Indent at least 4 spaces or a tab for code blocks. Syntax highlighted code block # Header 1 ## Header 2 ### Header 3 - Bulleted - List 1. Numbered 2. List **Bold** and _Italic_ and `Code` text [Link](url) and ![Image](src)","title":"Basics"},{"location":"Markup_and_Documentation/Markdown/#emphasis","text":"*single asterisks* _single underscores_ **double asterisks** __double underscores__ Emphasis can be used in the mi\\*dd\\*le of a word.","title":"Emphasis"},{"location":"Markup_and_Documentation/Markdown/#headers","text":"# H1 ## H2 ### H3 #### H4 ##### H5 ###### H6 Alt-H1 ====== Alt-H2 ------","title":"Headers"},{"location":"Markup_and_Documentation/Markdown/#links-and-images","text":"[ Text for the link ](URL) This is [an example][id] reference-style link. [id]: http://example.com/ \"Optional Title Here\" ![Alt text](/path/to/img.jpg \"Optional title\")","title":"Links and Images"},{"location":"Markup_and_Documentation/Markdown/#code","text":"`span of code` ```python def wiki_rocks(text): formatter = lambda t: \"funky\"+t return formatter(text) ``` will be displayed as def wiki_rocks ( text ): formatter = lambda t : \"funky\" + t return formatter ( text )","title":"Code"},{"location":"Markup_and_Documentation/Markdown/#blockquotes","text":"> This is a blockquote with two paragraphs. > > Second paragraph.","title":"Blockquotes"},{"location":"Markup_and_Documentation/Markdown/#github-pages","text":"GitHub Pages documentation GitHub Pages site will use the layout and styles from the Jekyll theme you have selected in your repository settings . The name of this theme is saved in the Jekyll _config.yml configuration file.","title":"GitHub Pages"},{"location":"Markup_and_Documentation/Markdown/#bitbucket","text":"Bitbucket doesn't support arbitrary HTML in Markdown, it instead uses safe mode. Safe mode requires that you replace, remove, or escape HTML tags appropriately. Code highlighting to bitbucket README.md written in Python Markdown friends = [ 'john' , 'pat' , 'gary' , 'michael' ] for i , name in enumerate ( friends ): print \"iteration {iteration} is {name} \" . format ( iteration = i , name = name ) Python markdown main site","title":"Bitbucket"},{"location":"Markup_and_Documentation/Markdown/#cloning-your-bitbucket-wiki","text":"$ git clone http://bitbucket.org/MY_USER/MY_REPO/wiki","title":"Cloning your Bitbucket Wiki"},{"location":"Markup_and_Documentation/MkDocs/","text":"This website is generated by mkdocs.org and the Material Theme . Basic MkDocs Commands \u00b6 mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs help - Print this help message. Install and documentation generation \u00b6 mkdocs.org . To install MkDocs / create a new documentation site: $ pip install mkdocs $ mkdocs new documentation To build the documentation site: $ cd documentation $ mkdocs build To start the live-reloading docs server - http://localhost:8000/ $ mkdocs serve MkDocs can use the ghp-import tool to commit to the gh-pages branch and push the gh-pages branch to GitHub Pages: $ mkdocs gh-deploy MkDocs project layout \u00b6 mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"MkDocs Basics"},{"location":"Markup_and_Documentation/MkDocs/#basic-mkdocs-commands","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs help - Print this help message.","title":"Basic MkDocs Commands"},{"location":"Markup_and_Documentation/MkDocs/#install-and-documentation-generation","text":"mkdocs.org . To install MkDocs / create a new documentation site: $ pip install mkdocs $ mkdocs new documentation To build the documentation site: $ cd documentation $ mkdocs build To start the live-reloading docs server - http://localhost:8000/ $ mkdocs serve MkDocs can use the ghp-import tool to commit to the gh-pages branch and push the gh-pages branch to GitHub Pages: $ mkdocs gh-deploy","title":"Install and documentation generation"},{"location":"Markup_and_Documentation/MkDocs/#mkdocs-project-layout","text":"mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"MkDocs project layout"},{"location":"Markup_and_Documentation/reStructuredText/","text":"reStructuredText \u00b6 reStructuredText Quick Ref reStructuredText Cheat Sheet (see below) reST Short Overview \u00b6 All reST files use an indentation of 3 spaces; no tabs are allowed. The maximum line length is 80 characters for normal text, but tables, deeply indented code samples and long links may extend beyond that. Code example bodies should use normal Python 4-space indentation. Paragraphs are simply chunks of text separated by one or more blank lines. As in Python, indentation is significant in reST, so all lines of the same paragraph must be left-aligned to the same level of indentation. Section headers are created by underlining (and optionally overlining) the section title with a punctuation character, at least as long as the text: ================= This is a heading ================= # with overline, for parts * with overline, for chapters = for sections - for subsections ^ for subsubsections \" for paragraphs one asterisk: *text* for emphasis (italics), two asterisks: **text** for strong emphasis (boldface), and backquotes: ``text`` for code samples. escape with a backslash \\ * This is a bulleted list. * It has two items, the second item uses two lines. 1. This is a numbered list. 2. It has two items too. . This is a numbered list. . It has two items too. Nested lists are possible, but be aware that they must be separated from the parent list items by blank lines Source Code Double Colon \u00b6 This is a normal text paragraph. The next paragraph is a code sample:: It is not processed in any way, except that the indentation is removed. It can span multiple lines. This is a normal text paragraph again. Links \u00b6 `Link text <http://target>`_ for inline web links. Definitions \u00b6 term (up to a line of text) Definition of the term, which must be indented and can even consist of multiple paragraphs next term Description. Footnotes \u00b6 Lorem ipsum [#]_ dolor sit amet ... [#]_ Use of reStructuredText in Python docstrings \u00b6 See http://infinitemonkeycorps.net/docs/pph/ # Typical function documentation: :param volume_id: The ID of the EBS volume to be attached. :type volume_id: str :param instance_id: The ID of the EC2 instance :type instance_id: str :return: `Reverse geocoder return value`_ dictionary giving closest address(es) to `(lat, lng)` :rtype: dict :raises GoogleMapsError: If the coordinates could not be reverse geocoded. Keyword arguments and return value are identical to those of :meth:`geocode()`. .. _`Reverse geocoder return value`: http://code.google.com/apis/maps/documentation/geocoding/index.html#ReverseGeocoding Normal docstring formatting conventions apply: see PEP 257. Identifier references go in `backticks`. :param lat: some text documents parameters :type lat: float documents parameter types :return: dictionary giving some info... documents return values :rtype: dict documents return type :raises SomeError: sometext... documents exceptions raised >>> starts a doctest and is automatically formatted as code Code can also be indicated by indenting four spaces or preceding with :: and a blank line Link to other methods, functions, classes, modules with :meth: mymethod , :func: myfunc , :class: myclass , and :mod: mymodule . Hyperlink names go in backticks with a trailing underscore: Google _ Targets can be defined anywhere with: .. _Google: http://www.google.com/ Explicit Markup \u00b6 An explicit markup block begins with a line starting with .. followed by whitespace and is terminated by the next paragraph at the same level of indentation. (There needs to be a blank line between explicit markup and normal paragraphs. .. sectionauthor:: Guido van Rossum <guido@python.org> .. rubric:: Footnotes .. [#] Text of the first footnote. .. [#] Text of the second footnote. :mod:`parrot` -- Dead parrot access =================================== .. module:: parrot :platform: Unix, Windows :synopsis: Analyze and reanimate dead parrots. .. moduleauthor:: Eric Cleese <eric@python.invalid> .. moduleauthor:: John Idle <john@python.invalid> .. function:: repeat([repeat=3[, number=1000000]]) repeat(y, z) :bar: no Return a line of text input from the user. .. class:: Spam Description of the class. .. data:: ham Description of the attribute. Inline markup \u00b6 :rolename:`content` or :role:`title <target>` :meth:`~Queue.Queue.get` will refer to Queue.Queue.get but only display get as the link text. The following roles refer to objects in modules and are possibly hyperlinked if a matching identifier is found: mod The name of a module; a dotted name may be used. This should also be used for package names. func The name of a Python function; dotted names may be used. The role text should not include trailing parentheses to enhance readability. The parentheses are stripped when searching for identifiers. data The name of a module-level variable or constant. const The name of a \u201cdefined\u201d constant. This may be a C-language #define or a Python variable that is not intended to be changed. class A class name; a dotted name may be used. meth The name of a method of an object. The role text should include the type name and the method name. A dotted name may be used. attr The name of a data attribute of an object. exc The name of an exception. A dotted name may be used. Official reStructuredText Cheatsheet \u00b6 ===================================================== The reStructuredText_ Cheat Sheet: Syntax Reminders ===================================================== :Info: See <http://docutils.sf.net/rst.html> for introductory docs. :Author: David Goodger <goodger@python.org> :Date: $Date: 2013-02-20 01:10:53 +0000 (Wed, 20 Feb 2013) $ :Revision: $Revision: 7612 $ :Description: This is a \"docinfo block\", or bibliographic field list .. NOTE:: If you are reading this as HTML, please read `<cheatsheet.txt>`_ instead to see the input syntax examples! Section Structure ================= Section titles are underlined or overlined & underlined. Body Elements ============= Grid table: +--------------------------------+-----------------------------------+ | Paragraphs are flush-left, | Literal block, preceded by \"::\":: | | separated by blank lines. | | | | Indented | | Block quotes are indented. | | +--------------------------------+ or:: | | >>> print 'Doctest block' | | | Doctest block | > Quoted | +--------------------------------+-----------------------------------+ | | Line blocks preserve line breaks & indents. [new in 0.3.6] | | | Useful for addresses, verse, and adornment-free lists; long | | lines can be wrapped with continuation lines. | +--------------------------------------------------------------------+ Simple tables: ================ ============================================================ List Type Examples (syntax in the `text source <cheatsheet.txt>`_) ================ ============================================================ Bullet list * items begin with \"-\", \"+\", or \"*\" Enumerated list 1. items use any variation of \"1.\", \"A)\", and \"(i)\" #. also auto-enumerated Definition list Term is flush-left : optional classifier Definition is indented, no blank line between Field list :field name: field body Option list -o at least 2 spaces between option & description ================ ============================================================ ================ ============================================================ Explicit Markup Examples (visible in the `text source`_) ================ ============================================================ Footnote .. [1] Manually numbered or [#] auto-numbered (even [#labelled]) or [*] auto-symbol Citation .. [CIT2002] A citation. Hyperlink Target .. _reStructuredText: http://docutils.sf.net/rst.html .. _indirect target: reStructuredText_ .. _internal target: Anonymous Target __ http://docutils.sf.net/docs/ref/rst/restructuredtext.html Directive (\"::\") .. image:: images/biohazard.png Substitution Def .. |substitution| replace:: like an inline directive Comment .. is anything else Empty Comment (\"..\" on a line by itself, with blank lines before & after, used to separate indentation contexts) ================ ============================================================ Inline Markup ============= *emphasis*; **strong emphasis**; `interpreted text`; `interpreted text with role`:emphasis:; ``inline literal text``; standalone hyperlink, http://docutils.sourceforge.net; named reference, reStructuredText_; `anonymous reference`__; footnote reference, [1]_; citation reference, [CIT2002]_; |substitution|; _`inline internal target`. Directive Quick Reference ========================= See <http://docutils.sf.net/docs/ref/rst/directives.html> for full info. ================ ============================================================ Directive Name Description (Docutils version added to, in [brackets]) ================ ============================================================ attention Specific admonition; also \"caution\", \"danger\", \"error\", \"hint\", \"important\", \"note\", \"tip\", \"warning\" admonition Generic titled admonition: ``.. admonition:: By The Way`` image ``.. image:: picture.png``; many options possible figure Like \"image\", but with optional caption and legend topic ``.. topic:: Title``; like a mini section sidebar ``.. sidebar:: Title``; like a mini parallel document parsed-literal A literal block with parsed inline markup rubric ``.. rubric:: Informal Heading`` epigraph Block quote with class=\"epigraph\" highlights Block quote with class=\"highlights\" pull-quote Block quote with class=\"pull-quote\" compound Compound paragraphs [0.3.6] container Generic block-level container element [0.3.10] table Create a titled table [0.3.1] list-table Create a table from a uniform two-level bullet list [0.3.8] csv-table Create a table from CSV data [0.3.4] contents Generate a table of contents sectnum Automatically number sections, subsections, etc. header, footer Create document decorations [0.3.8] target-notes Create an explicit footnote for each external target math Mathematical notation (input in LaTeX format) meta HTML-specific metadata include Read an external reST file as if it were inline raw Non-reST data passed untouched to the Writer replace Replacement text for substitution definitions unicode Unicode character code conversion for substitution defs date Generates today's date; for substitution defs class Set a \"class\" attribute on the next element role Create a custom interpreted text role [0.3.2] default-role Set the default interpreted text role [0.3.10] title Set the metadata document title [0.3.10] ================ ============================================================ Interpreted Text Role Quick Reference ===================================== See <http://docutils.sf.net/docs/ref/rst/roles.html> for full info. ================ ============================================================ Role Name Description ================ ============================================================ emphasis Equivalent to *emphasis* literal Equivalent to ``literal`` but processes backslash escapes math Mathematical notation (input in LaTeX format) PEP Reference to a numbered Python Enhancement Proposal RFC Reference to a numbered Internet Request For Comments raw For non-reST data; cannot be used directly (see docs) [0.3.6] strong Equivalent to **strong** sub Subscript sup Superscript title Title reference (book, etc.); standard default role ================ ============================================================","title":"reStructuredText Cheatsheet"},{"location":"Markup_and_Documentation/reStructuredText/#restructuredtext","text":"reStructuredText Quick Ref reStructuredText Cheat Sheet (see below)","title":"reStructuredText"},{"location":"Markup_and_Documentation/reStructuredText/#rest-short-overview","text":"All reST files use an indentation of 3 spaces; no tabs are allowed. The maximum line length is 80 characters for normal text, but tables, deeply indented code samples and long links may extend beyond that. Code example bodies should use normal Python 4-space indentation. Paragraphs are simply chunks of text separated by one or more blank lines. As in Python, indentation is significant in reST, so all lines of the same paragraph must be left-aligned to the same level of indentation. Section headers are created by underlining (and optionally overlining) the section title with a punctuation character, at least as long as the text: ================= This is a heading ================= # with overline, for parts * with overline, for chapters = for sections - for subsections ^ for subsubsections \" for paragraphs one asterisk: *text* for emphasis (italics), two asterisks: **text** for strong emphasis (boldface), and backquotes: ``text`` for code samples. escape with a backslash \\ * This is a bulleted list. * It has two items, the second item uses two lines. 1. This is a numbered list. 2. It has two items too. . This is a numbered list. . It has two items too. Nested lists are possible, but be aware that they must be separated from the parent list items by blank lines","title":"reST Short Overview"},{"location":"Markup_and_Documentation/reStructuredText/#source-code-double-colon","text":"This is a normal text paragraph. The next paragraph is a code sample:: It is not processed in any way, except that the indentation is removed. It can span multiple lines. This is a normal text paragraph again.","title":"Source Code Double Colon"},{"location":"Markup_and_Documentation/reStructuredText/#links","text":"`Link text <http://target>`_ for inline web links.","title":"Links"},{"location":"Markup_and_Documentation/reStructuredText/#definitions","text":"term (up to a line of text) Definition of the term, which must be indented and can even consist of multiple paragraphs next term Description.","title":"Definitions"},{"location":"Markup_and_Documentation/reStructuredText/#footnotes","text":"Lorem ipsum [#]_ dolor sit amet ... [#]_","title":"Footnotes"},{"location":"Markup_and_Documentation/reStructuredText/#use-of-restructuredtext-in-python-docstrings","text":"See http://infinitemonkeycorps.net/docs/pph/ # Typical function documentation: :param volume_id: The ID of the EBS volume to be attached. :type volume_id: str :param instance_id: The ID of the EC2 instance :type instance_id: str :return: `Reverse geocoder return value`_ dictionary giving closest address(es) to `(lat, lng)` :rtype: dict :raises GoogleMapsError: If the coordinates could not be reverse geocoded. Keyword arguments and return value are identical to those of :meth:`geocode()`. .. _`Reverse geocoder return value`: http://code.google.com/apis/maps/documentation/geocoding/index.html#ReverseGeocoding Normal docstring formatting conventions apply: see PEP 257. Identifier references go in `backticks`. :param lat: some text documents parameters :type lat: float documents parameter types :return: dictionary giving some info... documents return values :rtype: dict documents return type :raises SomeError: sometext... documents exceptions raised >>> starts a doctest and is automatically formatted as code Code can also be indicated by indenting four spaces or preceding with :: and a blank line Link to other methods, functions, classes, modules with :meth: mymethod , :func: myfunc , :class: myclass , and :mod: mymodule . Hyperlink names go in backticks with a trailing underscore: Google _ Targets can be defined anywhere with: .. _Google: http://www.google.com/","title":"Use of reStructuredText in Python docstrings"},{"location":"Markup_and_Documentation/reStructuredText/#explicit-markup","text":"An explicit markup block begins with a line starting with .. followed by whitespace and is terminated by the next paragraph at the same level of indentation. (There needs to be a blank line between explicit markup and normal paragraphs. .. sectionauthor:: Guido van Rossum <guido@python.org> .. rubric:: Footnotes .. [#] Text of the first footnote. .. [#] Text of the second footnote. :mod:`parrot` -- Dead parrot access =================================== .. module:: parrot :platform: Unix, Windows :synopsis: Analyze and reanimate dead parrots. .. moduleauthor:: Eric Cleese <eric@python.invalid> .. moduleauthor:: John Idle <john@python.invalid> .. function:: repeat([repeat=3[, number=1000000]]) repeat(y, z) :bar: no Return a line of text input from the user. .. class:: Spam Description of the class. .. data:: ham Description of the attribute.","title":"Explicit Markup"},{"location":"Markup_and_Documentation/reStructuredText/#inline-markup","text":":rolename:`content` or :role:`title <target>` :meth:`~Queue.Queue.get` will refer to Queue.Queue.get but only display get as the link text. The following roles refer to objects in modules and are possibly hyperlinked if a matching identifier is found: mod The name of a module; a dotted name may be used. This should also be used for package names. func The name of a Python function; dotted names may be used. The role text should not include trailing parentheses to enhance readability. The parentheses are stripped when searching for identifiers. data The name of a module-level variable or constant. const The name of a \u201cdefined\u201d constant. This may be a C-language #define or a Python variable that is not intended to be changed. class A class name; a dotted name may be used. meth The name of a method of an object. The role text should include the type name and the method name. A dotted name may be used. attr The name of a data attribute of an object. exc The name of an exception. A dotted name may be used.","title":"Inline markup"},{"location":"Markup_and_Documentation/reStructuredText/#official-restructuredtext-cheatsheet","text":"===================================================== The reStructuredText_ Cheat Sheet: Syntax Reminders ===================================================== :Info: See <http://docutils.sf.net/rst.html> for introductory docs. :Author: David Goodger <goodger@python.org> :Date: $Date: 2013-02-20 01:10:53 +0000 (Wed, 20 Feb 2013) $ :Revision: $Revision: 7612 $ :Description: This is a \"docinfo block\", or bibliographic field list .. NOTE:: If you are reading this as HTML, please read `<cheatsheet.txt>`_ instead to see the input syntax examples! Section Structure ================= Section titles are underlined or overlined & underlined. Body Elements ============= Grid table: +--------------------------------+-----------------------------------+ | Paragraphs are flush-left, | Literal block, preceded by \"::\":: | | separated by blank lines. | | | | Indented | | Block quotes are indented. | | +--------------------------------+ or:: | | >>> print 'Doctest block' | | | Doctest block | > Quoted | +--------------------------------+-----------------------------------+ | | Line blocks preserve line breaks & indents. [new in 0.3.6] | | | Useful for addresses, verse, and adornment-free lists; long | | lines can be wrapped with continuation lines. | +--------------------------------------------------------------------+ Simple tables: ================ ============================================================ List Type Examples (syntax in the `text source <cheatsheet.txt>`_) ================ ============================================================ Bullet list * items begin with \"-\", \"+\", or \"*\" Enumerated list 1. items use any variation of \"1.\", \"A)\", and \"(i)\" #. also auto-enumerated Definition list Term is flush-left : optional classifier Definition is indented, no blank line between Field list :field name: field body Option list -o at least 2 spaces between option & description ================ ============================================================ ================ ============================================================ Explicit Markup Examples (visible in the `text source`_) ================ ============================================================ Footnote .. [1] Manually numbered or [#] auto-numbered (even [#labelled]) or [*] auto-symbol Citation .. [CIT2002] A citation. Hyperlink Target .. _reStructuredText: http://docutils.sf.net/rst.html .. _indirect target: reStructuredText_ .. _internal target: Anonymous Target __ http://docutils.sf.net/docs/ref/rst/restructuredtext.html Directive (\"::\") .. image:: images/biohazard.png Substitution Def .. |substitution| replace:: like an inline directive Comment .. is anything else Empty Comment (\"..\" on a line by itself, with blank lines before & after, used to separate indentation contexts) ================ ============================================================ Inline Markup ============= *emphasis*; **strong emphasis**; `interpreted text`; `interpreted text with role`:emphasis:; ``inline literal text``; standalone hyperlink, http://docutils.sourceforge.net; named reference, reStructuredText_; `anonymous reference`__; footnote reference, [1]_; citation reference, [CIT2002]_; |substitution|; _`inline internal target`. Directive Quick Reference ========================= See <http://docutils.sf.net/docs/ref/rst/directives.html> for full info. ================ ============================================================ Directive Name Description (Docutils version added to, in [brackets]) ================ ============================================================ attention Specific admonition; also \"caution\", \"danger\", \"error\", \"hint\", \"important\", \"note\", \"tip\", \"warning\" admonition Generic titled admonition: ``.. admonition:: By The Way`` image ``.. image:: picture.png``; many options possible figure Like \"image\", but with optional caption and legend topic ``.. topic:: Title``; like a mini section sidebar ``.. sidebar:: Title``; like a mini parallel document parsed-literal A literal block with parsed inline markup rubric ``.. rubric:: Informal Heading`` epigraph Block quote with class=\"epigraph\" highlights Block quote with class=\"highlights\" pull-quote Block quote with class=\"pull-quote\" compound Compound paragraphs [0.3.6] container Generic block-level container element [0.3.10] table Create a titled table [0.3.1] list-table Create a table from a uniform two-level bullet list [0.3.8] csv-table Create a table from CSV data [0.3.4] contents Generate a table of contents sectnum Automatically number sections, subsections, etc. header, footer Create document decorations [0.3.8] target-notes Create an explicit footnote for each external target math Mathematical notation (input in LaTeX format) meta HTML-specific metadata include Read an external reST file as if it were inline raw Non-reST data passed untouched to the Writer replace Replacement text for substitution definitions unicode Unicode character code conversion for substitution defs date Generates today's date; for substitution defs class Set a \"class\" attribute on the next element role Create a custom interpreted text role [0.3.2] default-role Set the default interpreted text role [0.3.10] title Set the metadata document title [0.3.10] ================ ============================================================ Interpreted Text Role Quick Reference ===================================== See <http://docutils.sf.net/docs/ref/rst/roles.html> for full info. ================ ============================================================ Role Name Description ================ ============================================================ emphasis Equivalent to *emphasis* literal Equivalent to ``literal`` but processes backslash escapes math Mathematical notation (input in LaTeX format) PEP Reference to a numbered Python Enhancement Proposal RFC Reference to a numbered Internet Request For Comments raw For non-reST data; cannot be used directly (see docs) [0.3.6] strong Equivalent to **strong** sub Subscript sup Superscript title Title reference (book, etc.); standard default role ================ ============================================================","title":"Official reStructuredText Cheatsheet"},{"location":"Microservices/Microservices/","text":"MicroServices \u00b6 microservices.io Introduction to microservices Scala \u00b6 Lagom .NET \u00b6 Microservices in C#","title":"Microservices"},{"location":"Microservices/Microservices/#microservices","text":"microservices.io Introduction to microservices","title":"MicroServices"},{"location":"Microservices/Microservices/#scala","text":"Lagom","title":"Scala"},{"location":"Microservices/Microservices/#net","text":"Microservices in C#","title":".NET"},{"location":"Privacy/Privacy_engineering/","text":"Privacy Engineering \u00b6 Interesting Links \u00b6 Homomorphic Encryption","title":"Privacy Engineering"},{"location":"Privacy/Privacy_engineering/#privacy-engineering","text":"","title":"Privacy Engineering"},{"location":"Privacy/Privacy_engineering/#interesting-links","text":"Homomorphic Encryption","title":"Interesting Links"},{"location":"Python/Flask/","text":"Flask \u00b6 Useful Flask modules \u00b6 Flask-Login Flask-SSLify Flask-RESTPlus Building beautiful REST APIs using Flask, Swagger UI and Flask-RESTPlus","title":"Flask"},{"location":"Python/Flask/#flask","text":"","title":"Flask"},{"location":"Python/Flask/#useful-flask-modules","text":"Flask-Login Flask-SSLify Flask-RESTPlus Building beautiful REST APIs using Flask, Swagger UI and Flask-RESTPlus","title":"Useful Flask modules"},{"location":"Python/Jupyter/","text":"IPython / Jupyter \u00b6 Using IPython makes interactive work easy. Better shell Notebook interface Embeddable kernel Parallel python IPython shell shortcuts \u00b6 TAB expansion to complete python names and file paths ~ and * directory / file expansion many \"magic\" methods: %lsmagic # list of all magic methods %quickref # cheatsheet %magic Help \u00b6 ? # overall help help # python help system ?someobj or someobj? # help ??someobj or someobj?? # detailed help %pdoc %pdef %psource for docstring, function definition, source code only. Run \u00b6 To run a program directly from the IPython console: %run somescript.py # instead of execfile(\"somescript.py\") at the python prompt %run has special flags for timing the execution of your scripts ( -t ) or for running them under the control of either Python's pdb debugger ( -d ) or profiler ( -p ): %run -d myscript.py Other Commands \u00b6 %edit %ed # edit then execute %save %load example.py # load local (example) file (or url) allowing modification %load http://matplotlib.org/plot_directive/mpl_examples/mplot3d/contour3d_demo.py %macro # define macro with range of history lines, filenames or string objects %recall %whos # list identifiers you have defined interactively %reset -f -s # remove objects -f for force -s for soft (leaves history). %reset is not a kernel restart Restart with Ctrl+. in \"qtconsole\" import module ; reload(module) to reload a module from disk Debugging \u00b6 %debug # jump into the Python debugger (pdb) %pdb # start the debugger on any uncaught exception. %cd # change directory %pwd # print working directory %env # OS environment variables OS Commands \u00b6 !OScommand !ping www.bbc.co.uk %alias # system command alias History \u00b6 _ __ ___ # etc... for previous outputs. _i _ii _i4 # etc.. for previous input. _ih for list of previous inputs GUI integration \u00b6 Start with ipython --gui=qt or at the IPython prompt: %gui wx Arguments can be wx , qt , gtk and tk . Matplotlib / pylab graphics in an iPython shell \u00b6 Start with: ipython --matplotlib ( or --matplotlib=qt etc...) At the IPython prompt: %matplotlib # set matplotlib to work interactively; does not import anythig %matplotlib inline %matplotlib qt # request a specific GUI backend %pylab inline %pylab makes the following imports: import numpy import matplotlib from matplotlib import pylab , mlab , pyplot np = numpy plt = pyplot from IPython.display import display from IPython.core.pylabtools import figsize , getfigs from pylab import * from numpy import * Qtconsole - an improved console \u00b6 At the command prompt: ipython.exe qtconsole --pylab=inline --ConsoleWidget.font_size=10 alternative: --matplotlib inline or within IPython: %matplotlib inline %pylab inline To embed plots, SVG or HTML in qtconsole, call display: from IPython.core.display import display, display_html from IPython.core.display import display_png, display_svg display(plt.gcf()) # embeds the current figure in the qtconsole display(*getfigs()) # embeds all active figures in the qtconsole #or: f = plt.figure() plt.plot(np.rand(100)) display(f) ipython and ipython notebook for matlab users IPython Notebook web-based interface \u00b6 Start with: ipython notebook and switch to browser Keyboard shortcuts: Enter to edit a cell Shift + Enter to evaluate Ctrl + m or Esc for the \"command mode\" In command mode: h list of keyboard shortcuts 1-6 to convert to heading cell m to convert to markdown cell y to convert to code c copy / v paste d d delete cell s save notebook . to restart kernel Papermill \u00b6 Papermill Papermill is a tool for parameterizing and executing Jupyter Notebooks. Papermill GitHub","title":"IPython / Jupyter Cheatsheet"},{"location":"Python/Jupyter/#ipython-jupyter","text":"Using IPython makes interactive work easy. Better shell Notebook interface Embeddable kernel Parallel python","title":"IPython / Jupyter"},{"location":"Python/Jupyter/#ipython-shell-shortcuts","text":"TAB expansion to complete python names and file paths ~ and * directory / file expansion many \"magic\" methods: %lsmagic # list of all magic methods %quickref # cheatsheet %magic","title":"IPython shell shortcuts"},{"location":"Python/Jupyter/#help","text":"? # overall help help # python help system ?someobj or someobj? # help ??someobj or someobj?? # detailed help %pdoc %pdef %psource for docstring, function definition, source code only.","title":"Help"},{"location":"Python/Jupyter/#run","text":"To run a program directly from the IPython console: %run somescript.py # instead of execfile(\"somescript.py\") at the python prompt %run has special flags for timing the execution of your scripts ( -t ) or for running them under the control of either Python's pdb debugger ( -d ) or profiler ( -p ): %run -d myscript.py","title":"Run"},{"location":"Python/Jupyter/#other-commands","text":"%edit %ed # edit then execute %save %load example.py # load local (example) file (or url) allowing modification %load http://matplotlib.org/plot_directive/mpl_examples/mplot3d/contour3d_demo.py %macro # define macro with range of history lines, filenames or string objects %recall %whos # list identifiers you have defined interactively %reset -f -s # remove objects -f for force -s for soft (leaves history). %reset is not a kernel restart Restart with Ctrl+. in \"qtconsole\" import module ; reload(module) to reload a module from disk","title":"Other Commands"},{"location":"Python/Jupyter/#debugging","text":"%debug # jump into the Python debugger (pdb) %pdb # start the debugger on any uncaught exception. %cd # change directory %pwd # print working directory %env # OS environment variables","title":"Debugging"},{"location":"Python/Jupyter/#os-commands","text":"!OScommand !ping www.bbc.co.uk %alias # system command alias","title":"OS Commands"},{"location":"Python/Jupyter/#history","text":"_ __ ___ # etc... for previous outputs. _i _ii _i4 # etc.. for previous input. _ih for list of previous inputs","title":"History"},{"location":"Python/Jupyter/#gui-integration","text":"Start with ipython --gui=qt or at the IPython prompt: %gui wx Arguments can be wx , qt , gtk and tk .","title":"GUI integration"},{"location":"Python/Jupyter/#matplotlib-pylab-graphics-in-an-ipython-shell","text":"Start with: ipython --matplotlib ( or --matplotlib=qt etc...) At the IPython prompt: %matplotlib # set matplotlib to work interactively; does not import anythig %matplotlib inline %matplotlib qt # request a specific GUI backend %pylab inline %pylab makes the following imports: import numpy import matplotlib from matplotlib import pylab , mlab , pyplot np = numpy plt = pyplot from IPython.display import display from IPython.core.pylabtools import figsize , getfigs from pylab import * from numpy import *","title":"Matplotlib / pylab graphics in an iPython shell"},{"location":"Python/Jupyter/#qtconsole-an-improved-console","text":"At the command prompt: ipython.exe qtconsole --pylab=inline --ConsoleWidget.font_size=10 alternative: --matplotlib inline or within IPython: %matplotlib inline %pylab inline To embed plots, SVG or HTML in qtconsole, call display: from IPython.core.display import display, display_html from IPython.core.display import display_png, display_svg display(plt.gcf()) # embeds the current figure in the qtconsole display(*getfigs()) # embeds all active figures in the qtconsole #or: f = plt.figure() plt.plot(np.rand(100)) display(f) ipython and ipython notebook for matlab users","title":"Qtconsole - an improved console"},{"location":"Python/Jupyter/#ipython-notebook-web-based-interface","text":"Start with: ipython notebook and switch to browser Keyboard shortcuts: Enter to edit a cell Shift + Enter to evaluate Ctrl + m or Esc for the \"command mode\" In command mode: h list of keyboard shortcuts 1-6 to convert to heading cell m to convert to markdown cell y to convert to code c copy / v paste d d delete cell s save notebook . to restart kernel","title":"IPython Notebook web-based interface"},{"location":"Python/Jupyter/#papermill","text":"Papermill Papermill is a tool for parameterizing and executing Jupyter Notebooks. Papermill GitHub","title":"Papermill"},{"location":"Python/Matplotlib/","text":"Matplotlib Cheatsheet \u00b6 Matplotlib prepares 2D (and some 3D) graphics. Main page: http://www.matplotlib.org Image gallery: http://matplotlib.org/gallery.html pyplot command summary: http://matplotlib.org/api/pyplot_summary.html Examples http://matplotlib.org/examples/index.html Tutorial: http://www.loria.fr/~rougier/teaching/matplotlib/ See also: https://www.wakari.io/nb/url///wakari.io/static/notebooks/Lecture_4_Matplotlib.ipynb Matplotlib, pylab, and pyplot: how are they related? \u00b6 Matplotlib is the whole package. Pylab and matplotlib.pyplot (pyplot in the following) are modules in matplotlib. Pyplot makes matplotlib work like MATLAB. Pyplot provides the state-machine interface to the underlying plotting library (the matplotlib API in the matplotlib module). Each pyplot function makes some change to a figure: eg, create a figure, create a plotting area in a figure, plot some lines in a plotting area, decorate the plot with labels, etc.... Pyplot is stateful , in that it keeps track of the current figure and plotting area, and the plotting functions are directed to the current axes. Pylab combines the pyplot functionality (for plotting) with the numpy functionality (mathematics / arrays) in a single namespace, making that namespace (or environment) even more MATLAB-like. The pyplot interface is generally preferred for non-interactive plotting (i.e., scripting). The pylab interface is convenient for interactive calculations and plotting, as it minimizes typing. Examples \u00b6 import numpy as np import matplotlib.pyplot as plt # Compute the x and y coordinates for points on sine and cosine curves x = np . arange ( 0 , 3 * np . pi , 0.1 ) y_sin = np . sin ( x ) y_cos = np . cos ( x ) # Set up a subplot grid that has height 2 and width 1, # and set the first such subplot as active. plt . subplot ( 2 , 1 , 1 ) # Make the first plot plt . plot ( x , y_sin ) plt . title ( 'Sine' ) # Set the second subplot as active, and make the second plot. plt . subplot ( 2 , 1 , 2 ) plt . plot ( x , y_cos ) plt . title ( 'Cosine' ) # Show the figure. plt . show ()","title":"Matplotlib Cheatsheet"},{"location":"Python/Matplotlib/#matplotlib-cheatsheet","text":"Matplotlib prepares 2D (and some 3D) graphics. Main page: http://www.matplotlib.org Image gallery: http://matplotlib.org/gallery.html pyplot command summary: http://matplotlib.org/api/pyplot_summary.html Examples http://matplotlib.org/examples/index.html Tutorial: http://www.loria.fr/~rougier/teaching/matplotlib/ See also: https://www.wakari.io/nb/url///wakari.io/static/notebooks/Lecture_4_Matplotlib.ipynb","title":"Matplotlib Cheatsheet"},{"location":"Python/Matplotlib/#matplotlib-pylab-and-pyplot-how-are-they-related","text":"Matplotlib is the whole package. Pylab and matplotlib.pyplot (pyplot in the following) are modules in matplotlib. Pyplot makes matplotlib work like MATLAB. Pyplot provides the state-machine interface to the underlying plotting library (the matplotlib API in the matplotlib module). Each pyplot function makes some change to a figure: eg, create a figure, create a plotting area in a figure, plot some lines in a plotting area, decorate the plot with labels, etc.... Pyplot is stateful , in that it keeps track of the current figure and plotting area, and the plotting functions are directed to the current axes. Pylab combines the pyplot functionality (for plotting) with the numpy functionality (mathematics / arrays) in a single namespace, making that namespace (or environment) even more MATLAB-like. The pyplot interface is generally preferred for non-interactive plotting (i.e., scripting). The pylab interface is convenient for interactive calculations and plotting, as it minimizes typing.","title":"Matplotlib, pylab, and pyplot: how are they related?"},{"location":"Python/Matplotlib/#examples","text":"import numpy as np import matplotlib.pyplot as plt # Compute the x and y coordinates for points on sine and cosine curves x = np . arange ( 0 , 3 * np . pi , 0.1 ) y_sin = np . sin ( x ) y_cos = np . cos ( x ) # Set up a subplot grid that has height 2 and width 1, # and set the first such subplot as active. plt . subplot ( 2 , 1 , 1 ) # Make the first plot plt . plot ( x , y_sin ) plt . title ( 'Sine' ) # Set the second subplot as active, and make the second plot. plt . subplot ( 2 , 1 , 2 ) plt . plot ( x , y_cos ) plt . title ( 'Cosine' ) # Show the figure. plt . show ()","title":"Examples"},{"location":"Python/Python/","text":"Python useful links \u00b6 Python environments \u00b6 venv \u2014 Creation of virtual environments python3 -m venv /path/to/new/virtual/environment Virtualenv virtualenv is a tool to create isolated Python environments. Since Python 3.3, a subset of it has been integrated into the standard library under the venv module. Note though, that the venv module does not offer all features of this library (e.g. cannot create bootstrap scripts, cannot create virtual environments for other python versions than the host python, not relocatable, etc.). pip-tools A set of command line tools to help you keep your pip-based packages fresh, even when you've pinned them. Pipenv The problems that Pipenv seeks to solve are multi-faceted: You no longer need to use pip and virtualenv separately. They work together. Managing a requirements.txt file can be problematic, so Pipenv uses Pipfile and Pipfile.lock to separate abstract dependency declarations from the last tested combination. Hashes are used everywhere, always. Security. Automatically expose security vulnerabilities. Strongly encourage the use of the latest versions of dependencies to minimize security risks arising from outdated components. Give you insight into your dependency graph (e.g. $ pipenv graph). Streamline development workflow by loading .env files. Pyenv for managing multiple Python versions Testing \u00b6 Mockito Code coverage measurement for Python mechanize - Automate interaction with HTTP web servers Packaging \u00b6 Cookiecutter template for a Python package Python Packaging User Guide Twine Twine is a utility for publishing Python packages on PyPI. Build tools \u00b6 Buildout, an automation tool written in and extended with Python PyBuilder Uranium: a Python Build System Other \u00b6 Python Anywhere Host, run, and code Python in the cloud","title":"Python Links"},{"location":"Python/Python/#python-useful-links","text":"","title":"Python useful links"},{"location":"Python/Python/#python-environments","text":"venv \u2014 Creation of virtual environments python3 -m venv /path/to/new/virtual/environment Virtualenv virtualenv is a tool to create isolated Python environments. Since Python 3.3, a subset of it has been integrated into the standard library under the venv module. Note though, that the venv module does not offer all features of this library (e.g. cannot create bootstrap scripts, cannot create virtual environments for other python versions than the host python, not relocatable, etc.). pip-tools A set of command line tools to help you keep your pip-based packages fresh, even when you've pinned them. Pipenv The problems that Pipenv seeks to solve are multi-faceted: You no longer need to use pip and virtualenv separately. They work together. Managing a requirements.txt file can be problematic, so Pipenv uses Pipfile and Pipfile.lock to separate abstract dependency declarations from the last tested combination. Hashes are used everywhere, always. Security. Automatically expose security vulnerabilities. Strongly encourage the use of the latest versions of dependencies to minimize security risks arising from outdated components. Give you insight into your dependency graph (e.g. $ pipenv graph). Streamline development workflow by loading .env files. Pyenv for managing multiple Python versions","title":"Python environments"},{"location":"Python/Python/#testing","text":"Mockito Code coverage measurement for Python mechanize - Automate interaction with HTTP web servers","title":"Testing"},{"location":"Python/Python/#packaging","text":"Cookiecutter template for a Python package Python Packaging User Guide Twine Twine is a utility for publishing Python packages on PyPI.","title":"Packaging"},{"location":"Python/Python/#build-tools","text":"Buildout, an automation tool written in and extended with Python PyBuilder Uranium: a Python Build System","title":"Build tools"},{"location":"Python/Python/#other","text":"Python Anywhere Host, run, and code Python in the cloud","title":"Other"},{"location":"Python/Python3/","text":"What's new in Python 3.x \u00b6 What's really new in Python 3 nonlocal / global \u00b6 x = 0 def outer (): x = 1 def inner (): nonlocal x x = 2 print ( \"inner:\" , x ) inner () print ( \"outer:\" , x ) outer () print ( \"global:\" , x ) # inner: 2 # outer: 2 # global: 0 ## with global x = 0 def outer (): x = 1 def inner (): global x x = 2 print ( \"inner:\" , x ) inner () print ( \"outer:\" , x ) outer () print ( \"global:\" , x ) # inner: 2 # outer: 1 # global: 2 String interpolation - new in 3.6 \u00b6 name = \"David\" f \"My name is { name } \" value = decimal . Decimal ( \"10.4507\" ) print ( f \"result: { value : 10.5 } \" ) # width precision PEP 492 - Coroutines with async and await syntax \u00b6 async and await yield from iterator is equivalent for x in iterator : yield x Example: def lazy_range ( up_to ): \"\"\"Generator to return the sequence of integers from 0 to up_to, exclusive.\"\"\" index = 0 def gratuitous_refactor (): nonlocal index while index < up_to : yield index index += 1 yield from gratuitous_refactor () New 3.6 syntax: async def func ( param1 , param2 ): do_stuff () await some_coroutine () async def read_data ( db ): data = await db . fetch ( 'SELECT ...' ) async def display_date ( loop ): end_time = loop . time () + 5.0 while True : print ( datetime . datetime . now ()) if ( loop . time () + 1.0 ) >= end_time : break await asyncio . sleep ( 1 ) loop = asyncio . get_event_loop () # Blocking call which returns when the display_date() coroutine is done loop . run_until_complete ( display_date ( loop )) loop . close () Async for \u00b6 async for TARGET in ITER : BLOCK else : BLOCK2 Async improvements - 3.6 \u00b6 set comprehension: {i async for i in agen()} list comprehension: [i async for i in agen()] dict comprehension: {i: i ** 2 async for i in agen()} generator expression: (i ** 2 async for i in agen()) Type hinting \u00b6 PEP 484 mypy-lang.org def greet ( name : str ) -> str return 'Hello there, {} ' . format ( name ) Type aliases \u00b6 Url = str def retry ( url : Url , retry_count : int ) -> None : ... from typing import TypeVar , Iterable , Tuple Other common typings include: Any; Generic, Dict, List, Optional, Mapping, Set, Sequence - expressed as Sequence[int] T = TypeVar ( 'T' , int , float , complex ) # T is either or an int, a float or a complex Vector = Iterable [ Tuple [ T , T ]] # def inproduct ( v : Vector [ T ]) -> T : return sum ( x * y for x , y in v ) def dilate ( v : Vector [ T ], scale : T ) -> Vector [ T ]: return (( x * scale , y * scale ) for x , y in v ) vec = [] # type: Vector[float] For functions \u00b6 Callable [[ Arg1Type , Arg2Type ], ReturnType ] Type comments \u00b6 x = [] # type: List[Employee] x , y , z = [], [], [] # type: List[int], List[int], List[str]","title":"Python 3"},{"location":"Python/Python3/#whats-new-in-python-3x","text":"What's really new in Python 3","title":"What's new in Python 3.x"},{"location":"Python/Python3/#nonlocal-global","text":"x = 0 def outer (): x = 1 def inner (): nonlocal x x = 2 print ( \"inner:\" , x ) inner () print ( \"outer:\" , x ) outer () print ( \"global:\" , x ) # inner: 2 # outer: 2 # global: 0 ## with global x = 0 def outer (): x = 1 def inner (): global x x = 2 print ( \"inner:\" , x ) inner () print ( \"outer:\" , x ) outer () print ( \"global:\" , x ) # inner: 2 # outer: 1 # global: 2","title":"nonlocal / global"},{"location":"Python/Python3/#string-interpolation-new-in-36","text":"name = \"David\" f \"My name is { name } \" value = decimal . Decimal ( \"10.4507\" ) print ( f \"result: { value : 10.5 } \" ) # width precision","title":"String interpolation - new in 3.6"},{"location":"Python/Python3/#pep-492-coroutines-with-async-and-await-syntax","text":"async and await yield from iterator is equivalent for x in iterator : yield x Example: def lazy_range ( up_to ): \"\"\"Generator to return the sequence of integers from 0 to up_to, exclusive.\"\"\" index = 0 def gratuitous_refactor (): nonlocal index while index < up_to : yield index index += 1 yield from gratuitous_refactor () New 3.6 syntax: async def func ( param1 , param2 ): do_stuff () await some_coroutine () async def read_data ( db ): data = await db . fetch ( 'SELECT ...' ) async def display_date ( loop ): end_time = loop . time () + 5.0 while True : print ( datetime . datetime . now ()) if ( loop . time () + 1.0 ) >= end_time : break await asyncio . sleep ( 1 ) loop = asyncio . get_event_loop () # Blocking call which returns when the display_date() coroutine is done loop . run_until_complete ( display_date ( loop )) loop . close ()","title":"PEP 492 - Coroutines with async and await syntax"},{"location":"Python/Python3/#async-for","text":"async for TARGET in ITER : BLOCK else : BLOCK2","title":"Async for"},{"location":"Python/Python3/#async-improvements-36","text":"set comprehension: {i async for i in agen()} list comprehension: [i async for i in agen()] dict comprehension: {i: i ** 2 async for i in agen()} generator expression: (i ** 2 async for i in agen())","title":"Async improvements - 3.6"},{"location":"Python/Python3/#type-hinting","text":"PEP 484 mypy-lang.org def greet ( name : str ) -> str return 'Hello there, {} ' . format ( name )","title":"Type hinting"},{"location":"Python/Python3/#type-aliases","text":"Url = str def retry ( url : Url , retry_count : int ) -> None : ... from typing import TypeVar , Iterable , Tuple Other common typings include: Any; Generic, Dict, List, Optional, Mapping, Set, Sequence - expressed as Sequence[int] T = TypeVar ( 'T' , int , float , complex ) # T is either or an int, a float or a complex Vector = Iterable [ Tuple [ T , T ]] # def inproduct ( v : Vector [ T ]) -> T : return sum ( x * y for x , y in v ) def dilate ( v : Vector [ T ], scale : T ) -> Vector [ T ]: return (( x * scale , y * scale ) for x , y in v ) vec = [] # type: Vector[float]","title":"Type aliases"},{"location":"Python/Python3/#for-functions","text":"Callable [[ Arg1Type , Arg2Type ], ReturnType ]","title":"For functions"},{"location":"Python/Python3/#type-comments","text":"x = [] # type: List[Employee] x , y , z = [], [], [] # type: List[int], List[int], List[str]","title":"Type comments"},{"location":"Scala/Akka/","text":"Akka \u00b6 Actors have: A mailbox (the queue where messages end up). A behavior (the state of the actor, internal variables etc.). Messages (pieces of data representing a signal, similar to method calls and their parameters). An execution environment (the machinery that takes actors that have messages to react to and invokes their message handling code). An address. sbt: libraryDependencies ++= Seq ( \"com.typesafe.akka\" %% \"akka-actor\" % \"2.5.6\" , \"com.typesafe.akka\" %% \"akka-testkit\" % \"2.5.6\" % Test ) Basic Example \u00b6 //#full-example package com.lightbend.akka.sample import akka.actor. { Actor , ActorLogging , ActorRef , ActorSystem , Props } import scala.io.StdIn //#greeter-companion //#greeter-messages object Greeter { //#greeter-messages def props ( message : String , printerActor : ActorRef ) : Props = Props ( new Greeter ( message , printerActor )) //#greeter-messages final case class WhoToGreet ( who : String ) case object Greet } //#greeter-messages //#greeter-companion //#greeter-actor class Greeter ( message : String , printerActor : ActorRef ) extends Actor { import Greeter._ import Printer._ var greeting = \"\" def receive = { case WhoToGreet ( who ) => greeting = s\" $message , $who \" case Greet => //#greeter-send-message printerActor ! Greeting ( greeting ) //#greeter-send-message } } //#greeter-actor //#printer-companion //#printer-messages object Printer { //#printer-messages def props : Props = Props [ Printer ] //#printer-messages final case class Greeting ( greeting : String ) } //#printer-messages //#printer-companion //#printer-actor class Printer extends Actor with ActorLogging { import Printer._ def receive = { case Greeting ( greeting ) => log . info ( s\"Greeting received (from ${ sender () } ): $greeting \" ) } } //#printer-actor //#main-class object AkkaQuickstart extends App { import Greeter._ // Create the 'helloAkka' actor system val system : ActorSystem = ActorSystem ( \"helloAkka\" ) try { //#create-actors // Create the printer actor val printer : ActorRef = system . actorOf ( Printer . props , \"printerActor\" ) // Create the 'greeter' actors val howdyGreeter : ActorRef = system . actorOf ( Greeter . props ( \"Howdy\" , printer ), \"howdyGreeter\" ) val helloGreeter : ActorRef = system . actorOf ( Greeter . props ( \"Hello\" , printer ), \"helloGreeter\" ) val goodDayGreeter : ActorRef = system . actorOf ( Greeter . props ( \"Good day\" , printer ), \"goodDayGreeter\" ) //#create-actors //#main-send-messages howdyGreeter ! WhoToGreet ( \"Akka\" ) howdyGreeter ! Greet howdyGreeter ! WhoToGreet ( \"Lightbend\" ) howdyGreeter ! Greet helloGreeter ! WhoToGreet ( \"Scala\" ) helloGreeter ! Greet goodDayGreeter ! WhoToGreet ( \"Play\" ) goodDayGreeter ! Greet //#main-send-messages println ( \">>> Press ENTER to exit <<<\" ) StdIn . readLine () } finally { system . terminate () } } //#main-class //#full-example","title":"Akka Cheatsheet"},{"location":"Scala/Akka/#akka","text":"Actors have: A mailbox (the queue where messages end up). A behavior (the state of the actor, internal variables etc.). Messages (pieces of data representing a signal, similar to method calls and their parameters). An execution environment (the machinery that takes actors that have messages to react to and invokes their message handling code). An address. sbt: libraryDependencies ++= Seq ( \"com.typesafe.akka\" %% \"akka-actor\" % \"2.5.6\" , \"com.typesafe.akka\" %% \"akka-testkit\" % \"2.5.6\" % Test )","title":"Akka"},{"location":"Scala/Akka/#basic-example","text":"//#full-example package com.lightbend.akka.sample import akka.actor. { Actor , ActorLogging , ActorRef , ActorSystem , Props } import scala.io.StdIn //#greeter-companion //#greeter-messages object Greeter { //#greeter-messages def props ( message : String , printerActor : ActorRef ) : Props = Props ( new Greeter ( message , printerActor )) //#greeter-messages final case class WhoToGreet ( who : String ) case object Greet } //#greeter-messages //#greeter-companion //#greeter-actor class Greeter ( message : String , printerActor : ActorRef ) extends Actor { import Greeter._ import Printer._ var greeting = \"\" def receive = { case WhoToGreet ( who ) => greeting = s\" $message , $who \" case Greet => //#greeter-send-message printerActor ! Greeting ( greeting ) //#greeter-send-message } } //#greeter-actor //#printer-companion //#printer-messages object Printer { //#printer-messages def props : Props = Props [ Printer ] //#printer-messages final case class Greeting ( greeting : String ) } //#printer-messages //#printer-companion //#printer-actor class Printer extends Actor with ActorLogging { import Printer._ def receive = { case Greeting ( greeting ) => log . info ( s\"Greeting received (from ${ sender () } ): $greeting \" ) } } //#printer-actor //#main-class object AkkaQuickstart extends App { import Greeter._ // Create the 'helloAkka' actor system val system : ActorSystem = ActorSystem ( \"helloAkka\" ) try { //#create-actors // Create the printer actor val printer : ActorRef = system . actorOf ( Printer . props , \"printerActor\" ) // Create the 'greeter' actors val howdyGreeter : ActorRef = system . actorOf ( Greeter . props ( \"Howdy\" , printer ), \"howdyGreeter\" ) val helloGreeter : ActorRef = system . actorOf ( Greeter . props ( \"Hello\" , printer ), \"helloGreeter\" ) val goodDayGreeter : ActorRef = system . actorOf ( Greeter . props ( \"Good day\" , printer ), \"goodDayGreeter\" ) //#create-actors //#main-send-messages howdyGreeter ! WhoToGreet ( \"Akka\" ) howdyGreeter ! Greet howdyGreeter ! WhoToGreet ( \"Lightbend\" ) howdyGreeter ! Greet helloGreeter ! WhoToGreet ( \"Scala\" ) helloGreeter ! Greet goodDayGreeter ! WhoToGreet ( \"Play\" ) goodDayGreeter ! Greet //#main-send-messages println ( \">>> Press ENTER to exit <<<\" ) StdIn . readLine () } finally { system . terminate () } } //#main-class //#full-example","title":"Basic Example"},{"location":"Scala/Play_Framework/","text":"Links \u00b6 Play Framework The Play application layout \u00b6 The layout of a Play application is standardized to keep things as simple as possible. After a first successful compile, a Play application looks like this: app \u2192 Application sources \u2514 assets \u2192 Compiled asset sources \u2514 stylesheets \u2192 Typically LESS CSS sources \u2514 javascripts \u2192 Typically CoffeeScript sources \u2514 controllers \u2192 Application controllers \u2514 models \u2192 Application business layer \u2514 views \u2192 Templates build.sbt \u2192 Application build script conf \u2192 Configurations files and other non-compiled resources (on classpath) \u2514 application.conf \u2192 Main configuration file \u2514 routes \u2192 Routes definition dist \u2192 Arbitrary files to be included in your projects distribution public \u2192 Public assets \u2514 stylesheets \u2192 CSS files \u2514 javascripts \u2192 Javascript files \u2514 images \u2192 Image files project \u2192 sbt configuration files \u2514 build.properties \u2192 Marker for sbt project \u2514 plugins.sbt \u2192 sbt plugins including the declaration for Play itself lib \u2192 Unmanaged libraries dependencies logs \u2192 Logs folder \u2514 application.log \u2192 Default log file target \u2192 Generated stuff \u2514 resolution-cache \u2192 Info about dependencies \u2514 scala-2.11 \u2514 api \u2192 Generated API docs \u2514 classes \u2192 Compiled class files \u2514 routes \u2192 Sources generated from routes \u2514 twirl \u2192 Sources generated from templates \u2514 universal \u2192 Application packaging \u2514 web \u2192 Compiled web assets test \u2192 source folder for unit or functional tests","title":"Play Framework"},{"location":"Scala/Play_Framework/#links","text":"Play Framework","title":"Links"},{"location":"Scala/Play_Framework/#the-play-application-layout","text":"The layout of a Play application is standardized to keep things as simple as possible. After a first successful compile, a Play application looks like this: app \u2192 Application sources \u2514 assets \u2192 Compiled asset sources \u2514 stylesheets \u2192 Typically LESS CSS sources \u2514 javascripts \u2192 Typically CoffeeScript sources \u2514 controllers \u2192 Application controllers \u2514 models \u2192 Application business layer \u2514 views \u2192 Templates build.sbt \u2192 Application build script conf \u2192 Configurations files and other non-compiled resources (on classpath) \u2514 application.conf \u2192 Main configuration file \u2514 routes \u2192 Routes definition dist \u2192 Arbitrary files to be included in your projects distribution public \u2192 Public assets \u2514 stylesheets \u2192 CSS files \u2514 javascripts \u2192 Javascript files \u2514 images \u2192 Image files project \u2192 sbt configuration files \u2514 build.properties \u2192 Marker for sbt project \u2514 plugins.sbt \u2192 sbt plugins including the declaration for Play itself lib \u2192 Unmanaged libraries dependencies logs \u2192 Logs folder \u2514 application.log \u2192 Default log file target \u2192 Generated stuff \u2514 resolution-cache \u2192 Info about dependencies \u2514 scala-2.11 \u2514 api \u2192 Generated API docs \u2514 classes \u2192 Compiled class files \u2514 routes \u2192 Sources generated from routes \u2514 twirl \u2192 Sources generated from templates \u2514 universal \u2192 Application packaging \u2514 web \u2192 Compiled web assets test \u2192 source folder for unit or functional tests","title":"The Play application layout"},{"location":"Scala/Scala_Collections/","text":"Examples from Scala Koans . Core Packages \u00b6 The scala package contains core types like Int, Float, Array or Option which are accessible in all Scala compilation units without explicit qualification or imports. Notable packages include: scala.collection and its sub-packages contain Scala's collections framework scala.collection.immutable - Immutable, sequential data-structures such as Vector, List, Range, HashMap or HashSet scala.collection.mutable - Mutable, sequential data-structures such as ArrayBuffer, StringBuilder, HashMap or HashSet scala.collection.concurrent - Mutable, concurrent data-structures such as TrieMap scala.collection.parallel.immutable - Immutable, parallel data-structures such as ParVector, ParRange, ParHashMap or ParHashSet scala.collection.parallel.mutable - Mutable, parallel data-structures such as ParArray, ParHashMap, ParTrieMap or ParHashSet scala.concurrent - Primitives for concurrent programming such as Futures and Promises scala.io - Input and output operations scala.math - Basic math functions and additional numeric types like BigInt and BigDecimal scala.sys - Interaction with other processes and the operating system scala.util.matching - Regular expressions Additional parts of the standard library are shipped as separate libraries. These include: scala.reflect - Scala's reflection API (scala-reflect.jar) scala.xml - XML parsing, manipulation, and serialization (scala-xml.jar) scala.swing - A convenient wrapper around Java's GUI framework called Swing (scala-swing.jar) scala.util.parsing - Parser combinators (scala-parser-combinators.jar) Automatic imports Identifiers in the scala package and the scala.Predef object are always in scope by default. Some of these identifiers are type aliases provided as shortcuts to commonly used classes. For example, List is an alias for scala.collection.immutable.List. Other aliases refer to classes provided by the underlying platform. For example, on the JVM, String is an alias for java.lang.String. Traversables \u00b6 Traversables are the superclass of Lists, Arrays, Maps, Sets, Streams, and more. The methods involved can be applied to each other in a different type. val set = Set ( 1 , 9 , 10 , 22 ) val list = List ( 3 , 4 , 5 , 10 ) val result = set ++ list // ++ appends two Traversables together. result . size result . isEmpty result . hasDefiniteSize // false if a Stream Take some list . head list . headOption list . tail list . lastOption result . last list . init // collection without the last element list . slice ( 1 , 3 ) list . take ( 3 ) list drop 6 take 3 list . takeWhile ( _ < 100 ) list . dropWhile ( _ < 100 ) Filter, Map, Flatten list . filter ( _ < 100 ) list . filterNot ( _ < 100 ) list . find ( _ % 2 != 0 ) // get first element that matches list . foreach ( num => println ( num * 4 )) // side effect list . map ( _ * 4 ) // map val list = List ( List ( 1 ), List ( 2 , 3 , 4 ), List ( 5 , 6 , 7 ), List ( 8 , 9 , 10 )) list . flatten list . flatMap ( _ . map ( _ * 4 )) // map then flatten val result = list . collect { // apply a partial function to all elements of a Traversable and will return a different collection. case x : Int if ( x % 2 == 0 ) => x * 3 } // can use orElse or andThen Split val array = Array ( 87 , 44 , 5 , 4 , 200 , 10 , 39 , 100 ) // splitAt - will split a Traversable at a position, returning a tuple. val result = array splitAt 3 result . _1 result . _2 val result = array partition ( _ < 100 ) // partition will split a Traversable according to predicate, return a 2 product Tuple. The left side are the elements satisfied by the predicate, the right side is not. // groupBy returns a map e.g. Map( \"Odd\" -> ... , \"Even\" -> ...) val result = array groupBy { case x : Int if x % 2 = = 0 => \"Even\" ; case x : Int if x % 2 != 0 => \"Odd\" } Analyze list forall ( _ < 100 ) // true if predicate true for all elements list exists ( _ < 100 ) // true if predicate true for any element list count ( _ < 100 ) Reduce and Fold list . foldLeft ( 0 )( _ - _ ) ( 0 /: list )( _ - _ ) // Short hand list . foldRight ( 0 )( _ - _ ) ( list :\\ 0 )( _ - _ ) // Short hand list . reduceLeft { _ + _ } list . reduceRight { _ + _ } list . sum list . product list . max list . min val list = List ( List ( 1 , 2 , 3 ), List ( 4 , 5 , 6 ), List ( 7 , 8 , 9 )) list . transpose Conversions; toList, as well as other conversion methods like toSet, toArray will not convert if the collection type is the same. list . toArray list . toSet set . toList set . toIterable set . toSeq set . toIndexedSeq list . toStream val list = List ( \"Phoenix\" -> \"Arizona\" , \"Austin\" -> \"Texas\" ) // elements should be tuples val result = list . toMap Print result . mkString ( \",\" ) list . mkString ( \">\" , \",\" , \"<\" ) val list = List ( 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 , 11 , 12 , 13 , 14 , 15 ) stringBuilder . append ( \"I want all numbers 6-12: \" ) list . filter ( it => it > 5 && it < 13 ). addString ( stringBuilder , \",\" ) stringBuilder . mkString Lists \u00b6 val a = List ( 1 , 2 , 3 ) // immutable val b = 1 :: 2 :: 3 :: Nil // cons notation ( a == b ) // true a eq b // false a . length a . head a . tail a . reverse // reverse the list a . map { v => v * 2 } // or a.map {_ * 2} or a.map(_ * 2) a . filter { v => v % 3 == 0 } a . filterNot ( v => v == 5 ) // remove where value is 5 a . reduceLeft ( _ + _ ) // note the two _s below indicate the first and second args respectively a . foldLeft ( 10 )( _ + _ ) // foldlLeft is like reduce, but with an explicit starting value ( 1 to 5 ). toList // from range val a = a . toArray Nil lists are identical, even of different types Iterators \u00b6 val list = List ( 3 , 5 , 9 , 11 , 15 , 19 , 21 ) val it = list . iterator if ( it . hasNext ) { it . next should be ( __ ) } val it = list grouped 3 // `grouped` will return an fixed sized Iterable chucks of an Iterable val it = list sliding 3 // `sliding` will return an Iterable that shows a sliding window of an Iterable. val it = list sliding ( 3 , 3 ) // `sliding` can take the size of the window as well the size of the step during each iteration list takeRight 3 list dropRight 3 val xs = List ( 3 , 5 , 9 ) // `zip` will stitch two iterables into an iterable of pairs (tuples) of corresponding elements from both iterables. val ys = List ( \"Bob\" , \"Ann\" , \"Stella\" ) xs zip ys // If two Iterables aren't the same size, then `zip` will only zip what can only be paired. xs zipAll ( ys , - 1 , \"?\" ) // if two Iterables aren't the same size, then `zipAll` can provide fillers xs . zipWithIndex Arrays, Sequences \u00b6 val a = Array ( 1 , 2 , 3 ) val s = a . toSeq val l = s . toList val s = Seq ( \"hello\" , \"to\" , \"you\" ) val filtered = s . filter ( _ . length > 2 ) val r = s map { _ . reverse } val s = for ( v <- 1 to 10 if v % 3 == 0 ) yield v // create a sequence from a for comprehension with an optional condition s . toList Lazy Collections and Streams \u00b6 val strictList = List ( 10 , 20 , 30 ) val lazyList = strictList . view // Strict collection always processes its elements but lazy collection does it on demand val infinite = Stream . from ( 1 ) infinite . take ( 4 ). sum Stream . continually ( 1 ). take ( 4 ). sum // Always remember tail of a lazy collection is never computed unless required def makeLazy ( value : Int ) : Stream [ Int ] = { Stream . cons ( value , makeLazy ( value + 1 )) } val stream = makeLazy ( 1 ) stream . head Maps \u00b6 val myMap = Map ( \"MI\" -> \"Michigan\" , \"OH\" -> \"Ohio\" , \"WI\" -> \"Wisconsin\" , \"MI\" -> \"Michigan\" ) // access by key - Accessing a map by key results in an exception if key is not found myMap ( \"MI\" ) myMap . contains ( \"IL\" ) val aNewMap = myMap + ( \"IL\" -> \"Illinois\" ) // add - creates a new collection if immutable val aNewMap = myMap - \"MI\" // remove - Attempted removal of nonexistent elements from a map is handled gracefully val aNewMap = myMap -- List ( \"MI\" , \"OH\" ) // remove multiples val aNewMap = myMap - ( \"MI\" , \"WI\" ) // Notice: single '-' operator for tuples var anotherMap += ( \"IL\" -> \"Illinois\" ) // compiler trick - creates a new collection and reassigns; note the 'var' // Map values can be iterated val mapValues = myMap . values mapValues . size mapValues . head mapValues . last for ( mval <- mapValues ) println ( mval ) // NOTE that the following will not compile, as iterators do not implement \"contains\" //mapValues.contains(\"Illinois\") // Map keys may be of mixed type val myMap = Map ( \"Ann Arbor\" -> \"MI\" , 49931 -> \"MI\" ) // Mixed type values can be added to a map val myMap = scala . collection . mutable . Map . empty [ String , Any ] myMap ( \"Ann Arbor\" ) = ( 48103 , 48104 , 48108 ) myMap ( \"Houghton\" ) = 49931 // Map equivalency is independent of order val myMap1 = Map ( \"MI\" -> \"Michigan\" , \"OH\" -> \"Ohio\" , \"WI\" -> \"Wisconsin\" , \"IA\" -> \"Iowa\" ) val myMap2 = Map ( \"WI\" -> \"Wisconsin\" , \"MI\" -> \"Michigan\" , \"IA\" -> \"Iowa\" , \"OH\" -> \"Ohio\" ) myMap1 . equals ( myMap2 ) Maps insertion with duplicate key updates previous entry with subsequent value Mutable Maps val myMap = mutable . Map ( \"MI\" -> \"Michigan\" , \"OH\" -> \"Ohio\" , \"WI\" -> \"Wisconsin\" , \"IA\" -> \"Iowa\" ) // same methods than immutable maps work val myMap += ( \"IL\" -> \"Illinois\" ) // this is a method; note the difference from immutable += myMap . clear () // Convention is to use parens if possible when method called changes state Sets \u00b6 val mySet = Set ( 1 , 3 , 4 , 9 ) // immutable val mySet = mutable . Set ( \"Michigan\" , \"Ohio\" , \"Wisconsin\" , \"Iowa\" ) mySet . size mySet contains \"Ohio\" mySet += \"Oregon\" mySet += ( \"Iowa\" , \"Ohio\" ) mySet ++= List ( \"Iowa\" , \"Ohio\" ) mySet -= \"Ohio\" mySet --= List ( \"Iowa\" , \"Ohio\" ) mySet . clear () // mutable only var sum = 0 for ( i <- mySet ) // for comprehension sum = sum + i // of course this is the same thing as mySet.reduce(_ + _) val mySet2 = Set ( \"Wisconsin\" , \"Michigan\" , \"Minnesota\" ) mySet intersect mySet2 // or & operator mySet1 union mySet2 // or | operator mySet2 subsetOf mySet1 mySet1 diff mySet2 mySet1 . equals ( mySet2 ) // independent of order Option[T] \u00b6 val someValue : Option [ String ] = Some ( \"I am wrapped in something\" ) val nullValue : Option [ String ] = None someValue . get // java.util.NoSuchElementException if None nullValue getOrElse \"No value\" nullValue . isEmpty val value = someValue match { // pattern matching case Some ( v ) => v case None => 0.0 } Option is more than just a replacement of null, its also a collection. Some ( 10 ) filter { _ == 10 } Some ( Some ( 10 )) flatMap { _ map { _ + 10 }} var newValue1 = 0 Some ( 20 ) foreach { newValue1 = _ } flatMap of Options will filter out all Nones and keep the Somes val list = List ( 1 , 2 , 3 , 4 , 5 ) val result = list . flatMap ( it => if ( it % 2 == 0 ) Some ( it ) else None ) Using \"for comprehension\" val values = List ( Some ( 10 ), Some ( 20 ), None , Some ( 15 )) val newValues = for { someValue <- values value <- someValue } yield value Java Interop \u00b6 Scala can implicitly convert from a Scala collection type into a Java collection type. import scala.collection.JavaConversions._","title":"Scala Collections"},{"location":"Scala/Scala_Collections/#core-packages","text":"The scala package contains core types like Int, Float, Array or Option which are accessible in all Scala compilation units without explicit qualification or imports. Notable packages include: scala.collection and its sub-packages contain Scala's collections framework scala.collection.immutable - Immutable, sequential data-structures such as Vector, List, Range, HashMap or HashSet scala.collection.mutable - Mutable, sequential data-structures such as ArrayBuffer, StringBuilder, HashMap or HashSet scala.collection.concurrent - Mutable, concurrent data-structures such as TrieMap scala.collection.parallel.immutable - Immutable, parallel data-structures such as ParVector, ParRange, ParHashMap or ParHashSet scala.collection.parallel.mutable - Mutable, parallel data-structures such as ParArray, ParHashMap, ParTrieMap or ParHashSet scala.concurrent - Primitives for concurrent programming such as Futures and Promises scala.io - Input and output operations scala.math - Basic math functions and additional numeric types like BigInt and BigDecimal scala.sys - Interaction with other processes and the operating system scala.util.matching - Regular expressions Additional parts of the standard library are shipped as separate libraries. These include: scala.reflect - Scala's reflection API (scala-reflect.jar) scala.xml - XML parsing, manipulation, and serialization (scala-xml.jar) scala.swing - A convenient wrapper around Java's GUI framework called Swing (scala-swing.jar) scala.util.parsing - Parser combinators (scala-parser-combinators.jar) Automatic imports Identifiers in the scala package and the scala.Predef object are always in scope by default. Some of these identifiers are type aliases provided as shortcuts to commonly used classes. For example, List is an alias for scala.collection.immutable.List. Other aliases refer to classes provided by the underlying platform. For example, on the JVM, String is an alias for java.lang.String.","title":"Core Packages"},{"location":"Scala/Scala_Collections/#traversables","text":"Traversables are the superclass of Lists, Arrays, Maps, Sets, Streams, and more. The methods involved can be applied to each other in a different type. val set = Set ( 1 , 9 , 10 , 22 ) val list = List ( 3 , 4 , 5 , 10 ) val result = set ++ list // ++ appends two Traversables together. result . size result . isEmpty result . hasDefiniteSize // false if a Stream Take some list . head list . headOption list . tail list . lastOption result . last list . init // collection without the last element list . slice ( 1 , 3 ) list . take ( 3 ) list drop 6 take 3 list . takeWhile ( _ < 100 ) list . dropWhile ( _ < 100 ) Filter, Map, Flatten list . filter ( _ < 100 ) list . filterNot ( _ < 100 ) list . find ( _ % 2 != 0 ) // get first element that matches list . foreach ( num => println ( num * 4 )) // side effect list . map ( _ * 4 ) // map val list = List ( List ( 1 ), List ( 2 , 3 , 4 ), List ( 5 , 6 , 7 ), List ( 8 , 9 , 10 )) list . flatten list . flatMap ( _ . map ( _ * 4 )) // map then flatten val result = list . collect { // apply a partial function to all elements of a Traversable and will return a different collection. case x : Int if ( x % 2 == 0 ) => x * 3 } // can use orElse or andThen Split val array = Array ( 87 , 44 , 5 , 4 , 200 , 10 , 39 , 100 ) // splitAt - will split a Traversable at a position, returning a tuple. val result = array splitAt 3 result . _1 result . _2 val result = array partition ( _ < 100 ) // partition will split a Traversable according to predicate, return a 2 product Tuple. The left side are the elements satisfied by the predicate, the right side is not. // groupBy returns a map e.g. Map( \"Odd\" -> ... , \"Even\" -> ...) val result = array groupBy { case x : Int if x % 2 = = 0 => \"Even\" ; case x : Int if x % 2 != 0 => \"Odd\" } Analyze list forall ( _ < 100 ) // true if predicate true for all elements list exists ( _ < 100 ) // true if predicate true for any element list count ( _ < 100 ) Reduce and Fold list . foldLeft ( 0 )( _ - _ ) ( 0 /: list )( _ - _ ) // Short hand list . foldRight ( 0 )( _ - _ ) ( list :\\ 0 )( _ - _ ) // Short hand list . reduceLeft { _ + _ } list . reduceRight { _ + _ } list . sum list . product list . max list . min val list = List ( List ( 1 , 2 , 3 ), List ( 4 , 5 , 6 ), List ( 7 , 8 , 9 )) list . transpose Conversions; toList, as well as other conversion methods like toSet, toArray will not convert if the collection type is the same. list . toArray list . toSet set . toList set . toIterable set . toSeq set . toIndexedSeq list . toStream val list = List ( \"Phoenix\" -> \"Arizona\" , \"Austin\" -> \"Texas\" ) // elements should be tuples val result = list . toMap Print result . mkString ( \",\" ) list . mkString ( \">\" , \",\" , \"<\" ) val list = List ( 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 , 11 , 12 , 13 , 14 , 15 ) stringBuilder . append ( \"I want all numbers 6-12: \" ) list . filter ( it => it > 5 && it < 13 ). addString ( stringBuilder , \",\" ) stringBuilder . mkString","title":"Traversables"},{"location":"Scala/Scala_Collections/#lists","text":"val a = List ( 1 , 2 , 3 ) // immutable val b = 1 :: 2 :: 3 :: Nil // cons notation ( a == b ) // true a eq b // false a . length a . head a . tail a . reverse // reverse the list a . map { v => v * 2 } // or a.map {_ * 2} or a.map(_ * 2) a . filter { v => v % 3 == 0 } a . filterNot ( v => v == 5 ) // remove where value is 5 a . reduceLeft ( _ + _ ) // note the two _s below indicate the first and second args respectively a . foldLeft ( 10 )( _ + _ ) // foldlLeft is like reduce, but with an explicit starting value ( 1 to 5 ). toList // from range val a = a . toArray Nil lists are identical, even of different types","title":"Lists"},{"location":"Scala/Scala_Collections/#iterators","text":"val list = List ( 3 , 5 , 9 , 11 , 15 , 19 , 21 ) val it = list . iterator if ( it . hasNext ) { it . next should be ( __ ) } val it = list grouped 3 // `grouped` will return an fixed sized Iterable chucks of an Iterable val it = list sliding 3 // `sliding` will return an Iterable that shows a sliding window of an Iterable. val it = list sliding ( 3 , 3 ) // `sliding` can take the size of the window as well the size of the step during each iteration list takeRight 3 list dropRight 3 val xs = List ( 3 , 5 , 9 ) // `zip` will stitch two iterables into an iterable of pairs (tuples) of corresponding elements from both iterables. val ys = List ( \"Bob\" , \"Ann\" , \"Stella\" ) xs zip ys // If two Iterables aren't the same size, then `zip` will only zip what can only be paired. xs zipAll ( ys , - 1 , \"?\" ) // if two Iterables aren't the same size, then `zipAll` can provide fillers xs . zipWithIndex","title":"Iterators"},{"location":"Scala/Scala_Collections/#arrays-sequences","text":"val a = Array ( 1 , 2 , 3 ) val s = a . toSeq val l = s . toList val s = Seq ( \"hello\" , \"to\" , \"you\" ) val filtered = s . filter ( _ . length > 2 ) val r = s map { _ . reverse } val s = for ( v <- 1 to 10 if v % 3 == 0 ) yield v // create a sequence from a for comprehension with an optional condition s . toList","title":"Arrays, Sequences"},{"location":"Scala/Scala_Collections/#lazy-collections-and-streams","text":"val strictList = List ( 10 , 20 , 30 ) val lazyList = strictList . view // Strict collection always processes its elements but lazy collection does it on demand val infinite = Stream . from ( 1 ) infinite . take ( 4 ). sum Stream . continually ( 1 ). take ( 4 ). sum // Always remember tail of a lazy collection is never computed unless required def makeLazy ( value : Int ) : Stream [ Int ] = { Stream . cons ( value , makeLazy ( value + 1 )) } val stream = makeLazy ( 1 ) stream . head","title":"Lazy Collections and Streams"},{"location":"Scala/Scala_Collections/#maps","text":"val myMap = Map ( \"MI\" -> \"Michigan\" , \"OH\" -> \"Ohio\" , \"WI\" -> \"Wisconsin\" , \"MI\" -> \"Michigan\" ) // access by key - Accessing a map by key results in an exception if key is not found myMap ( \"MI\" ) myMap . contains ( \"IL\" ) val aNewMap = myMap + ( \"IL\" -> \"Illinois\" ) // add - creates a new collection if immutable val aNewMap = myMap - \"MI\" // remove - Attempted removal of nonexistent elements from a map is handled gracefully val aNewMap = myMap -- List ( \"MI\" , \"OH\" ) // remove multiples val aNewMap = myMap - ( \"MI\" , \"WI\" ) // Notice: single '-' operator for tuples var anotherMap += ( \"IL\" -> \"Illinois\" ) // compiler trick - creates a new collection and reassigns; note the 'var' // Map values can be iterated val mapValues = myMap . values mapValues . size mapValues . head mapValues . last for ( mval <- mapValues ) println ( mval ) // NOTE that the following will not compile, as iterators do not implement \"contains\" //mapValues.contains(\"Illinois\") // Map keys may be of mixed type val myMap = Map ( \"Ann Arbor\" -> \"MI\" , 49931 -> \"MI\" ) // Mixed type values can be added to a map val myMap = scala . collection . mutable . Map . empty [ String , Any ] myMap ( \"Ann Arbor\" ) = ( 48103 , 48104 , 48108 ) myMap ( \"Houghton\" ) = 49931 // Map equivalency is independent of order val myMap1 = Map ( \"MI\" -> \"Michigan\" , \"OH\" -> \"Ohio\" , \"WI\" -> \"Wisconsin\" , \"IA\" -> \"Iowa\" ) val myMap2 = Map ( \"WI\" -> \"Wisconsin\" , \"MI\" -> \"Michigan\" , \"IA\" -> \"Iowa\" , \"OH\" -> \"Ohio\" ) myMap1 . equals ( myMap2 ) Maps insertion with duplicate key updates previous entry with subsequent value Mutable Maps val myMap = mutable . Map ( \"MI\" -> \"Michigan\" , \"OH\" -> \"Ohio\" , \"WI\" -> \"Wisconsin\" , \"IA\" -> \"Iowa\" ) // same methods than immutable maps work val myMap += ( \"IL\" -> \"Illinois\" ) // this is a method; note the difference from immutable += myMap . clear () // Convention is to use parens if possible when method called changes state","title":"Maps"},{"location":"Scala/Scala_Collections/#sets","text":"val mySet = Set ( 1 , 3 , 4 , 9 ) // immutable val mySet = mutable . Set ( \"Michigan\" , \"Ohio\" , \"Wisconsin\" , \"Iowa\" ) mySet . size mySet contains \"Ohio\" mySet += \"Oregon\" mySet += ( \"Iowa\" , \"Ohio\" ) mySet ++= List ( \"Iowa\" , \"Ohio\" ) mySet -= \"Ohio\" mySet --= List ( \"Iowa\" , \"Ohio\" ) mySet . clear () // mutable only var sum = 0 for ( i <- mySet ) // for comprehension sum = sum + i // of course this is the same thing as mySet.reduce(_ + _) val mySet2 = Set ( \"Wisconsin\" , \"Michigan\" , \"Minnesota\" ) mySet intersect mySet2 // or & operator mySet1 union mySet2 // or | operator mySet2 subsetOf mySet1 mySet1 diff mySet2 mySet1 . equals ( mySet2 ) // independent of order","title":"Sets"},{"location":"Scala/Scala_Collections/#optiont","text":"val someValue : Option [ String ] = Some ( \"I am wrapped in something\" ) val nullValue : Option [ String ] = None someValue . get // java.util.NoSuchElementException if None nullValue getOrElse \"No value\" nullValue . isEmpty val value = someValue match { // pattern matching case Some ( v ) => v case None => 0.0 } Option is more than just a replacement of null, its also a collection. Some ( 10 ) filter { _ == 10 } Some ( Some ( 10 )) flatMap { _ map { _ + 10 }} var newValue1 = 0 Some ( 20 ) foreach { newValue1 = _ } flatMap of Options will filter out all Nones and keep the Somes val list = List ( 1 , 2 , 3 , 4 , 5 ) val result = list . flatMap ( it => if ( it % 2 == 0 ) Some ( it ) else None ) Using \"for comprehension\" val values = List ( Some ( 10 ), Some ( 20 ), None , Some ( 15 )) val newValues = for { someValue <- values value <- someValue } yield value","title":"Option[T]"},{"location":"Scala/Scala_Collections/#java-interop","text":"Scala can implicitly convert from a Scala collection type into a Java collection type. import scala.collection.JavaConversions._","title":"Java Interop"},{"location":"Scala/Scala_Database_Access/","text":"Database Access / ORM Libraries \u00b6 Slick Manual JOOQ Squeryl SORM Doobie ReactiveMongo Comparison of multiple frameworks Database Initialization / Migration \u00b6 Flyway","title":"Scala Database Access"},{"location":"Scala/Scala_Database_Access/#database-access-orm-libraries","text":"Slick Manual JOOQ Squeryl SORM Doobie ReactiveMongo Comparison of multiple frameworks","title":"Database Access / ORM Libraries"},{"location":"Scala/Scala_Database_Access/#database-initialization-migration","text":"Flyway","title":"Database Initialization / Migration"},{"location":"Scala/Scala_Design_Patterns/","text":"Create a new project from template \u00b6 Use the \u201csbt new\u201d command, providing the name of the template. For example, \u201c$ sbt new akka/hello-akka.g8\u201d. You can find a list of templates here . Or download from Scala Project Templates Static Factory \u00b6 trait Animal class Bird extends Animal class Mammal extends Animal class Fish extends Animal object Animal { def apply ( animal : String ) : Animal = animal . toLowerCase match { case \"bird\" => new Bird case \"mammal\" => new Mammal case \"fish\" => new Fish case x : String => throw new RuntimeException ( s\"Unknown animal: $x \" ) } } Algebraic Data Types and Pattern Matching \u00b6 Goal: translate data descriptions into code Model data with logical ors and logical ands Two patterns: product types (and) sum types (or) Product type: A has a B and C Sum type: A is a B or C Sum and product together make algebraic data types // A has a B and C case class A ( b : B , c : C ) // A is a B or C sealed trait A case class B () extends A case class C () extends A They have only data and do not contain any functionality on top of this data as normal classes would. sealed trait Shape case class Circle ( radius : Double ) extends Shape case class Rectangle ( height : Double , width : Double ) extends Shape object Shape { def area ( shape : Shape ) : Double = shape match { case Circle ( Point ( x , y ), radius ) => Math . PI * Math . pow ( radius , 2 ) // use pattern matching to process case Rectangle ( _ , h , w ) => h * w } } Stackable Traits \u00b6 abstract class StringWriter { def write ( data : String ) : String } class BasicStringWriter extends StringWriter { override def write ( data : String ) : String = s\"Writing the following data: ${ data } \" } trait CapitalizingStringWriter extends StringWriter { abstract override def write ( data : String ) : String = { super . write ( data . split ( \"\\\\s+\" ). map ( _ . capitalize ). mkString ( \" \" )) } } trait UppercasingStringWriter extends StringWriter { abstract override def write ( data : String ) : String = { super . write ( data . toUpperCase ) } } object Example { def main ( args : Array [ String ]) : Unit = { val writer1 = new BasicStringWriter with UppercasingStringWriter with CapitalizingStringWriter System . out . println ( s\"Writer 1: ' ${ writer1 . write ( \"we like learning scala!\" ) } '\" ) } } Stackable traits order of execution Stackable traits are always executed from the right mixin to the left. Sometimes, however, if we only get output and it doesn't depend on what is passed to the method, we simply end up with method calls on a stack, which then get evaluated and it will appear as if things are applied from left to right. Components / Cake Pattern \u00b6 http://jonasboner.com/real-world-scala-dependency-injection-di/ // Service Interfaces and Component Definitions trait OnOffDeviceComponent { val onOff : OnOffDevice // abstract val trait OnOffDevice { def on: Unit def off: Unit } } trait SensorDeviceComponent { val sensor : SensorDevice trait SensorDevice { def isCoffeePresent : Boolean } } // ======================= // Component / Service Implementations trait OnOffDeviceComponentImpl extends OnOffDeviceComponent { class Heater extends OnOffDevice { def on = println ( \"heater.on\" ) def off = println ( \"heater.off\" ) } } trait SensorDeviceComponentImpl extends SensorDeviceComponent { class PotSensor extends SensorDevice { def isCoffeePresent = true } } // ======================= // Component declaring two dependencies that it wants injected trait WarmerComponentImpl { this: SensorDeviceComponent with OnOffDeviceComponent => // Use of self-type for composition class Warmer { def trigger = { if ( sensor . isCoffeePresent ) onOff . on else onOff . off } } } // ======================= // Instantiation (and configuration) of the services in the ComponentRegistry module object ComponentRegistry extends OnOffDeviceComponentImpl with SensorDeviceComponentImpl with WarmerComponentImpl { val onOff = new Heater // all instantiations in one spot; can be easily be replaced by e.g. mocks val sensor = new PotSensor val warmer = new Warmer } // ======================= val warmer = ComponentRegistry . warmer warmer . trigger Type Classes (using context-bound type parameters) \u00b6 Ad-hoc polymorphism Break free from your class oppressors! Concerns that cross class hierarchy e.g. serialize to JSON Common behaviour without (useful) common type Abstract behaviour to a type class Can implement type class instances in ad-hoc manner // Define some behavior in terms of operations that a type must support in order to be considered a member of the type class. trait Number [ T ] { def plus ( x : T , y : T ) : T def divide ( x : T , y : Int ) : T } // Define the default type class members in the companion object of the trait object Number { implicit object DoubleNumber extends Number [ Double ] { // note the implicit override def plus ( x : Double , y : Double ) : Double = x + y override def divide ( x : Double , y : Int ) : Double = x / y } } object Stats { // older pattern with implicit parameter // def mean[T](xs: Vector[T])(implicit ev: Number[T]): T = // note the implicit // ev.divide(xs.reduce(ev.plus(_, _)), xs.size) def mean [ T: Number ]( xs : Vector [ T ]) : T = // note the context bound implicitly [ Number [ T ]]. divide ( xs . reduce ( implicitly [ Number [ T ]]. plus ( _ , _ )), // retrieve the evidence via implicitly[] xs . size ) } Visitor Pattern \u00b6 abstract class Element ( text : String ) { def accept ( visitor : Visitor ) } case class Title ( text : String ) extends Element ( text ) { override def accept ( visitor : Visitor ) : Unit = { visitor . visit ( this ) } } case class Text ( text : String ) extends Element ( text ) { override def accept ( visitor : Visitor ) : Unit = { visitor . visit ( this ) } } class Document ( parts : List [ Element ]) { def accept ( visitor : Visitor ) : Unit = { parts . foreach ( p => p . accept ( visitor )) } } trait Visitor { def visit ( element : Element ) } class VisitorImpl1 extends Visitor { override def visit ( element : Element ) : Unit = { element match { case Title ( text ) => ??? case Text ( text ) => ??? //... } } } Configuration \u00b6 import com.typesafe.config.ConfigFactory trait AppConfigComponent { val appConfigService : AppConfigService class AppConfigService () { //-Dconfig.resource=production.conf for overriding private val conf = ConfigFactory . load () private val appConf = conf . getConfig ( \"job-scheduler\" ) private val db = appConf . getConfig ( \"db\" ) val configPath = appConf . getString ( \"config-path\" ) val configExtension = appConf . getString ( \"config-extension\" ) val workers = appConf . getInt ( \"workers\" ) val dbConnectionString = db . getString ( \"connection-string\" ) val dbUsername = db . getString ( \"username\" ) val dbPassword = db . getString ( \"password\" ) } } Memoization \u00b6 import scala.collection.mutable.Map trait Memoizer { def memo [ X , Y ]( f : X => Y ) : ( X => Y ) = { val cache = Map [ X , Y ]() ( x : X ) => cache . getOrElseUpdate ( x , f ( x )) } } Using scalaz: val memoScalaz : String => String = Memo . mutableHashMapMemo { func } Pimp my Library Pattern \u00b6 The pimp my library design pattern is really similar to extension methods in C#. package object pimp { implicit class StringExtensions ( val s : String ) extends AnyVal { def isAllUpperCase : Boolean = ( 0 to s . size - 1 ). find { case index => ! s . charAt ( index ). isUpper }. isEmpty } }","title":"Scala Design Patterns"},{"location":"Scala/Scala_Design_Patterns/#create-a-new-project-from-template","text":"Use the \u201csbt new\u201d command, providing the name of the template. For example, \u201c$ sbt new akka/hello-akka.g8\u201d. You can find a list of templates here . Or download from Scala Project Templates","title":"Create a new project from template"},{"location":"Scala/Scala_Design_Patterns/#static-factory","text":"trait Animal class Bird extends Animal class Mammal extends Animal class Fish extends Animal object Animal { def apply ( animal : String ) : Animal = animal . toLowerCase match { case \"bird\" => new Bird case \"mammal\" => new Mammal case \"fish\" => new Fish case x : String => throw new RuntimeException ( s\"Unknown animal: $x \" ) } }","title":"Static Factory"},{"location":"Scala/Scala_Design_Patterns/#algebraic-data-types-and-pattern-matching","text":"Goal: translate data descriptions into code Model data with logical ors and logical ands Two patterns: product types (and) sum types (or) Product type: A has a B and C Sum type: A is a B or C Sum and product together make algebraic data types // A has a B and C case class A ( b : B , c : C ) // A is a B or C sealed trait A case class B () extends A case class C () extends A They have only data and do not contain any functionality on top of this data as normal classes would. sealed trait Shape case class Circle ( radius : Double ) extends Shape case class Rectangle ( height : Double , width : Double ) extends Shape object Shape { def area ( shape : Shape ) : Double = shape match { case Circle ( Point ( x , y ), radius ) => Math . PI * Math . pow ( radius , 2 ) // use pattern matching to process case Rectangle ( _ , h , w ) => h * w } }","title":"Algebraic Data Types and Pattern Matching"},{"location":"Scala/Scala_Design_Patterns/#stackable-traits","text":"abstract class StringWriter { def write ( data : String ) : String } class BasicStringWriter extends StringWriter { override def write ( data : String ) : String = s\"Writing the following data: ${ data } \" } trait CapitalizingStringWriter extends StringWriter { abstract override def write ( data : String ) : String = { super . write ( data . split ( \"\\\\s+\" ). map ( _ . capitalize ). mkString ( \" \" )) } } trait UppercasingStringWriter extends StringWriter { abstract override def write ( data : String ) : String = { super . write ( data . toUpperCase ) } } object Example { def main ( args : Array [ String ]) : Unit = { val writer1 = new BasicStringWriter with UppercasingStringWriter with CapitalizingStringWriter System . out . println ( s\"Writer 1: ' ${ writer1 . write ( \"we like learning scala!\" ) } '\" ) } } Stackable traits order of execution Stackable traits are always executed from the right mixin to the left. Sometimes, however, if we only get output and it doesn't depend on what is passed to the method, we simply end up with method calls on a stack, which then get evaluated and it will appear as if things are applied from left to right.","title":"Stackable Traits"},{"location":"Scala/Scala_Design_Patterns/#components-cake-pattern","text":"http://jonasboner.com/real-world-scala-dependency-injection-di/ // Service Interfaces and Component Definitions trait OnOffDeviceComponent { val onOff : OnOffDevice // abstract val trait OnOffDevice { def on: Unit def off: Unit } } trait SensorDeviceComponent { val sensor : SensorDevice trait SensorDevice { def isCoffeePresent : Boolean } } // ======================= // Component / Service Implementations trait OnOffDeviceComponentImpl extends OnOffDeviceComponent { class Heater extends OnOffDevice { def on = println ( \"heater.on\" ) def off = println ( \"heater.off\" ) } } trait SensorDeviceComponentImpl extends SensorDeviceComponent { class PotSensor extends SensorDevice { def isCoffeePresent = true } } // ======================= // Component declaring two dependencies that it wants injected trait WarmerComponentImpl { this: SensorDeviceComponent with OnOffDeviceComponent => // Use of self-type for composition class Warmer { def trigger = { if ( sensor . isCoffeePresent ) onOff . on else onOff . off } } } // ======================= // Instantiation (and configuration) of the services in the ComponentRegistry module object ComponentRegistry extends OnOffDeviceComponentImpl with SensorDeviceComponentImpl with WarmerComponentImpl { val onOff = new Heater // all instantiations in one spot; can be easily be replaced by e.g. mocks val sensor = new PotSensor val warmer = new Warmer } // ======================= val warmer = ComponentRegistry . warmer warmer . trigger","title":"Components / Cake Pattern"},{"location":"Scala/Scala_Design_Patterns/#type-classes-using-context-bound-type-parameters","text":"Ad-hoc polymorphism Break free from your class oppressors! Concerns that cross class hierarchy e.g. serialize to JSON Common behaviour without (useful) common type Abstract behaviour to a type class Can implement type class instances in ad-hoc manner // Define some behavior in terms of operations that a type must support in order to be considered a member of the type class. trait Number [ T ] { def plus ( x : T , y : T ) : T def divide ( x : T , y : Int ) : T } // Define the default type class members in the companion object of the trait object Number { implicit object DoubleNumber extends Number [ Double ] { // note the implicit override def plus ( x : Double , y : Double ) : Double = x + y override def divide ( x : Double , y : Int ) : Double = x / y } } object Stats { // older pattern with implicit parameter // def mean[T](xs: Vector[T])(implicit ev: Number[T]): T = // note the implicit // ev.divide(xs.reduce(ev.plus(_, _)), xs.size) def mean [ T: Number ]( xs : Vector [ T ]) : T = // note the context bound implicitly [ Number [ T ]]. divide ( xs . reduce ( implicitly [ Number [ T ]]. plus ( _ , _ )), // retrieve the evidence via implicitly[] xs . size ) }","title":"Type Classes (using context-bound type parameters)"},{"location":"Scala/Scala_Design_Patterns/#visitor-pattern","text":"abstract class Element ( text : String ) { def accept ( visitor : Visitor ) } case class Title ( text : String ) extends Element ( text ) { override def accept ( visitor : Visitor ) : Unit = { visitor . visit ( this ) } } case class Text ( text : String ) extends Element ( text ) { override def accept ( visitor : Visitor ) : Unit = { visitor . visit ( this ) } } class Document ( parts : List [ Element ]) { def accept ( visitor : Visitor ) : Unit = { parts . foreach ( p => p . accept ( visitor )) } } trait Visitor { def visit ( element : Element ) } class VisitorImpl1 extends Visitor { override def visit ( element : Element ) : Unit = { element match { case Title ( text ) => ??? case Text ( text ) => ??? //... } } }","title":"Visitor Pattern"},{"location":"Scala/Scala_Design_Patterns/#configuration","text":"import com.typesafe.config.ConfigFactory trait AppConfigComponent { val appConfigService : AppConfigService class AppConfigService () { //-Dconfig.resource=production.conf for overriding private val conf = ConfigFactory . load () private val appConf = conf . getConfig ( \"job-scheduler\" ) private val db = appConf . getConfig ( \"db\" ) val configPath = appConf . getString ( \"config-path\" ) val configExtension = appConf . getString ( \"config-extension\" ) val workers = appConf . getInt ( \"workers\" ) val dbConnectionString = db . getString ( \"connection-string\" ) val dbUsername = db . getString ( \"username\" ) val dbPassword = db . getString ( \"password\" ) } }","title":"Configuration"},{"location":"Scala/Scala_Design_Patterns/#memoization","text":"import scala.collection.mutable.Map trait Memoizer { def memo [ X , Y ]( f : X => Y ) : ( X => Y ) = { val cache = Map [ X , Y ]() ( x : X ) => cache . getOrElseUpdate ( x , f ( x )) } } Using scalaz: val memoScalaz : String => String = Memo . mutableHashMapMemo { func }","title":"Memoization"},{"location":"Scala/Scala_Design_Patterns/#pimp-my-library-pattern","text":"The pimp my library design pattern is really similar to extension methods in C#. package object pimp { implicit class StringExtensions ( val s : String ) extends AnyVal { def isAllUpperCase : Boolean = ( 0 to s . size - 1 ). find { case index => ! s . charAt ( index ). isUpper }. isEmpty } }","title":"Pimp my Library Pattern"},{"location":"Scala/Scala_Generalities/","text":"Main Features of Scala \u00b6 All types are objects Type inference Nested Functions Functions are objects Domain specific language (DSL) support Traits Closures Concurrency support inspired by Erlang Tools and Frameworks \u00b6 REPL http://ammonite.io/ https://scalafiddle.io/ The Lift Framework The Play framework The Bowler framework Akka https://typelevel.org/ Install \u00b6 Need to have Java Software Development Kit (SDK) installed java -version export JAVA_HOME = /usr/local/java-current export PATH = $PATH : $JAVA_HOME /bin/ http://www.scala-lang.org/download/ Compilation \u00b6 scalac HelloWorld.scala // produces HelloWorld.class scala -classpath . HelloWorld","title":"Scala (Generalities)"},{"location":"Scala/Scala_Generalities/#main-features-of-scala","text":"All types are objects Type inference Nested Functions Functions are objects Domain specific language (DSL) support Traits Closures Concurrency support inspired by Erlang","title":"Main Features of Scala"},{"location":"Scala/Scala_Generalities/#tools-and-frameworks","text":"REPL http://ammonite.io/ https://scalafiddle.io/ The Lift Framework The Play framework The Bowler framework Akka https://typelevel.org/","title":"Tools and Frameworks"},{"location":"Scala/Scala_Generalities/#install","text":"Need to have Java Software Development Kit (SDK) installed java -version export JAVA_HOME = /usr/local/java-current export PATH = $PATH : $JAVA_HOME /bin/ http://www.scala-lang.org/download/","title":"Install"},{"location":"Scala/Scala_Generalities/#compilation","text":"scalac HelloWorld.scala // produces HelloWorld.class scala -classpath . HelloWorld","title":"Compilation"},{"location":"Scala/Scala_Language/","text":"Links \u00b6 Scala Cheatsheet Scala @ TutorialPoint Scala Tutorial (PDF) Some examples are derived from Scala Koans . Basics \u00b6 Style \u00b6 Class Names - For all class names, the first letter should be in Upper Case. If several words are used to form a name of the class, each inner word's first letter should be in Upper Case. class MyFirstScalaClass Method Names - All method names should start with a Lower Case letter. If multiple words are used to form the name of the method, then each inner word's first letter should be in Upper Case. def myMethodName() Program File Name - Name of the program file should exactly match the object name. When saving the file you should save it using the object name (Remember Scala is case-sensitive) and append \".scala\" to the end of the name. If the file name and the object name do not match your program will not compile. Assume 'HelloWorld' is the object name: the file should be saved as 'HelloWorld.scala'. Packages \u00b6 package pkg // at start of file package pkg { ... } // bracket style Imports \u00b6 import scala.collection._ // wildcard import. When importing all the names of a package or class, one uses the underscore character (_) instead of the asterisk (*). import scala.collection.Vector // one class import import scala.collection. { Vector , Sequence } // selective import. Multiple classes can be imported from the same package by enclosing them in curly braces import scala.collection. { Vector => Vec28 } // renaming import. import java.util. { Date => _ , _ } // import all from java.util except Date. All classes from the java.lang package are imported by default. The Predef object provides definitions that are accessible in all Scala compilation units without explicit qualification: - immutable Map, Set, List, ::, Nil, print, println, assert, assume, require, ensuring import scala.collection.mutable.HashMap // Mutable collections must be imported. import scala.collection.immutable. { TreeMap , TreeSet } // So are specialized collections. Application Entry Point \u00b6 object HelloWorld { def main ( args : Array [ String ]) { println ( \"Hello, world!\" ) } } Blocks \u00b6 You can combine expressions by surrounding them with {}. We call this a block. The result of the last expression in the block is the result of the overall block, too. println ({ val x = 1 + 1 x + 1 }) // 3 Variables and Values \u00b6 var x = 5 // variable val x = 5 // immutable value / \"const\" var x : Double = 5 // explicit type println ( x ) A lazy val is assignment that will not evaluated until it is called. Note there is no lazy var lazy val a = { heavymath (); 19 } Literals \u00b6 val a = 2 // int val b = 31L // long val c = 0x30B // hexadecimal val d = 3 f // float val e = 3.22d // double val f = 93 e - 9 val g = 'a' // character val h = '\\u0061' // unicode for a val i = ' \\ 141 ' // octal for a val j = '\\\"' // escape sequences val k = '\\\\' val s = \"To be or not to be\" // string s . charAt ( 0 ) val s2 = \"\"\"An apple a day keeps the doctor away\"\"\" // multi-lines string s2 . split ( '\\n' ) val s3 = \"\"\"An apple a day |keeps the doctor away\"\"\" // Multiline String literals can use | to specify the starting position of subsequent lines, then use stripMargin to remove the surplus indentation. s3 . stripMargin Enumerations \u00b6 object Planets extends Enumeration { val Mercury = Value val Venus = Value val Earth = Value val Mars = Value val Jupiter = Value val Saturn = Value val Uranus = Value val Neptune = Value val Pluto = Value } Planets . Mercury . id Planets . Mercury . toString //How does it get the name? by Reflection. object GreekPlanets extends Enumeration { val Mercury = Value ( 1 , \"Hermes\" ) // enumeration with your own index and/or your own Strings val Venus = Value ( 2 , \"Aphrodite\" ) //Fun Fact: Tellus is Roman for (Mother) Earth val Earth = Value ( 3 , \"Gaia\" ) val Mars = Value ( 4 , \"Ares\" ) val Jupiter = Value ( 5 , \"Zeus\" ) val Saturn = Value ( 6 , \"Cronus\" ) val Uranus = Value ( 7 , \"Ouranus\" ) val Neptune = Value ( 8 , \"Poseidon\" ) val Pluto = Value ( 9 , \"Hades\" ) } Common Data Structures \u00b6 ( 1 , 2 , 3 ) // tuple literal. (Tuple3) var ( x , y , z ) = ( 1 , 2 , 3 ) // destructuring bind: tuple unpacking via pattern matching. // BAD var x,y,z = (1,2,3) // hidden error: each assigned to the entire tuple. val tuple = ( \"apple\" , 3 ) // mixed type tuple tuple . _1 tuple . _2 tuple . swap var xs = List ( 1 , 2 , 3 ) // list (immutable). xs ( 2 ) // paren indexing 1 :: List ( 2 , 3 ) // cons (create a new list by prepending the element). 1 to 5 // Range sugar. Same as `1 until 6` 1 to 10 by 2 Range ( 1 , 10 , 2 ) // Range does not include the last item, even in a step increment Range ( 1 , 9 , 2 ). inclusive () // (empty parens) sole member of the Unit type (like C/Java void). Control Constructs \u00b6 if ( check ) happy else sad // conditional. if ( check ) happy // if ( check ) happy else () // same as above while ( x < 5 ) { println ( x ); x += 1 } // while loop. do { println ( x ); x += 1 } while ( x < 5 ) // do while loop. for ( x <- xs if x % 2 == 0 ) yield x * 10 // for comprehension with guard xs . filter ( _ % 2 == 0 ). map ( _ * 10 ) // same as filter/map for (( x , y ) <- xs zip ys ) yield x * y // for comprehension: destructuring bind ( xs zip ys ) map { case ( x , y ) => x * y } // same as for ( x <- xs ; y <- ys ) yield x * y // for comprehension: cross product. Later generators varying more rapidly than earlier ones xs flatMap { x => ys map { y => x * y }} // same as for ( x <- xs ; y <- ys ) { println ( \"%d/%d = %.1f\" . format ( x , y , x / y . toFloat )) // for comprehension: imperative-ish } for ( i <- 1 to 5 ) { // for comprehension: iterate including the upper bound println ( i ) } for ( i <- 1 until 5 ) { // for comprehension: iterate omitting the upper bound println ( i ) } import scala.util.control.Breaks._ // break breakable { for ( x <- xs ) { if ( Math . random < 0.1 ) break } } Formatting and Interpolation \u00b6 val helloMessage = \"Hello World\" s\"Application $helloMessage \" // string interpolation; can include expressions which can include numbers and strings // use `f` prefix before the string instead of an `s` for sprintf formatting Functions \u00b6 Scala is a functional language in the sense that every function is a value and every value is an object so ultimately every function is an object. Scala provides a lightweight syntax for defining anonymous functions, it supports higher-order functions, it allows functions to be nested, and supports currying. def add ( x : Int , y : Int ) : Int = x + y // the return type is declared after the parameter list and a colon // GOOD def f(x: Any) = println(x) // BAD def f(x) = println(x) // syntax error: need types for every arg. def f ( x : Int ) = { // inferred return type val square = x * x square . toString } // The last expression in the body is the method\u2019s return value. (Scala does have a return keyword, but it\u2019s rarely used.) // BAD def f(x: Int) { x*x } hidden error: without = it\u2019s a Unit-returning procedure; causes havoc // When performing recursion, the return type on the method is mandatory! Backticks for reserved keywords and identifiers with a space (rare) def `put employee on probation` ( employee : Employee ) = { new Employee ( employee . `first name` , employee . `last name` , \"Probation\" ) } Multiple parameter lists or none at all \u00b6 def addThenMultiply ( x : Int , y : Int )( multiplier : Int ) : Int = ( x + y ) * multiplier def name : String = System . getProperty ( \"name\" ) Procedures \u00b6 def foo ( x : Int ) { //Note: No `=`; returns Unit print ( x . toString ) } def foo ( x : Int ) : Unit = print ( x . toString ) // or Convention (not required for the compiler) states that if you a call a method that returns a Unit / has a side effect, invoke that method with empty parenthesis, other leave the parenthesis out def performSideEffect () : Unit = System . currentTimeMillis performSideEffect () Default and named parameters \u00b6 def addColorsWithDefaults ( red : Int = 0 , green : Int = 0 , blue : Int = 0 ) = { ( red , green , blue ) } me . addColors ( blue = 40 ) Variable Length Arguments \u00b6 def sum ( args : Int* ) = args . reduceLeft ( _ + _ ) // varargs. must be last arg def capitalizeAll ( args : String* ) = { args . map { arg => arg . capitalize } } capitalizeAll ( \"rarity\" , \"applejack\" ) If you want a collection expanded into a vararg, add :_* def repeatedParameterMethod ( x : Int , y : String , z : Any* ) = { \"%d %ss can give you %s\" . format ( x , y , z . mkString ( \", \" )) } repeatedParameterMethod ( 3 , \"egg\" , List ( \"a delicious sandwich\" , \"protein\" , \"high cholesterol\" ) :_ * ) should be ( __ ) Tail recursion \u00b6 As a precaution, the helpful @tailrec annotation will throw a compile time if a method is not tail recursive, meaning that the last call and only call of the method is the recursive method. Scala optimizes recursive calls to a loop from a stack import scala.annotation.tailrec // importing annotation! @tailrec // compiler will check that the function is tail recursive def factorial ( i : BigInt ) : BigInt = { @tailrec def fact ( i : BigInt , accumulator : BigInt ) : BigInt = { // methods can be placed inside in methods; return type is obligatory if ( i <= 1 ) accumulator else fact ( i - 1 , i * accumulator ) } fact ( i , 1 ) } factorial ( 3 ) Infix, Postfix and Prefix Notations; Operators \u00b6 object FrenchDate { def main ( args : Array [ String ]) { val now = new Date val df = getDateInstance ( LONG , Locale . FRANCE ) println ( df format now ) // Methods taking one argument can be used with an infix syntax. Equivalent to df.format(now) } } 1 + 2 * 3 / x consists exclusively of method calls, because it is equivalent to the following expression: (1).+(((2).*(3))./(x)) This also means that +, *, etc. are valid identifiers in Scala. Infix Operators do NOT work if an object has a method that takes two parameters. val g : Int = 31 val s : String = g toHexString // Postfix operators work if an object has a method that takes no parameters Prefix operators work if an object has a method name that starts with unary_ class Stereo { def unary_+ = \"on\" def unary_- = \"off\" } val stereo = new Stereo + stereo // it is on Methods with colons are right-associative, that means the object that a method is on will be on the right and the method parameter will be on the left class Foo ( y : Int ) { def ~:( n : Int ) = n + y + 3 } val foo = new Foo ( 9 ) 10 ~: foo foo .~:( 10 ) // same as Anonymous Functions \u00b6 def lambda = ( x : Int ) => x + 1 // other variants def lambda2 = { x : Int => x + 1 } val lambda3 = new Function1 [ Int , Int ] { def apply ( v1 : Int ) : Int = v1 + 1 } val everything = () => 42 // without parameter val add = ( x : Int , y : Int ) => x + y // multiple parameters ( 1 to 5 ). map ( _ * 2 ) // underscore notation. ( 1 to 5 ) map ( _ * 2 ) // same with infix sugar. ( 1 to 5 ). reduceLeft ( _ + _ ) // underscores are positionally matched 1st and 2nd args. ( 1 to 5 ). map ( x => x * x ) // to use an arg twice, have to name it. ( 1 to 5 ). map { x => val y = x * 2 ; println ( y ); y } // block style returns last expression. ( 1 to 5 ) filter { _ % 2 == 0 } map { _ * 2 } // pipeline style (works with parens too). // GOOD (1 to 5).map(2*) // BAD (1 to 5).map(*2) // anonymous function: bound infix method. Use 2*_ for sanity\u2019s sake instead. def compose ( g : R => R , h : R => R ) = ( x : R ) => g ( h ( x )) val f = compose ({ _ * 2 }, { _ - 1 }) // anonymous functions: to pass in multiple blocks, need outer parens. Passing anonymous functions as parameter: def makeWhatEverYouLike ( xs : List [ String ], func : String => String ) = { xs map func } Function returning another function using an anonymous function: def add ( x : Int ) = ( y : Int ) => x + y Function Values: object Timer { def oncePerSecond ( callback : () => Unit ) { // () => T is a Function type that takes a Unit type. Unit is known as 'void' to a Java programmer. while ( true ) { callback (); Thread sleep 1000 } } def timeFlies () { println ( \"time flies like an arrow...\" ) } def main ( args : Array [ String ]) { oncePerSecond ( timeFlies ) // function value; could also be () => timeFlies() } } By-name parameter \u00b6 This is used extensively in scala to create blocks. def calc ( x : => Int ) : Either [ Throwable , Int ] = { //x is a call by name parameter; delayed execution of x try { Right ( x ) } catch { case b : Throwable => Left ( b ) } } val y = calc { //This looks like a natural block println ( \"Here we go!\" ) //Some superfluous call 49 + 20 } By name parameters can also be used with an Object and apply to make interesting block-like calls object PigLatinizer { def apply ( x : => String ) = x . tail + x . head + \"ay\" } val result = PigLatinizer { val x = \"pret\" val z = \"zel\" x ++ z //concatenate the strings } Closures \u00b6 var incrementer = 1 def closure = { x : Int => x + incrementer } Currying \u00b6 val zscore = ( mean : R , sd : R ) => ( x : R ) => ( x - mean )/ sd // currying, obvious syntax. def zscore ( mean : R , sd : R ) = ( x : R ) => ( x - mean )/ sd // currying, obvious syntax def zscore ( mean : R , sd : R )( x : R ) = ( x - mean )/ sd // currying, sugar syntax. but then: val normer = zscore ( 7 , 0.4 ) _ // need trailing underscore to get the partial, only for the sugar version. def mapmake [ T ]( g : T => T )( seq : List [ T ]) = seq . map ( g ) // generic type. def multiply ( x : Int , y : Int ) = x * y val multiplyCurried = ( multiply _ ). curried multiply ( 4 , 5 ) multiplyCurried ( 3 )( 2 ) Partial Applications \u00b6 def adder ( m : Int , n : Int ) = m + n val add2 = adder ( 2 , _: Int ) // You can partially apply any argument in the argument list, not just the last one. add2 ( 3 ) // which is 5 val add3 = adder _ // underscore to convert from a function to a lambda adder ( 1 , 9 ) add3 ( 1 , 9 ) Partial Functions \u00b6 val doubleEvens : PartialFunction [ Int , Int ] = new PartialFunction [ Int , Int ] { // full declaration //States that this partial function will take on the task def isDefinedAt ( x : Int ) = x % 2 == 0 //What we do if this does partial function matches def apply ( v1 : Int ) = v1 * 2 } val tripleOdds : PartialFunction [ Int , Int ] = { case x : Int if ( x % 2 ) != 0 => x * 3 // syntaxic sugar (usual way) } val whatToDo = doubleEvens orElse tripleOdds // combine the partial functions together: OrElse val addFive = ( x : Int ) => x + 5 val whatToDo = doubleEvens orElse tripleOdds andThen addFive // chain (partial) functions together: andThen Classes, Objects, and Traits \u00b6 class C ( x : R ) // constructor params - x is only available in class body class C ( val x : R ) // c.x constructor params - automatic public (immutable) member defined class D ( var x : R ) // you can define class with var or val parameters class C ( var x : R ) { assert ( x > 0 , \"positive please\" ) // constructor is class body var y = x // declare a public member val readonly = 5 // declare a gettable but not settable member private var secret = 1 // declare a private member def this = this ( 42 ) // alternative constructor } new { ... } // anonymous class abstract class D { ... } // define an abstract(non-createable) class. class C extends D { ... } // define an inherited class. Class hierarchy is linear, a class can only extend from one parent class class C ( x : R ) extends D ( x ) // inheritance and constructor params. (wishlist: automatically pass-up params by default) // A class can be placed inside another class object O extends D { ... } // define a singleton. trait T { ... } // traits. See below. class C extends T { ... } class C extends D with T { ... } // interfaces-with-implementation. no constructor params. mixin-able. trait T1 ; trait T2 class C extends T1 with T2 // multiple traits. class C extends D with T1 with T2 // parent class and (multiple) trait(s). class C extends D { override def f = ...} // must declare method overrides. var c = new C ( 4 ) // Instantiation //BAD new List[Int] //GOOD List(1,2,3) // Instead, convention: callable factory shadowing the type classOf [ String ] // class literal. classOf [ String ]. getCanonicalName classOf [ String ]. getSimpleName val zoom = \"zoom\" zoom . getClass == classOf [ String ] x . isInstanceOf [ String ] // type check (runtime) x . asInstanceOf [ String ] // type cast (runtime) x : String // compare to parameter ascription (compile time) Methods \u00b6 class Complex ( real : Double , imaginary : Double ) { def re = real // return type inferred automatically by the compiler def im = imaginary // methods without arguments def print () : Unit = println ( s\" $real + i * $imaginary \" ) override def toString () = \"\" + re + ( if ( im < 0 ) \"\" else \"+\" ) + im + \"i\" // override methods inherited from a super-class } Asserts and Contracts \u00b6 Asserts take a boolean argument and can take a message. assert ( true ) // should be true assert ( true , \"This should be true\" ) def addNaturals ( nats : List [ Int ]) : Int = { require ( nats forall ( _ >= 0 ), \"List contains negative numbers\" ) nats . foldLeft ( 0 )( _ + _ ) } ensuring ( _ >= 0 ) Path-dependent Classes \u00b6 When a class is instantiated inside of another object, it belongs to the instance. This is a path dependent type. Once established, it cannot be placed inside of another object case class Board ( length : Int , height : Int ) { case class Coordinate ( x : Int , y : Int ) } val b1 = Board ( 20 , 20 ) val b2 = Board ( 30 , 30 ) val c1 = b1 . Coordinate ( 15 , 15 ) val c2 = b2 . Coordinate ( 25 , 25 ) // val c1 = c2 won't work Use A#B for a Java-style inner class: class Graph { class Node { var connectedNodes : List [ Graph # Node ] = Nil // accepts Nodes from any Graph def connectTo ( node : Graph # Node ) { if ( connectedNodes . find ( node . equals ). isEmpty ) { connectedNodes = node :: connectedNodes } } } var nodes : List [ Node ] = Nil def newNode : Node = { val res = new Node nodes = res :: nodes res } } Companion Objects \u00b6 Static members (methods or fields) do not exist in Scala. Rather than defining static members, the Scala programmer declares these members in singleton objects, that is a class with a single instance. object TimerAnonymous { def oncePerSecond ( callback : () => Unit ) { while ( true ) { callback (); Thread sleep 1000 } } def main ( args : Array [ String ]) { oncePerSecond (() => println ( \"time flies like an arrow...\" )) } } An object that has the same name as class is called a companion object, it is used to contain factories for the class that it complements. A companion object can also store shared variables and values for every instantiated class to share. A companion object can see private values and variables of the instantiated object Apply Method \u00b6 The apply method is a magical method in Scala. class Employee ( val firstName : String , val lastName : String ) object Employee { def apply ( firstName : String , lastName : String ) = new Employee ( firstName , lastName ) // would also work in a class, but rarer } val a = Employee ( \"John\" , \"Doe\" ) // is equivalent to var b = Employee . apply ( \"John\" , \"Doe\" ) Case Classes \u00b6 The new keyword is not mandatory to create instances of these classes (i.e. one can write Const(5) instead of new Const(5)), Getter functions are automatically defined for the constructor parameters (i.e. it is possible to get the value of the v constructor parameter of some instance c of class Const just by writing c.v), Default definitions for methods equals and hashCode are provided, which work on the structure of the instances and not on their identity, A default definition for method toString is provided, and prints the value in a source form (e.g. the tree for expression x+1 prints as Sum(Var(x),Const(1))), Instances of these classes can be decomposed through pattern matching case class Person ( first : String , last : String , age : Int = 0 ) // Case classes can have default and named parameters val p1 = Person ( \"Fred\" , \"Jones\" ) // new is optional val p2 = new Person ( \"Fred\" , \"Jones\" ) p1 == p2 // true p1 . hashCode == p2 . hashCode // true p1 eq p2 // false val p3 = p2 . copy ( first = \"Jane\" ) // copy the case class but change the name in the copy case class Dog ( var name : String , breed : String ) // Case classes can have mutable properties - potentially unsafe Case classes can be disassembled to their constituent parts as a tuple: val parts = Person . unapply ( p1 ). get // returns Option[T] parts . _1 parts . _2 Algebraic data type \u00b6 sealed trait Tree // or abstract class final case class Sum ( l : Tree , r : Tree ) extends Tree final case class Var ( n : String ) extends Tree final case class Const ( v : Int ) extends Tree Pattern Matching \u00b6 { case \"x\" => 5 } defines a partial function which, when given the string \"x\" as argument, returns the integer 5, and fails with an exception otherwise. type Environment = String => Int // the type Environment can be used as an alias of the type of functions from String to Int def eval ( t : Tree , env : Environment ) : Int = t match { case Sum ( l , r ) => eval ( l , env ) + eval ( r , env ) case Var ( n ) => env ( n ) case Const ( v ) => v } def derive ( t : Tree , v : String ) : Tree = t match { case Sum ( l , r ) => Sum ( derive ( l , v ), derive ( r , v )) case Var ( n ) if ( v == n ) => Const ( 1 ) // guard, an expression following the if keyword. case _ => Const ( 0 ) // wild-card, written _, which is a pattern matching any value, without giving it a name. } // GOOD (xs zip ys) map { case (x,y) => x*y } // BAD (xs zip ys) map( (x,y) => x*y ) // use case in function args for pattern matching. // BAD val v42 = 42 Some ( 3 ) match { case Some ( v42 ) => println ( \"42\" ) case _ => println ( \"Not 42\" ) } // \u201cv42\u201d is interpreted as a name matching any Int value, and \u201c42\u201d is printed. // GOOD val v42 = 42 Some ( 3 ) match { case Some ( `v42` ) => println ( \"42\" ) case _ => println ( \"Not 42\" ) } // \u201d`v42`\u201d with backticks is interpreted as the existing val v42, and \u201cNot 42\u201d is printed. // GOOD val UppercaseVal = 42 Some ( 3 ) match { case Some ( UppercaseVal ) => println ( \"42\" ) case _ => println ( \"Not 42\" ) } // UppercaseVal is treated as an existing val, rather than a new pattern variable, because it starts with an uppercase letter. // Thus, the value contained within UppercaseVal is checked against 3, and \u201cNot 42\u201d is printed. List Matching \u00b6 val secondElement = List ( 1 , 2 , 3 ) match { case x :: y :: xs => xs case x :: Nil => x case _ => 0 } Regex \u00b6 val MyRegularExpression = \"\"\"a=([^,]+),\\s+b=(.+)\"\"\" . r //.r turns a String to a regular expression expr match { case ( MyRegularExpression ( a , b )) => a + b } import scala.util.matching.Regex val numberPattern : Regex = \"[0-9]\" . r numberPattern . findFirstMatchIn ( \"awesomepassword\" ) match { case Some ( _ ) => println ( \"Password OK\" ) case None => println ( \"Password must contain a number\" ) } With groups: val keyValPattern : Regex = \"([0-9a-zA-Z-#() ]+): ([0-9a-zA-Z-#() ]+)\" . r for ( patternMatch <- keyValPattern . findAllMatchIn ( input )) println ( s\"key: ${ patternMatch . group ( 1 ) } value: ${ patternMatch . group ( 2 ) } \" ) Extractors (unapply) \u00b6 class Car ( val make : String , val model : String , val year : Short , val topSpeed : Short ) object Car { // What is typical is to create a custom extractor in the companion object of the class. def unapply ( x : Car ) = Some ( x . make , x . model , x . year , x . topSpeed ) // returns an Option[T] } val Car ( a , b , c , d ) = new Car ( \"Chevy\" , \"Camaro\" , 1978 , 120 ) // assign values to a .. d val x = new Car ( \"Chevy\" , \"Camaro\" , 1978 , 120 ) match { // pattern matching case Car ( s , t , _ , _ ) => ( s , t ) // _ for variables we don't care about. case _ => ( \"Ford\" , \"Edsel\" ) // fallback } As long as the method signatures aren't the same, you can have an many unapply methods as you want in the same class / object. When you create a case class, it automatically can be used with pattern matching since it has an extractor. Value Class \u00b6 Avoid allocating runtime objects. class Wrapper ( val underlying : Int ) extends AnyVal { def foo : Wrapper = new Wrapper ( underlying * 19 ) } It has a single, public val parameter that is the underlying runtime representation. The type at compile time is Wrapper, but at runtime, the representation is an Int. A value class can define defs, but no vals, vars, or nested traitss, classes or objects A value class can only extend universal traits and cannot be extended itself. A universal trait is a trait that extends Any, only has defs as members, and does no initialization. Universal traits allow basic inheritance of methods for value classes, but they incur the overhead of allocation. Traits \u00b6 Apart from inheriting code from a super-class, a Scala class can also import code from one or several traits i.e. interfaces which can also contain code. In Scala, when a class inherits from a trait, it implements that traits's interface, and inherits all the code contained in the trait. trait Ord { def < ( that : Any ) : Boolean // The type Any which is used above is the type which is a super-type of all other types in Scala def <= ( that: Any ) : Boolean = ( this < that ) || ( this == that ) def > ( that : Any ) : Boolean = !( this <= that ) def >=( that : Any ) : Boolean = !( this < that ) } class Date ( y : Int , m : Int , d : Int ) extends Ord { def year = y def month = m def day = d override def toString () : String = year + \"-\" + month + \"-\" + day override def equals ( that : Any ) : Boolean = that . isInstanceOf [ Date ] && { val o = that . asInstanceOf [ Date ] o . day == day && o . month == month && o . year == year } def <( that : Any ) : Boolean = { // The trait declare the type (e.g. method), where a concrete implementer will satisfy the type if (! that . isInstanceOf [ Date ]) error ( \"cannot compare \" + that + \" and a Date\" ) val o = that . asInstanceOf [ Date ]( year < o . year ) || ( year == o . year && ( month < o . month || ( month == o . month && day < o . day ))) } } Traits can have concrete implementations that can be mixed into concrete classes with its own state Traits can be mixed in during instantiation! trait Logging { var logCache = List [ String ]() def log ( value : String ) = { logCache = logCache :+ value } def log = logCache } val a = new A ( \"stuff\" ) with Logging // mixin traits during instantiation! a . log ( \"I did something\" ) a . log . size Stackable Traits \u00b6 abstract class IntQueue { def get () : Int def put ( x : Int ) } import scala.collection.mutable.ArrayBuffer class BasicIntQueue extends IntQueue { private val buf = new ArrayBuffer [ Int ] def get () = buf . remove ( 0 ) def put ( x : Int ) { buf += x } } trait Doubling extends IntQueue { abstract override def put ( x : Int ) { super . put ( 2 * x ) } // abstract override is necessary to stack traits } class MyQueue extends BasicIntQueue with Doubling // could also mixin during instantiation val myQueue = new MyQueue myQueue . put ( 3 ) myQueue . get () More traits can be stacked one atop another, make sure that all overrides are labelled abstract override . The order of the mixins are important. Traits on the right take effect first. Traits are instantiated before a classes instantiation from left to right. Linerization: the diamond inheritance problem is avoided since instantiations are tracked and will not allow multiple instantiations of the same parent trait Classes versus Traits \u00b6 Use classes: When a behavior is not going to be reused at all or in multiple places When you plan to use your Scala code from another language, for example, if you are building a library that could be used in Java Use traits: When a behavior is going to be reused in multiple unrelated classes. When you want to define interfaces and want to use them outside Scala, for example Java. The reason is that the traits that do not have any implementations are compiled similar to interfaces. Keyword List \u00b6 abstract case catch class def do else extends false final finally for forSome if implicit import lazy match new Null object override package private protected return sealed super this throw trait Try true type val var while with yield - : = => <- <: <% >: # @","title":"Scala Language"},{"location":"Scala/Scala_Language/#links","text":"Scala Cheatsheet Scala @ TutorialPoint Scala Tutorial (PDF) Some examples are derived from Scala Koans .","title":"Links"},{"location":"Scala/Scala_Language/#basics","text":"","title":"Basics"},{"location":"Scala/Scala_Language/#style","text":"Class Names - For all class names, the first letter should be in Upper Case. If several words are used to form a name of the class, each inner word's first letter should be in Upper Case. class MyFirstScalaClass Method Names - All method names should start with a Lower Case letter. If multiple words are used to form the name of the method, then each inner word's first letter should be in Upper Case. def myMethodName() Program File Name - Name of the program file should exactly match the object name. When saving the file you should save it using the object name (Remember Scala is case-sensitive) and append \".scala\" to the end of the name. If the file name and the object name do not match your program will not compile. Assume 'HelloWorld' is the object name: the file should be saved as 'HelloWorld.scala'.","title":"Style"},{"location":"Scala/Scala_Language/#packages","text":"package pkg // at start of file package pkg { ... } // bracket style","title":"Packages"},{"location":"Scala/Scala_Language/#imports","text":"import scala.collection._ // wildcard import. When importing all the names of a package or class, one uses the underscore character (_) instead of the asterisk (*). import scala.collection.Vector // one class import import scala.collection. { Vector , Sequence } // selective import. Multiple classes can be imported from the same package by enclosing them in curly braces import scala.collection. { Vector => Vec28 } // renaming import. import java.util. { Date => _ , _ } // import all from java.util except Date. All classes from the java.lang package are imported by default. The Predef object provides definitions that are accessible in all Scala compilation units without explicit qualification: - immutable Map, Set, List, ::, Nil, print, println, assert, assume, require, ensuring import scala.collection.mutable.HashMap // Mutable collections must be imported. import scala.collection.immutable. { TreeMap , TreeSet } // So are specialized collections.","title":"Imports"},{"location":"Scala/Scala_Language/#application-entry-point","text":"object HelloWorld { def main ( args : Array [ String ]) { println ( \"Hello, world!\" ) } }","title":"Application Entry Point"},{"location":"Scala/Scala_Language/#blocks","text":"You can combine expressions by surrounding them with {}. We call this a block. The result of the last expression in the block is the result of the overall block, too. println ({ val x = 1 + 1 x + 1 }) // 3","title":"Blocks"},{"location":"Scala/Scala_Language/#variables-and-values","text":"var x = 5 // variable val x = 5 // immutable value / \"const\" var x : Double = 5 // explicit type println ( x ) A lazy val is assignment that will not evaluated until it is called. Note there is no lazy var lazy val a = { heavymath (); 19 }","title":"Variables and Values"},{"location":"Scala/Scala_Language/#literals","text":"val a = 2 // int val b = 31L // long val c = 0x30B // hexadecimal val d = 3 f // float val e = 3.22d // double val f = 93 e - 9 val g = 'a' // character val h = '\\u0061' // unicode for a val i = ' \\ 141 ' // octal for a val j = '\\\"' // escape sequences val k = '\\\\' val s = \"To be or not to be\" // string s . charAt ( 0 ) val s2 = \"\"\"An apple a day keeps the doctor away\"\"\" // multi-lines string s2 . split ( '\\n' ) val s3 = \"\"\"An apple a day |keeps the doctor away\"\"\" // Multiline String literals can use | to specify the starting position of subsequent lines, then use stripMargin to remove the surplus indentation. s3 . stripMargin","title":"Literals"},{"location":"Scala/Scala_Language/#enumerations","text":"object Planets extends Enumeration { val Mercury = Value val Venus = Value val Earth = Value val Mars = Value val Jupiter = Value val Saturn = Value val Uranus = Value val Neptune = Value val Pluto = Value } Planets . Mercury . id Planets . Mercury . toString //How does it get the name? by Reflection. object GreekPlanets extends Enumeration { val Mercury = Value ( 1 , \"Hermes\" ) // enumeration with your own index and/or your own Strings val Venus = Value ( 2 , \"Aphrodite\" ) //Fun Fact: Tellus is Roman for (Mother) Earth val Earth = Value ( 3 , \"Gaia\" ) val Mars = Value ( 4 , \"Ares\" ) val Jupiter = Value ( 5 , \"Zeus\" ) val Saturn = Value ( 6 , \"Cronus\" ) val Uranus = Value ( 7 , \"Ouranus\" ) val Neptune = Value ( 8 , \"Poseidon\" ) val Pluto = Value ( 9 , \"Hades\" ) }","title":"Enumerations"},{"location":"Scala/Scala_Language/#common-data-structures","text":"( 1 , 2 , 3 ) // tuple literal. (Tuple3) var ( x , y , z ) = ( 1 , 2 , 3 ) // destructuring bind: tuple unpacking via pattern matching. // BAD var x,y,z = (1,2,3) // hidden error: each assigned to the entire tuple. val tuple = ( \"apple\" , 3 ) // mixed type tuple tuple . _1 tuple . _2 tuple . swap var xs = List ( 1 , 2 , 3 ) // list (immutable). xs ( 2 ) // paren indexing 1 :: List ( 2 , 3 ) // cons (create a new list by prepending the element). 1 to 5 // Range sugar. Same as `1 until 6` 1 to 10 by 2 Range ( 1 , 10 , 2 ) // Range does not include the last item, even in a step increment Range ( 1 , 9 , 2 ). inclusive () // (empty parens) sole member of the Unit type (like C/Java void).","title":"Common Data Structures"},{"location":"Scala/Scala_Language/#control-constructs","text":"if ( check ) happy else sad // conditional. if ( check ) happy // if ( check ) happy else () // same as above while ( x < 5 ) { println ( x ); x += 1 } // while loop. do { println ( x ); x += 1 } while ( x < 5 ) // do while loop. for ( x <- xs if x % 2 == 0 ) yield x * 10 // for comprehension with guard xs . filter ( _ % 2 == 0 ). map ( _ * 10 ) // same as filter/map for (( x , y ) <- xs zip ys ) yield x * y // for comprehension: destructuring bind ( xs zip ys ) map { case ( x , y ) => x * y } // same as for ( x <- xs ; y <- ys ) yield x * y // for comprehension: cross product. Later generators varying more rapidly than earlier ones xs flatMap { x => ys map { y => x * y }} // same as for ( x <- xs ; y <- ys ) { println ( \"%d/%d = %.1f\" . format ( x , y , x / y . toFloat )) // for comprehension: imperative-ish } for ( i <- 1 to 5 ) { // for comprehension: iterate including the upper bound println ( i ) } for ( i <- 1 until 5 ) { // for comprehension: iterate omitting the upper bound println ( i ) } import scala.util.control.Breaks._ // break breakable { for ( x <- xs ) { if ( Math . random < 0.1 ) break } }","title":"Control Constructs"},{"location":"Scala/Scala_Language/#formatting-and-interpolation","text":"val helloMessage = \"Hello World\" s\"Application $helloMessage \" // string interpolation; can include expressions which can include numbers and strings // use `f` prefix before the string instead of an `s` for sprintf formatting","title":"Formatting and Interpolation"},{"location":"Scala/Scala_Language/#functions","text":"Scala is a functional language in the sense that every function is a value and every value is an object so ultimately every function is an object. Scala provides a lightweight syntax for defining anonymous functions, it supports higher-order functions, it allows functions to be nested, and supports currying. def add ( x : Int , y : Int ) : Int = x + y // the return type is declared after the parameter list and a colon // GOOD def f(x: Any) = println(x) // BAD def f(x) = println(x) // syntax error: need types for every arg. def f ( x : Int ) = { // inferred return type val square = x * x square . toString } // The last expression in the body is the method\u2019s return value. (Scala does have a return keyword, but it\u2019s rarely used.) // BAD def f(x: Int) { x*x } hidden error: without = it\u2019s a Unit-returning procedure; causes havoc // When performing recursion, the return type on the method is mandatory! Backticks for reserved keywords and identifiers with a space (rare) def `put employee on probation` ( employee : Employee ) = { new Employee ( employee . `first name` , employee . `last name` , \"Probation\" ) }","title":"Functions"},{"location":"Scala/Scala_Language/#multiple-parameter-lists-or-none-at-all","text":"def addThenMultiply ( x : Int , y : Int )( multiplier : Int ) : Int = ( x + y ) * multiplier def name : String = System . getProperty ( \"name\" )","title":"Multiple parameter lists or none at all"},{"location":"Scala/Scala_Language/#procedures","text":"def foo ( x : Int ) { //Note: No `=`; returns Unit print ( x . toString ) } def foo ( x : Int ) : Unit = print ( x . toString ) // or Convention (not required for the compiler) states that if you a call a method that returns a Unit / has a side effect, invoke that method with empty parenthesis, other leave the parenthesis out def performSideEffect () : Unit = System . currentTimeMillis performSideEffect ()","title":"Procedures"},{"location":"Scala/Scala_Language/#default-and-named-parameters","text":"def addColorsWithDefaults ( red : Int = 0 , green : Int = 0 , blue : Int = 0 ) = { ( red , green , blue ) } me . addColors ( blue = 40 )","title":"Default and named parameters"},{"location":"Scala/Scala_Language/#variable-length-arguments","text":"def sum ( args : Int* ) = args . reduceLeft ( _ + _ ) // varargs. must be last arg def capitalizeAll ( args : String* ) = { args . map { arg => arg . capitalize } } capitalizeAll ( \"rarity\" , \"applejack\" ) If you want a collection expanded into a vararg, add :_* def repeatedParameterMethod ( x : Int , y : String , z : Any* ) = { \"%d %ss can give you %s\" . format ( x , y , z . mkString ( \", \" )) } repeatedParameterMethod ( 3 , \"egg\" , List ( \"a delicious sandwich\" , \"protein\" , \"high cholesterol\" ) :_ * ) should be ( __ )","title":"Variable Length Arguments"},{"location":"Scala/Scala_Language/#tail-recursion","text":"As a precaution, the helpful @tailrec annotation will throw a compile time if a method is not tail recursive, meaning that the last call and only call of the method is the recursive method. Scala optimizes recursive calls to a loop from a stack import scala.annotation.tailrec // importing annotation! @tailrec // compiler will check that the function is tail recursive def factorial ( i : BigInt ) : BigInt = { @tailrec def fact ( i : BigInt , accumulator : BigInt ) : BigInt = { // methods can be placed inside in methods; return type is obligatory if ( i <= 1 ) accumulator else fact ( i - 1 , i * accumulator ) } fact ( i , 1 ) } factorial ( 3 )","title":"Tail recursion"},{"location":"Scala/Scala_Language/#infix-postfix-and-prefix-notations-operators","text":"object FrenchDate { def main ( args : Array [ String ]) { val now = new Date val df = getDateInstance ( LONG , Locale . FRANCE ) println ( df format now ) // Methods taking one argument can be used with an infix syntax. Equivalent to df.format(now) } } 1 + 2 * 3 / x consists exclusively of method calls, because it is equivalent to the following expression: (1).+(((2).*(3))./(x)) This also means that +, *, etc. are valid identifiers in Scala. Infix Operators do NOT work if an object has a method that takes two parameters. val g : Int = 31 val s : String = g toHexString // Postfix operators work if an object has a method that takes no parameters Prefix operators work if an object has a method name that starts with unary_ class Stereo { def unary_+ = \"on\" def unary_- = \"off\" } val stereo = new Stereo + stereo // it is on Methods with colons are right-associative, that means the object that a method is on will be on the right and the method parameter will be on the left class Foo ( y : Int ) { def ~:( n : Int ) = n + y + 3 } val foo = new Foo ( 9 ) 10 ~: foo foo .~:( 10 ) // same as","title":"Infix, Postfix and Prefix Notations; Operators"},{"location":"Scala/Scala_Language/#anonymous-functions","text":"def lambda = ( x : Int ) => x + 1 // other variants def lambda2 = { x : Int => x + 1 } val lambda3 = new Function1 [ Int , Int ] { def apply ( v1 : Int ) : Int = v1 + 1 } val everything = () => 42 // without parameter val add = ( x : Int , y : Int ) => x + y // multiple parameters ( 1 to 5 ). map ( _ * 2 ) // underscore notation. ( 1 to 5 ) map ( _ * 2 ) // same with infix sugar. ( 1 to 5 ). reduceLeft ( _ + _ ) // underscores are positionally matched 1st and 2nd args. ( 1 to 5 ). map ( x => x * x ) // to use an arg twice, have to name it. ( 1 to 5 ). map { x => val y = x * 2 ; println ( y ); y } // block style returns last expression. ( 1 to 5 ) filter { _ % 2 == 0 } map { _ * 2 } // pipeline style (works with parens too). // GOOD (1 to 5).map(2*) // BAD (1 to 5).map(*2) // anonymous function: bound infix method. Use 2*_ for sanity\u2019s sake instead. def compose ( g : R => R , h : R => R ) = ( x : R ) => g ( h ( x )) val f = compose ({ _ * 2 }, { _ - 1 }) // anonymous functions: to pass in multiple blocks, need outer parens. Passing anonymous functions as parameter: def makeWhatEverYouLike ( xs : List [ String ], func : String => String ) = { xs map func } Function returning another function using an anonymous function: def add ( x : Int ) = ( y : Int ) => x + y Function Values: object Timer { def oncePerSecond ( callback : () => Unit ) { // () => T is a Function type that takes a Unit type. Unit is known as 'void' to a Java programmer. while ( true ) { callback (); Thread sleep 1000 } } def timeFlies () { println ( \"time flies like an arrow...\" ) } def main ( args : Array [ String ]) { oncePerSecond ( timeFlies ) // function value; could also be () => timeFlies() } }","title":"Anonymous Functions"},{"location":"Scala/Scala_Language/#by-name-parameter","text":"This is used extensively in scala to create blocks. def calc ( x : => Int ) : Either [ Throwable , Int ] = { //x is a call by name parameter; delayed execution of x try { Right ( x ) } catch { case b : Throwable => Left ( b ) } } val y = calc { //This looks like a natural block println ( \"Here we go!\" ) //Some superfluous call 49 + 20 } By name parameters can also be used with an Object and apply to make interesting block-like calls object PigLatinizer { def apply ( x : => String ) = x . tail + x . head + \"ay\" } val result = PigLatinizer { val x = \"pret\" val z = \"zel\" x ++ z //concatenate the strings }","title":"By-name parameter"},{"location":"Scala/Scala_Language/#closures","text":"var incrementer = 1 def closure = { x : Int => x + incrementer }","title":"Closures"},{"location":"Scala/Scala_Language/#currying","text":"val zscore = ( mean : R , sd : R ) => ( x : R ) => ( x - mean )/ sd // currying, obvious syntax. def zscore ( mean : R , sd : R ) = ( x : R ) => ( x - mean )/ sd // currying, obvious syntax def zscore ( mean : R , sd : R )( x : R ) = ( x - mean )/ sd // currying, sugar syntax. but then: val normer = zscore ( 7 , 0.4 ) _ // need trailing underscore to get the partial, only for the sugar version. def mapmake [ T ]( g : T => T )( seq : List [ T ]) = seq . map ( g ) // generic type. def multiply ( x : Int , y : Int ) = x * y val multiplyCurried = ( multiply _ ). curried multiply ( 4 , 5 ) multiplyCurried ( 3 )( 2 )","title":"Currying"},{"location":"Scala/Scala_Language/#partial-applications","text":"def adder ( m : Int , n : Int ) = m + n val add2 = adder ( 2 , _: Int ) // You can partially apply any argument in the argument list, not just the last one. add2 ( 3 ) // which is 5 val add3 = adder _ // underscore to convert from a function to a lambda adder ( 1 , 9 ) add3 ( 1 , 9 )","title":"Partial Applications"},{"location":"Scala/Scala_Language/#partial-functions","text":"val doubleEvens : PartialFunction [ Int , Int ] = new PartialFunction [ Int , Int ] { // full declaration //States that this partial function will take on the task def isDefinedAt ( x : Int ) = x % 2 == 0 //What we do if this does partial function matches def apply ( v1 : Int ) = v1 * 2 } val tripleOdds : PartialFunction [ Int , Int ] = { case x : Int if ( x % 2 ) != 0 => x * 3 // syntaxic sugar (usual way) } val whatToDo = doubleEvens orElse tripleOdds // combine the partial functions together: OrElse val addFive = ( x : Int ) => x + 5 val whatToDo = doubleEvens orElse tripleOdds andThen addFive // chain (partial) functions together: andThen","title":"Partial Functions"},{"location":"Scala/Scala_Language/#classes-objects-and-traits","text":"class C ( x : R ) // constructor params - x is only available in class body class C ( val x : R ) // c.x constructor params - automatic public (immutable) member defined class D ( var x : R ) // you can define class with var or val parameters class C ( var x : R ) { assert ( x > 0 , \"positive please\" ) // constructor is class body var y = x // declare a public member val readonly = 5 // declare a gettable but not settable member private var secret = 1 // declare a private member def this = this ( 42 ) // alternative constructor } new { ... } // anonymous class abstract class D { ... } // define an abstract(non-createable) class. class C extends D { ... } // define an inherited class. Class hierarchy is linear, a class can only extend from one parent class class C ( x : R ) extends D ( x ) // inheritance and constructor params. (wishlist: automatically pass-up params by default) // A class can be placed inside another class object O extends D { ... } // define a singleton. trait T { ... } // traits. See below. class C extends T { ... } class C extends D with T { ... } // interfaces-with-implementation. no constructor params. mixin-able. trait T1 ; trait T2 class C extends T1 with T2 // multiple traits. class C extends D with T1 with T2 // parent class and (multiple) trait(s). class C extends D { override def f = ...} // must declare method overrides. var c = new C ( 4 ) // Instantiation //BAD new List[Int] //GOOD List(1,2,3) // Instead, convention: callable factory shadowing the type classOf [ String ] // class literal. classOf [ String ]. getCanonicalName classOf [ String ]. getSimpleName val zoom = \"zoom\" zoom . getClass == classOf [ String ] x . isInstanceOf [ String ] // type check (runtime) x . asInstanceOf [ String ] // type cast (runtime) x : String // compare to parameter ascription (compile time)","title":"Classes, Objects, and Traits"},{"location":"Scala/Scala_Language/#methods","text":"class Complex ( real : Double , imaginary : Double ) { def re = real // return type inferred automatically by the compiler def im = imaginary // methods without arguments def print () : Unit = println ( s\" $real + i * $imaginary \" ) override def toString () = \"\" + re + ( if ( im < 0 ) \"\" else \"+\" ) + im + \"i\" // override methods inherited from a super-class }","title":"Methods"},{"location":"Scala/Scala_Language/#asserts-and-contracts","text":"Asserts take a boolean argument and can take a message. assert ( true ) // should be true assert ( true , \"This should be true\" ) def addNaturals ( nats : List [ Int ]) : Int = { require ( nats forall ( _ >= 0 ), \"List contains negative numbers\" ) nats . foldLeft ( 0 )( _ + _ ) } ensuring ( _ >= 0 )","title":"Asserts and Contracts"},{"location":"Scala/Scala_Language/#path-dependent-classes","text":"When a class is instantiated inside of another object, it belongs to the instance. This is a path dependent type. Once established, it cannot be placed inside of another object case class Board ( length : Int , height : Int ) { case class Coordinate ( x : Int , y : Int ) } val b1 = Board ( 20 , 20 ) val b2 = Board ( 30 , 30 ) val c1 = b1 . Coordinate ( 15 , 15 ) val c2 = b2 . Coordinate ( 25 , 25 ) // val c1 = c2 won't work Use A#B for a Java-style inner class: class Graph { class Node { var connectedNodes : List [ Graph # Node ] = Nil // accepts Nodes from any Graph def connectTo ( node : Graph # Node ) { if ( connectedNodes . find ( node . equals ). isEmpty ) { connectedNodes = node :: connectedNodes } } } var nodes : List [ Node ] = Nil def newNode : Node = { val res = new Node nodes = res :: nodes res } }","title":"Path-dependent Classes"},{"location":"Scala/Scala_Language/#companion-objects","text":"Static members (methods or fields) do not exist in Scala. Rather than defining static members, the Scala programmer declares these members in singleton objects, that is a class with a single instance. object TimerAnonymous { def oncePerSecond ( callback : () => Unit ) { while ( true ) { callback (); Thread sleep 1000 } } def main ( args : Array [ String ]) { oncePerSecond (() => println ( \"time flies like an arrow...\" )) } } An object that has the same name as class is called a companion object, it is used to contain factories for the class that it complements. A companion object can also store shared variables and values for every instantiated class to share. A companion object can see private values and variables of the instantiated object","title":"Companion Objects"},{"location":"Scala/Scala_Language/#apply-method","text":"The apply method is a magical method in Scala. class Employee ( val firstName : String , val lastName : String ) object Employee { def apply ( firstName : String , lastName : String ) = new Employee ( firstName , lastName ) // would also work in a class, but rarer } val a = Employee ( \"John\" , \"Doe\" ) // is equivalent to var b = Employee . apply ( \"John\" , \"Doe\" )","title":"Apply Method"},{"location":"Scala/Scala_Language/#case-classes","text":"The new keyword is not mandatory to create instances of these classes (i.e. one can write Const(5) instead of new Const(5)), Getter functions are automatically defined for the constructor parameters (i.e. it is possible to get the value of the v constructor parameter of some instance c of class Const just by writing c.v), Default definitions for methods equals and hashCode are provided, which work on the structure of the instances and not on their identity, A default definition for method toString is provided, and prints the value in a source form (e.g. the tree for expression x+1 prints as Sum(Var(x),Const(1))), Instances of these classes can be decomposed through pattern matching case class Person ( first : String , last : String , age : Int = 0 ) // Case classes can have default and named parameters val p1 = Person ( \"Fred\" , \"Jones\" ) // new is optional val p2 = new Person ( \"Fred\" , \"Jones\" ) p1 == p2 // true p1 . hashCode == p2 . hashCode // true p1 eq p2 // false val p3 = p2 . copy ( first = \"Jane\" ) // copy the case class but change the name in the copy case class Dog ( var name : String , breed : String ) // Case classes can have mutable properties - potentially unsafe Case classes can be disassembled to their constituent parts as a tuple: val parts = Person . unapply ( p1 ). get // returns Option[T] parts . _1 parts . _2","title":"Case Classes"},{"location":"Scala/Scala_Language/#algebraic-data-type","text":"sealed trait Tree // or abstract class final case class Sum ( l : Tree , r : Tree ) extends Tree final case class Var ( n : String ) extends Tree final case class Const ( v : Int ) extends Tree","title":"Algebraic data type"},{"location":"Scala/Scala_Language/#pattern-matching","text":"{ case \"x\" => 5 } defines a partial function which, when given the string \"x\" as argument, returns the integer 5, and fails with an exception otherwise. type Environment = String => Int // the type Environment can be used as an alias of the type of functions from String to Int def eval ( t : Tree , env : Environment ) : Int = t match { case Sum ( l , r ) => eval ( l , env ) + eval ( r , env ) case Var ( n ) => env ( n ) case Const ( v ) => v } def derive ( t : Tree , v : String ) : Tree = t match { case Sum ( l , r ) => Sum ( derive ( l , v ), derive ( r , v )) case Var ( n ) if ( v == n ) => Const ( 1 ) // guard, an expression following the if keyword. case _ => Const ( 0 ) // wild-card, written _, which is a pattern matching any value, without giving it a name. } // GOOD (xs zip ys) map { case (x,y) => x*y } // BAD (xs zip ys) map( (x,y) => x*y ) // use case in function args for pattern matching. // BAD val v42 = 42 Some ( 3 ) match { case Some ( v42 ) => println ( \"42\" ) case _ => println ( \"Not 42\" ) } // \u201cv42\u201d is interpreted as a name matching any Int value, and \u201c42\u201d is printed. // GOOD val v42 = 42 Some ( 3 ) match { case Some ( `v42` ) => println ( \"42\" ) case _ => println ( \"Not 42\" ) } // \u201d`v42`\u201d with backticks is interpreted as the existing val v42, and \u201cNot 42\u201d is printed. // GOOD val UppercaseVal = 42 Some ( 3 ) match { case Some ( UppercaseVal ) => println ( \"42\" ) case _ => println ( \"Not 42\" ) } // UppercaseVal is treated as an existing val, rather than a new pattern variable, because it starts with an uppercase letter. // Thus, the value contained within UppercaseVal is checked against 3, and \u201cNot 42\u201d is printed.","title":"Pattern Matching"},{"location":"Scala/Scala_Language/#list-matching","text":"val secondElement = List ( 1 , 2 , 3 ) match { case x :: y :: xs => xs case x :: Nil => x case _ => 0 }","title":"List Matching"},{"location":"Scala/Scala_Language/#regex","text":"val MyRegularExpression = \"\"\"a=([^,]+),\\s+b=(.+)\"\"\" . r //.r turns a String to a regular expression expr match { case ( MyRegularExpression ( a , b )) => a + b } import scala.util.matching.Regex val numberPattern : Regex = \"[0-9]\" . r numberPattern . findFirstMatchIn ( \"awesomepassword\" ) match { case Some ( _ ) => println ( \"Password OK\" ) case None => println ( \"Password must contain a number\" ) } With groups: val keyValPattern : Regex = \"([0-9a-zA-Z-#() ]+): ([0-9a-zA-Z-#() ]+)\" . r for ( patternMatch <- keyValPattern . findAllMatchIn ( input )) println ( s\"key: ${ patternMatch . group ( 1 ) } value: ${ patternMatch . group ( 2 ) } \" )","title":"Regex"},{"location":"Scala/Scala_Language/#extractors-unapply","text":"class Car ( val make : String , val model : String , val year : Short , val topSpeed : Short ) object Car { // What is typical is to create a custom extractor in the companion object of the class. def unapply ( x : Car ) = Some ( x . make , x . model , x . year , x . topSpeed ) // returns an Option[T] } val Car ( a , b , c , d ) = new Car ( \"Chevy\" , \"Camaro\" , 1978 , 120 ) // assign values to a .. d val x = new Car ( \"Chevy\" , \"Camaro\" , 1978 , 120 ) match { // pattern matching case Car ( s , t , _ , _ ) => ( s , t ) // _ for variables we don't care about. case _ => ( \"Ford\" , \"Edsel\" ) // fallback } As long as the method signatures aren't the same, you can have an many unapply methods as you want in the same class / object. When you create a case class, it automatically can be used with pattern matching since it has an extractor.","title":"Extractors (unapply)"},{"location":"Scala/Scala_Language/#value-class","text":"Avoid allocating runtime objects. class Wrapper ( val underlying : Int ) extends AnyVal { def foo : Wrapper = new Wrapper ( underlying * 19 ) } It has a single, public val parameter that is the underlying runtime representation. The type at compile time is Wrapper, but at runtime, the representation is an Int. A value class can define defs, but no vals, vars, or nested traitss, classes or objects A value class can only extend universal traits and cannot be extended itself. A universal trait is a trait that extends Any, only has defs as members, and does no initialization. Universal traits allow basic inheritance of methods for value classes, but they incur the overhead of allocation.","title":"Value Class"},{"location":"Scala/Scala_Language/#traits","text":"Apart from inheriting code from a super-class, a Scala class can also import code from one or several traits i.e. interfaces which can also contain code. In Scala, when a class inherits from a trait, it implements that traits's interface, and inherits all the code contained in the trait. trait Ord { def < ( that : Any ) : Boolean // The type Any which is used above is the type which is a super-type of all other types in Scala def <= ( that: Any ) : Boolean = ( this < that ) || ( this == that ) def > ( that : Any ) : Boolean = !( this <= that ) def >=( that : Any ) : Boolean = !( this < that ) } class Date ( y : Int , m : Int , d : Int ) extends Ord { def year = y def month = m def day = d override def toString () : String = year + \"-\" + month + \"-\" + day override def equals ( that : Any ) : Boolean = that . isInstanceOf [ Date ] && { val o = that . asInstanceOf [ Date ] o . day == day && o . month == month && o . year == year } def <( that : Any ) : Boolean = { // The trait declare the type (e.g. method), where a concrete implementer will satisfy the type if (! that . isInstanceOf [ Date ]) error ( \"cannot compare \" + that + \" and a Date\" ) val o = that . asInstanceOf [ Date ]( year < o . year ) || ( year == o . year && ( month < o . month || ( month == o . month && day < o . day ))) } } Traits can have concrete implementations that can be mixed into concrete classes with its own state Traits can be mixed in during instantiation! trait Logging { var logCache = List [ String ]() def log ( value : String ) = { logCache = logCache :+ value } def log = logCache } val a = new A ( \"stuff\" ) with Logging // mixin traits during instantiation! a . log ( \"I did something\" ) a . log . size","title":"Traits"},{"location":"Scala/Scala_Language/#stackable-traits","text":"abstract class IntQueue { def get () : Int def put ( x : Int ) } import scala.collection.mutable.ArrayBuffer class BasicIntQueue extends IntQueue { private val buf = new ArrayBuffer [ Int ] def get () = buf . remove ( 0 ) def put ( x : Int ) { buf += x } } trait Doubling extends IntQueue { abstract override def put ( x : Int ) { super . put ( 2 * x ) } // abstract override is necessary to stack traits } class MyQueue extends BasicIntQueue with Doubling // could also mixin during instantiation val myQueue = new MyQueue myQueue . put ( 3 ) myQueue . get () More traits can be stacked one atop another, make sure that all overrides are labelled abstract override . The order of the mixins are important. Traits on the right take effect first. Traits are instantiated before a classes instantiation from left to right. Linerization: the diamond inheritance problem is avoided since instantiations are tracked and will not allow multiple instantiations of the same parent trait","title":"Stackable Traits"},{"location":"Scala/Scala_Language/#classes-versus-traits","text":"Use classes: When a behavior is not going to be reused at all or in multiple places When you plan to use your Scala code from another language, for example, if you are building a library that could be used in Java Use traits: When a behavior is going to be reused in multiple unrelated classes. When you want to define interfaces and want to use them outside Scala, for example Java. The reason is that the traits that do not have any implementations are compiled similar to interfaces.","title":"Classes versus Traits"},{"location":"Scala/Scala_Language/#keyword-list","text":"abstract case catch class def do else extends false final finally for forSome if implicit import lazy match new Null object override package private protected return sealed super this throw trait Try true type val var while with yield - : = => <- <: <% >: # @","title":"Keyword List"},{"location":"Scala/Scala_Testing/","text":"Links \u00b6 http://www.scalatest.org/ Writing TDD unit tests with scalatest At a Glance Examples \u00b6 libraryDependencies += \"org.scalatest\" %% \"scalatest\" % \"2.2.6\" % \"test\" package com.acme.pizza import org.scalatest.FunSuite import org.scalatest.BeforeAndAfter class PizzaTests extends FunSuite with BeforeAndAfter { var pizza : Pizza = _ before { pizza = new Pizza } test ( \"new pizza has zero toppings\" ) { assert ( pizza . getToppings . size == 0 ) } test ( \"adding one topping\" ) { pizza . addTopping ( Topping ( \"green olives\" )) assert ( pizza . getToppings . size === 1 ) } // mark that you want a test here in the future test ( \"test pizza pricing\" ) ( pending ) } Styles \u00b6 FunSuite \u00b6 import org.scalatest.FunSuite class AddSuite extends FunSuite { test ( \"3 plus 3 is 6\" ) { assert (( 3 + 3 ) == 6 ) } } FlatSpec \u00b6 The structure of this test is flat\u2014like xUnit, but the test name can be written in specification style: import org.scalatest.FlatSpec class AddSpec extends FlatSpec { \"Addition of 3 and 3\" should \"have result 6\" in { assert (( 3 + 3 ) == 0 ) } } import collection.mutable.Stack import org.scalatest._ class ExampleSpec extends FlatSpec with Matchers { \"A Stack\" should \"pop values in last-in-first-out order\" in { val stack = new Stack [ Int ] stack . push ( 1 ) stack . push ( 2 ) stack . pop () should be ( 2 ) stack . pop () should be ( 1 ) } it should \"throw NoSuchElementException if an empty stack is popped\" in { val emptyStack = new Stack [ Int ] a [ NoSuchElementException ] should be thrownBy { emptyStack . pop () } } } FeatureSpec \u00b6 import org.scalatest._ class Calculator { def add ( a : Int , b : Int ) : Int = a + b } class CalcSpec extends FeatureSpec with GivenWhenThen { info ( \"As a calculator owner\" ) info ( \"I want to be able add two numbers\" ) info ( \"so I can get a correct result\" ) feature ( \"Addition\" ) { scenario ( \"User adds two numbers\" ) { Given ( \"a calculator\" ) val calc = new Calculator When ( \"two numbers are added\" ) var result = calc . add ( 3 , 3 ) Then ( \"we get correct result\" ) assert ( result == 6 ) } } }","title":"Scala Testing"},{"location":"Scala/Scala_Testing/#links","text":"http://www.scalatest.org/ Writing TDD unit tests with scalatest At a Glance","title":"Links"},{"location":"Scala/Scala_Testing/#examples","text":"libraryDependencies += \"org.scalatest\" %% \"scalatest\" % \"2.2.6\" % \"test\" package com.acme.pizza import org.scalatest.FunSuite import org.scalatest.BeforeAndAfter class PizzaTests extends FunSuite with BeforeAndAfter { var pizza : Pizza = _ before { pizza = new Pizza } test ( \"new pizza has zero toppings\" ) { assert ( pizza . getToppings . size == 0 ) } test ( \"adding one topping\" ) { pizza . addTopping ( Topping ( \"green olives\" )) assert ( pizza . getToppings . size === 1 ) } // mark that you want a test here in the future test ( \"test pizza pricing\" ) ( pending ) }","title":"Examples"},{"location":"Scala/Scala_Testing/#styles","text":"","title":"Styles"},{"location":"Scala/Scala_Testing/#funsuite","text":"import org.scalatest.FunSuite class AddSuite extends FunSuite { test ( \"3 plus 3 is 6\" ) { assert (( 3 + 3 ) == 6 ) } }","title":"FunSuite"},{"location":"Scala/Scala_Testing/#flatspec","text":"The structure of this test is flat\u2014like xUnit, but the test name can be written in specification style: import org.scalatest.FlatSpec class AddSpec extends FlatSpec { \"Addition of 3 and 3\" should \"have result 6\" in { assert (( 3 + 3 ) == 0 ) } } import collection.mutable.Stack import org.scalatest._ class ExampleSpec extends FlatSpec with Matchers { \"A Stack\" should \"pop values in last-in-first-out order\" in { val stack = new Stack [ Int ] stack . push ( 1 ) stack . push ( 2 ) stack . pop () should be ( 2 ) stack . pop () should be ( 1 ) } it should \"throw NoSuchElementException if an empty stack is popped\" in { val emptyStack = new Stack [ Int ] a [ NoSuchElementException ] should be thrownBy { emptyStack . pop () } } }","title":"FlatSpec"},{"location":"Scala/Scala_Testing/#featurespec","text":"import org.scalatest._ class Calculator { def add ( a : Int , b : Int ) : Int = a + b } class CalcSpec extends FeatureSpec with GivenWhenThen { info ( \"As a calculator owner\" ) info ( \"I want to be able add two numbers\" ) info ( \"so I can get a correct result\" ) feature ( \"Addition\" ) { scenario ( \"User adds two numbers\" ) { Given ( \"a calculator\" ) val calc = new Calculator When ( \"two numbers are added\" ) var result = calc . add ( 3 , 3 ) Then ( \"we get correct result\" ) assert ( result == 6 ) } } }","title":"FeatureSpec"},{"location":"Scala/Scala_Types/","text":"Type Refinement \u00b6 Type Refinement = \"subclassing without naming the subclass\". class Entity trait Persister { def doPersist ( e : Entity ) = { e . persistForReal () } } // our refined instance (and type): val refinedMockPersister = new Persister { override def doPersist ( e : Entity ) = () } Scala Types of Types Generics \u00b6 class Reference [ T ] { private var contents : T = _ // _ represents a default value. This default value is 0 for numeric types, false for the Boolean type, () for the Unit type and null for all object types. def set ( value : T ) { contents = value } def get : T = contents } trait Cache [ K , V ] { def get ( key : K ) : V def put ( key : K , value : V ) def delete ( key : K ) } def remove [ K ]( key : K ) // function Type Variance \u00b6 Covariance +A allow you to set the your container to a either a variable with the same type or parent type. class MyContainer [ +A ]( a : A )( implicit manifest : scala.reflect.Manifest [ A ]) { private [ this ] val item = a def get = item def contents = manifest . runtimeClass . getSimpleName } val fruitBasket : MyContainer [ Fruit ] = new MyContainer [ Orange ]( new Orange ()) fruitBasket . contents Contravariance -A is the opposite of covariance Declaring neither -/+, indicates invariance variance. You cannot use a superclass variable reference (\"contravariant\" position) or a subclass variable reference (\"covariant\" position) of that type. Upper and Lower Type Bounds \u00b6 abstract class Pet extends Animal { def name : String } class Cat extends Pet { override def name : String = \"Cat\" } class PetContainer [ P <: Pet ]( p : P ) { def pet : P = p // The class PetContainer take a type parameter P which must be a subtype of Pet. } Lower type bounds declare a type to be a supertype of another type. The term B >: A expresses that the type parameter B or the abstract type B refer to a supertype of type A. Abstract Types \u00b6 type R = Double // type alias trait Container { type T val data : T def compare ( other : T ) = data . equals ( other ) } class StringContainer ( val data : String ) extends Container { override type T = String } Generics vs Abstract Types \u00b6 Generics: If you need just type instantiation. A good example is the standard collection classes. If you are creating a family of types. Abstract types: If you want to allow people to mix in types using traits. If you need better readability in scenarios where both could be interchangeable. If you want to hide the type definition from the client code. Infix Type \u00b6 We can make a type infix, meaning that a generic type with two type parameters can be displayed between two types. The type specifier Pair[String,Int] can be written as String Pair Int . class Pair [ A , B ]( a : A , b : B ) type ~ [ A , B ] = Pair [ A , B ] val pairlist : List [ String ~ Int ] // operator-like usage case class Item [ T ]( i : T ) { def ~( j : Item [ T ]) = new Pair ( this , j ) // creating an infix operator method to use with our infix type } ( Item ( \"a\" ) ~ Item ( \"b\" )). isInstanceOf [ String ~ String ] ShapeLess Structural Types \u00b6 import scala.language.reflectiveCalls // use reflection --> slow def onlyThoseThatCanPerformQuacks ( quacker : { def quack:String }) : String = { \"received message: %s\" . format ( quacker . quack ) } type SpeakerAndMover = { def speak : String ; def move ( steps : Int , direction : String ) : String } // with type aliasing Self-type \u00b6 Self-types are a way to declare that a trait must be mixed into another trait, even though it doesn\u2019t directly extend it. That makes the members of the dependency available without imports. trait User { def username : String } trait Tweeter { this: User => // reassign this def tweet ( tweetText : String ) = println ( s\" $username : $tweetText \" ) } class VerifiedTweeter ( val username_ : String ) extends Tweeter with User { // We mixin User because Tweeter required it def username = s\"real $username_ \" } Difference between a self type and extending a trait \u00b6 If you say that B extends A, then B is an A. When you use self-types, B requires an A. There are two specific requirements that are created with self-types: 1. If B is extended, then you're required to mix-in an A. 1. When a concrete class finally extends/mixes-in these traits, some class/trait must implement A. trait Wrong extends Tweeter { def noCanDo = name // does not compile } If Tweeter was a subclass of User, there would be no error. In the code above, we required a User whenever Tweeter is used, however a User wasn't provided to Wrong, so we got an error. Self types allow you to define cyclical dependencies. For example, you can achieve this: trait A { self : B => } trait B { self : A => } Inheritance using extends does not allow that. Because self-types aren't part of the hierarchy of the required class they can be excluded from pattern matching, especially when you are exhaustively matching against a sealed hierarchy. This is convenient when you want to model orthogonal behaviors such as: sealed trait Person trait Student extends Person trait Teacher extends Person trait Adult { this : Person => } // orthogonal to its condition val p : Person = new Student {} p match { case s : Student => println ( \"a student\" ) case t : Teacher => println ( \"a teacher\" ) } // that's it we're exhaustive Implicits \u00b6 Implicits wrap around existing classes to provide extra functionality object MyPredef { // usually in a companion object class IntWrapper ( val original : Int ) { def isOdd = original % 2 != 0 def isEven = ! isOdd } implicit def thisMethodNameIsIrrelevant ( value : Int ) = new IntWrapper ( value ) } import MyPredef._ //imported implicits come into effect within this scope 19. isOdd // Implicits can be used to automatically convert one type to another import java.math.BigInteger implicit def Int2BigIntegerConvert ( value : Int ) : BigInteger = new BigInteger ( value . toString ) def add ( a : BigInteger , b : BigInteger ) = a . add ( b ) add ( 3 , 6 ) // 3 and 6 are Int // Implicits function parameters def howMuchCanIMake_? ( hours : Int )( implicit amount : BigDecimal , currencyName : String ) = ( amount * hours ). toString () + \" \" + currencyName implicit var hourlyRate = BigDecimal ( 34.00 ) implicit val currencyName = \"Dollars\" howMuchCanIMake_? ( 30 ) Default arguments though are preferred to Implicit Function Parameters. Context-bound Types \u00b6 def inspect [ T : TypeTag ]( l : List [ T ]) = typeOf [ T ]. typeSymbol . name . decoded val list = 1 :: 2 :: 3 :: 4 :: 5 :: Nil inspect ( list ) equivalent to def inspect [ T ]( l : List [ T ])( implicit tt : TypeTag [ T ]) = tt . tpe . typeSymbol . name . decoded val list = 1 :: 2 :: 3 :: 4 :: 5 :: Nil inspect ( list ) TypeTags can be used to determine a type used before it erased by the VM by using an implicit TypeTag argument.","title":"Scala Types"},{"location":"Scala/Scala_Types/#type-refinement","text":"Type Refinement = \"subclassing without naming the subclass\". class Entity trait Persister { def doPersist ( e : Entity ) = { e . persistForReal () } } // our refined instance (and type): val refinedMockPersister = new Persister { override def doPersist ( e : Entity ) = () } Scala Types of Types","title":"Type Refinement"},{"location":"Scala/Scala_Types/#generics","text":"class Reference [ T ] { private var contents : T = _ // _ represents a default value. This default value is 0 for numeric types, false for the Boolean type, () for the Unit type and null for all object types. def set ( value : T ) { contents = value } def get : T = contents } trait Cache [ K , V ] { def get ( key : K ) : V def put ( key : K , value : V ) def delete ( key : K ) } def remove [ K ]( key : K ) // function","title":"Generics"},{"location":"Scala/Scala_Types/#type-variance","text":"Covariance +A allow you to set the your container to a either a variable with the same type or parent type. class MyContainer [ +A ]( a : A )( implicit manifest : scala.reflect.Manifest [ A ]) { private [ this ] val item = a def get = item def contents = manifest . runtimeClass . getSimpleName } val fruitBasket : MyContainer [ Fruit ] = new MyContainer [ Orange ]( new Orange ()) fruitBasket . contents Contravariance -A is the opposite of covariance Declaring neither -/+, indicates invariance variance. You cannot use a superclass variable reference (\"contravariant\" position) or a subclass variable reference (\"covariant\" position) of that type.","title":"Type Variance"},{"location":"Scala/Scala_Types/#upper-and-lower-type-bounds","text":"abstract class Pet extends Animal { def name : String } class Cat extends Pet { override def name : String = \"Cat\" } class PetContainer [ P <: Pet ]( p : P ) { def pet : P = p // The class PetContainer take a type parameter P which must be a subtype of Pet. } Lower type bounds declare a type to be a supertype of another type. The term B >: A expresses that the type parameter B or the abstract type B refer to a supertype of type A.","title":"Upper and Lower Type Bounds"},{"location":"Scala/Scala_Types/#abstract-types","text":"type R = Double // type alias trait Container { type T val data : T def compare ( other : T ) = data . equals ( other ) } class StringContainer ( val data : String ) extends Container { override type T = String }","title":"Abstract Types"},{"location":"Scala/Scala_Types/#generics-vs-abstract-types","text":"Generics: If you need just type instantiation. A good example is the standard collection classes. If you are creating a family of types. Abstract types: If you want to allow people to mix in types using traits. If you need better readability in scenarios where both could be interchangeable. If you want to hide the type definition from the client code.","title":"Generics vs Abstract Types"},{"location":"Scala/Scala_Types/#infix-type","text":"We can make a type infix, meaning that a generic type with two type parameters can be displayed between two types. The type specifier Pair[String,Int] can be written as String Pair Int . class Pair [ A , B ]( a : A , b : B ) type ~ [ A , B ] = Pair [ A , B ] val pairlist : List [ String ~ Int ] // operator-like usage case class Item [ T ]( i : T ) { def ~( j : Item [ T ]) = new Pair ( this , j ) // creating an infix operator method to use with our infix type } ( Item ( \"a\" ) ~ Item ( \"b\" )). isInstanceOf [ String ~ String ] ShapeLess","title":"Infix Type"},{"location":"Scala/Scala_Types/#structural-types","text":"import scala.language.reflectiveCalls // use reflection --> slow def onlyThoseThatCanPerformQuacks ( quacker : { def quack:String }) : String = { \"received message: %s\" . format ( quacker . quack ) } type SpeakerAndMover = { def speak : String ; def move ( steps : Int , direction : String ) : String } // with type aliasing","title":"Structural Types"},{"location":"Scala/Scala_Types/#self-type","text":"Self-types are a way to declare that a trait must be mixed into another trait, even though it doesn\u2019t directly extend it. That makes the members of the dependency available without imports. trait User { def username : String } trait Tweeter { this: User => // reassign this def tweet ( tweetText : String ) = println ( s\" $username : $tweetText \" ) } class VerifiedTweeter ( val username_ : String ) extends Tweeter with User { // We mixin User because Tweeter required it def username = s\"real $username_ \" }","title":"Self-type"},{"location":"Scala/Scala_Types/#difference-between-a-self-type-and-extending-a-trait","text":"If you say that B extends A, then B is an A. When you use self-types, B requires an A. There are two specific requirements that are created with self-types: 1. If B is extended, then you're required to mix-in an A. 1. When a concrete class finally extends/mixes-in these traits, some class/trait must implement A. trait Wrong extends Tweeter { def noCanDo = name // does not compile } If Tweeter was a subclass of User, there would be no error. In the code above, we required a User whenever Tweeter is used, however a User wasn't provided to Wrong, so we got an error. Self types allow you to define cyclical dependencies. For example, you can achieve this: trait A { self : B => } trait B { self : A => } Inheritance using extends does not allow that. Because self-types aren't part of the hierarchy of the required class they can be excluded from pattern matching, especially when you are exhaustively matching against a sealed hierarchy. This is convenient when you want to model orthogonal behaviors such as: sealed trait Person trait Student extends Person trait Teacher extends Person trait Adult { this : Person => } // orthogonal to its condition val p : Person = new Student {} p match { case s : Student => println ( \"a student\" ) case t : Teacher => println ( \"a teacher\" ) } // that's it we're exhaustive","title":"Difference between a self type and extending a trait"},{"location":"Scala/Scala_Types/#implicits","text":"Implicits wrap around existing classes to provide extra functionality object MyPredef { // usually in a companion object class IntWrapper ( val original : Int ) { def isOdd = original % 2 != 0 def isEven = ! isOdd } implicit def thisMethodNameIsIrrelevant ( value : Int ) = new IntWrapper ( value ) } import MyPredef._ //imported implicits come into effect within this scope 19. isOdd // Implicits can be used to automatically convert one type to another import java.math.BigInteger implicit def Int2BigIntegerConvert ( value : Int ) : BigInteger = new BigInteger ( value . toString ) def add ( a : BigInteger , b : BigInteger ) = a . add ( b ) add ( 3 , 6 ) // 3 and 6 are Int // Implicits function parameters def howMuchCanIMake_? ( hours : Int )( implicit amount : BigDecimal , currencyName : String ) = ( amount * hours ). toString () + \" \" + currencyName implicit var hourlyRate = BigDecimal ( 34.00 ) implicit val currencyName = \"Dollars\" howMuchCanIMake_? ( 30 ) Default arguments though are preferred to Implicit Function Parameters.","title":"Implicits"},{"location":"Scala/Scala_Types/#context-bound-types","text":"def inspect [ T : TypeTag ]( l : List [ T ]) = typeOf [ T ]. typeSymbol . name . decoded val list = 1 :: 2 :: 3 :: 4 :: 5 :: Nil inspect ( list ) equivalent to def inspect [ T ]( l : List [ T ])( implicit tt : TypeTag [ T ]) = tt . tpe . typeSymbol . name . decoded val list = 1 :: 2 :: 3 :: 4 :: 5 :: Nil inspect ( list ) TypeTags can be used to determine a type used before it erased by the VM by using an implicit TypeTag argument.","title":"Context-bound Types"},{"location":"Scala/Scaladoc/","text":"Scaladoc Scaladoc Style Guide /** Start the comment here * and use the left star followed by a * white space on every line. * * Even on empty paragraph-break lines. * * Note that the * on each line is aligned * with the second * in /** so that the * left margin is on the same column on the * first line and on subsequent ones. * * The closing Scaladoc tag goes on its own, * separate line. E.g. * * Calculate the square of the given number * * @param d the Double to square * @return the result of squaring d */ def square(d: Double): Double = d * d Tags \u00b6 Class specific tags \u00b6 @constructor placed in the class comment will describe the primary constructor. Method specific tags @return detail the return value from a method (one per method). Method, Constructor and/or Class tags @throws what exceptions (if any) the method or constructor may throw. @param detail a value parameter for a method or constructor, provide one per parameter to the method/constructor. @tparam detail a type parameter for a method, constructor or class. Provide one per type parameter. Usage tags \u00b6 @see reference other sources of information like external document links or related entities in the documentation. @note add a note for pre or post conditions, or any other notable restrictions or expectations. @example for providing example code or related example documentation. @usecase provide a simplified method definition for when the full method definition is too complex or noisy. An example is (in the collections API), providing documentation for methods that omit the implicit canBuildFrom. Member grouping tags \u00b6 @group <group> - mark the entity as a member of the group. @groupname <group> <name> - provide an optional name for the group. is displayed as the group header before the group description. @groupdesc <group> <description> - add optional descriptive text to display under the group name. Supports multiline formatted text. @groupprio - control the order of the group on the page. Defaults to 0. Ungrouped elements have an implicit priority of 1000. Use a value between 0 and 999 to set a relative position to other groups. Low values will appear before high values. Diagram tags \u00b6 @contentDiagram - use with traits and classes to include a content hierarchy diagram showing included types. The diagram content can be fine tuned with additional specifiers taken from hideNodes, hideOutgoingImplicits, hideSubclasses, hideEdges, hideIncomingImplicits, hideSuperclasses and hideInheritedNode. hideDiagram can be supplied to prevent a diagram from being created if it would be created by default. Packages and objects have content diagrams by default. @inheritanceDiagram Other tags \u00b6 @author provide author information for the following entity @version the version of the system or API that this entity is a part of. @since like @version but defines the system or API that this entity was first defined in. @todo for documenting unimplemented features or unimplemented aspects of an entity. @deprecated marks the entity as deprecated, providing both the replacement implementation that should be used and the version/date at which this entity was deprecated. @migration like deprecated but provides advanced warning of planned changes ahead of deprecation. Same fields as @deprecated . @inheritdoc take comments from a superclass as defaults if comments are not provided locally. @documentable Expand a type alias and abstract type into a full template page. - TODO: Test the \u201cabstract type\u201d claim - no examples of this in the Scala code base Macros \u00b6 @define <name> <definition> allows use of $name in other Scaladoc comments within the same source file which will be expanded to the contents of . Markup \u00b6 `monospace` ''italic text'' '''bold text''' __underline__ ^superscript^ ,,subscript,, [[entity link]], e.g. [[scala.collection.Seq]] [[http://external.link External Link]], e.g. [[http://scala-lang.org Scala Language Site]] Other formatting notes \u00b6 Paragraphs are started with one (or more) blank lines. * in the margin for the comment is valid (and should be included) but the line should be blank otherwise. Code blocks are contained within {{{ this }}} and may be multi-line. Indentation is relative to the starting * for the comment. Headings are defined with surrounding = characters, with more = denoting subheadings. E.g. =Heading=, ==Sub-Heading==, etc. List blocks are a sequence of list items with the same style and level, with no interruptions from other block styles. Unordered lists can be bulleted using -, while numbered lists can be denoted using 1., i., I., a. for the various numbering styles.","title":"Scaladoc"},{"location":"Scala/Scaladoc/#tags","text":"","title":"Tags"},{"location":"Scala/Scaladoc/#class-specific-tags","text":"@constructor placed in the class comment will describe the primary constructor. Method specific tags @return detail the return value from a method (one per method). Method, Constructor and/or Class tags @throws what exceptions (if any) the method or constructor may throw. @param detail a value parameter for a method or constructor, provide one per parameter to the method/constructor. @tparam detail a type parameter for a method, constructor or class. Provide one per type parameter.","title":"Class specific tags"},{"location":"Scala/Scaladoc/#usage-tags","text":"@see reference other sources of information like external document links or related entities in the documentation. @note add a note for pre or post conditions, or any other notable restrictions or expectations. @example for providing example code or related example documentation. @usecase provide a simplified method definition for when the full method definition is too complex or noisy. An example is (in the collections API), providing documentation for methods that omit the implicit canBuildFrom.","title":"Usage tags"},{"location":"Scala/Scaladoc/#member-grouping-tags","text":"@group <group> - mark the entity as a member of the group. @groupname <group> <name> - provide an optional name for the group. is displayed as the group header before the group description. @groupdesc <group> <description> - add optional descriptive text to display under the group name. Supports multiline formatted text. @groupprio - control the order of the group on the page. Defaults to 0. Ungrouped elements have an implicit priority of 1000. Use a value between 0 and 999 to set a relative position to other groups. Low values will appear before high values.","title":"Member grouping tags"},{"location":"Scala/Scaladoc/#diagram-tags","text":"@contentDiagram - use with traits and classes to include a content hierarchy diagram showing included types. The diagram content can be fine tuned with additional specifiers taken from hideNodes, hideOutgoingImplicits, hideSubclasses, hideEdges, hideIncomingImplicits, hideSuperclasses and hideInheritedNode. hideDiagram can be supplied to prevent a diagram from being created if it would be created by default. Packages and objects have content diagrams by default. @inheritanceDiagram","title":"Diagram tags"},{"location":"Scala/Scaladoc/#other-tags","text":"@author provide author information for the following entity @version the version of the system or API that this entity is a part of. @since like @version but defines the system or API that this entity was first defined in. @todo for documenting unimplemented features or unimplemented aspects of an entity. @deprecated marks the entity as deprecated, providing both the replacement implementation that should be used and the version/date at which this entity was deprecated. @migration like deprecated but provides advanced warning of planned changes ahead of deprecation. Same fields as @deprecated . @inheritdoc take comments from a superclass as defaults if comments are not provided locally. @documentable Expand a type alias and abstract type into a full template page. - TODO: Test the \u201cabstract type\u201d claim - no examples of this in the Scala code base","title":"Other tags"},{"location":"Scala/Scaladoc/#macros","text":"@define <name> <definition> allows use of $name in other Scaladoc comments within the same source file which will be expanded to the contents of .","title":"Macros"},{"location":"Scala/Scaladoc/#markup","text":"`monospace` ''italic text'' '''bold text''' __underline__ ^superscript^ ,,subscript,, [[entity link]], e.g. [[scala.collection.Seq]] [[http://external.link External Link]], e.g. [[http://scala-lang.org Scala Language Site]]","title":"Markup"},{"location":"Scala/Scaladoc/#other-formatting-notes","text":"Paragraphs are started with one (or more) blank lines. * in the margin for the comment is valid (and should be included) but the line should be blank otherwise. Code blocks are contained within {{{ this }}} and may be multi-line. Indentation is relative to the starting * for the comment. Headings are defined with surrounding = characters, with more = denoting subheadings. E.g. =Heading=, ==Sub-Heading==, etc. List blocks are a sequence of list items with the same style and level, with no interruptions from other block styles. Unordered lists can be bulleted using -, while numbered lists can be denoted using 1., i., I., a. for the various numbering styles.","title":"Other formatting notes"},{"location":"Scala/sbt/","text":"SBT Links \u00b6 SBT Home Page Scala school's SBT page SBT: The Missing Tutorial Create a New Project \u00b6 $ sbt new sbt/scala-seed.g8 $ cd hello $ sbt ... > run > exit Giter8 templates Layout \u00b6 sbt uses the same directory structure as Maven for source files by default (all paths are relative to the base directory): src/ main/ resources/ <files to include in main jar here> scala/ <main Scala sources> java/ <main Java sources> test/ resources <files to include in test jar here> scala/ <test Scala sources> java/ <test Java sources> Other directories in src/ will be ignored. Additionally, all hidden directories will be ignored. Source code can be placed in the project\u2019s base directory as hello/app.scala , which may be for small projects, though for normal projects people tend to keep the projects in the src/main/ directory to keep things neat. Build Definition \u00b6 The build definition goes in a file called build.sbt , located in the project\u2019s base directory. The \u201cbase directory\u201d is the directory containing the project. In addition to build.sbt , the project directory can contain .scala files that defines helper objects and one-off plugins. build.sbt project/ Dependencies.scala .gitignore (or equivalent for other version control systems) should contain: target/ As part of your build definition, specify the version of sbt that your build uses. This allows people with different versions of the sbt launcher to build the same projects with consistent results. To do this, create a file named project/build.properties that specifies the sbt version as follows: sbt.version=1.0.2 A build definition is defined in build.sbt, and it consists of a set of projects (of type Project). Because the term project can be ambiguous, we often call it a subproject. lazy val root = ( project in file ( \".\" )) . settings ( name := \"Hello\" , scalaVersion := \"2.12.3\" ) Each subproject is configured by key-value pairs. build.sbt may also be interspersed with vals, lazy vals, and defs. Top-level objects and classes are not allowed in build.sbt . Those should go in the project/ directory as Scala source files. There are three flavors of key: SettingKey[T]: a key for a value computed once (the value is computed when loading the subproject, and kept around). TaskKey[T]: a key for a value, called a task, that has to be recomputed each time, potentially with side effects. InputKey[T]: a key for a task that has command line arguments as input. Check out Input Tasks for more details. Built-in Keys \u00b6 The built-in keys are just fields in an object called Keys. A build.sbt implicitly has an import sbt.Keys._ , so sbt.Keys.name can be referred to as name. Adding Library Dependencies \u00b6 To depend on third-party libraries, there are two options. The first is to drop jars in lib/ (unmanaged dependencies) and the other is to add managed dependencies, which will look like this in build.sbt : val derby = \"org.apache.derby\" % \"derby\" % \"10.4.1.3\" lazy val commonSettings = Seq ( organization := \"com.example\" , version := \"0.1.0-SNAPSHOT\" , scalaVersion := \"2.12.3\" ) lazy val root = ( project in file ( \".\" )) . settings ( commonSettings , name := \"Hello\" , libraryDependencies += derby ) The libraryDependencies key involves two complexities: += rather than := , and the % method. += appends to the key\u2019s old value rather than replacing it. The % method is used to construct an Ivy module ID from strings.","title":"Sbt"},{"location":"Scala/sbt/#sbt-links","text":"SBT Home Page Scala school's SBT page SBT: The Missing Tutorial","title":"SBT Links"},{"location":"Scala/sbt/#create-a-new-project","text":"$ sbt new sbt/scala-seed.g8 $ cd hello $ sbt ... > run > exit Giter8 templates","title":"Create a New Project"},{"location":"Scala/sbt/#layout","text":"sbt uses the same directory structure as Maven for source files by default (all paths are relative to the base directory): src/ main/ resources/ <files to include in main jar here> scala/ <main Scala sources> java/ <main Java sources> test/ resources <files to include in test jar here> scala/ <test Scala sources> java/ <test Java sources> Other directories in src/ will be ignored. Additionally, all hidden directories will be ignored. Source code can be placed in the project\u2019s base directory as hello/app.scala , which may be for small projects, though for normal projects people tend to keep the projects in the src/main/ directory to keep things neat.","title":"Layout"},{"location":"Scala/sbt/#build-definition","text":"The build definition goes in a file called build.sbt , located in the project\u2019s base directory. The \u201cbase directory\u201d is the directory containing the project. In addition to build.sbt , the project directory can contain .scala files that defines helper objects and one-off plugins. build.sbt project/ Dependencies.scala .gitignore (or equivalent for other version control systems) should contain: target/ As part of your build definition, specify the version of sbt that your build uses. This allows people with different versions of the sbt launcher to build the same projects with consistent results. To do this, create a file named project/build.properties that specifies the sbt version as follows: sbt.version=1.0.2 A build definition is defined in build.sbt, and it consists of a set of projects (of type Project). Because the term project can be ambiguous, we often call it a subproject. lazy val root = ( project in file ( \".\" )) . settings ( name := \"Hello\" , scalaVersion := \"2.12.3\" ) Each subproject is configured by key-value pairs. build.sbt may also be interspersed with vals, lazy vals, and defs. Top-level objects and classes are not allowed in build.sbt . Those should go in the project/ directory as Scala source files. There are three flavors of key: SettingKey[T]: a key for a value computed once (the value is computed when loading the subproject, and kept around). TaskKey[T]: a key for a value, called a task, that has to be recomputed each time, potentially with side effects. InputKey[T]: a key for a task that has command line arguments as input. Check out Input Tasks for more details.","title":"Build Definition"},{"location":"Scala/sbt/#built-in-keys","text":"The built-in keys are just fields in an object called Keys. A build.sbt implicitly has an import sbt.Keys._ , so sbt.Keys.name can be referred to as name.","title":"Built-in Keys"},{"location":"Scala/sbt/#adding-library-dependencies","text":"To depend on third-party libraries, there are two options. The first is to drop jars in lib/ (unmanaged dependencies) and the other is to add managed dependencies, which will look like this in build.sbt : val derby = \"org.apache.derby\" % \"derby\" % \"10.4.1.3\" lazy val commonSettings = Seq ( organization := \"com.example\" , version := \"0.1.0-SNAPSHOT\" , scalaVersion := \"2.12.3\" ) lazy val root = ( project in file ( \".\" )) . settings ( commonSettings , name := \"Hello\" , libraryDependencies += derby ) The libraryDependencies key involves two complexities: += rather than := , and the % method. += appends to the key\u2019s old value rather than replacing it. The % method is used to construct an Ivy module ID from strings.","title":"Adding Library Dependencies"},{"location":"Search/ElasticSearch/","text":"Cheatsheets \u00b6 Jolicode Development URLs \u00b6 Kibana (port 5601) Sense ElasticSearch (port 9200) INSTALL \u00b6 Install curl Install Java Download ElasticSearch Optionally change the cluster.name in the elasticsearch.yml configuration cd elasticsearch-<version> ./bin/elasticsearch -d # or on Windows # bin\\elasticsearch.bat curl 'http://localhost:9200/?pretty' Install Kibana Open config/kibana.yml in an editor Set the elasticsearch.url to point at your Elasticsearch instance Run ./bin/kibana (orbin\\kibana.bat on Windows) Point your browser at http://localhost:5601 Install Sense ./bin/kibana plugin --install elastic/sense On Windows: bin \\k ibana.bat plugin --install elastic/sense Then go to http://localhost:5601/app/sense CURL \u00b6 curl -X<VERB> '<PROTOCOL>://<HOST>:<PORT>/<PATH>?<QUERY_STRING>' -d '<BODY>' verb is GET, POST, PUT, HEAD, or DELETE Examples \u00b6 curl -XGET 'http://localhost:9200/_count?pretty' -d '{ \"query\": { \"match_all\": {} }}' curl -XGET <id>.us-west-2.es.amazonaws.com curl -XGET 'https://<id>.us-west-2.es.amazonaws.com/_count?pretty' -d '{ \"query\": { \"match_all\": {} } }' curl -XPUT https://<id>.us-west-2.es.amazonaws.com/movies/movie/tt0116996 -d '{\"directors\" : [\"Tim Burton\"],\"genres\" : [\"Comedy\",\"Sci-Fi\"], \"plot\": \"The Earth is invaded by Martians with irresistible weapons and a cruel sense of humor.\", \"title\" : \"Mars Attacks!\", \"actors\" :[\"Jack Nicholson\",\"Pierce Brosnan\",\"Sarah Jessica Parker\"], \"year\" : 1996}' Sense \u00b6 Sense syntax is similar to curl: Index a document PUT index/type/1 { \"body\": \"here\" } and retrieve it GET index/type/1 PLUGINS \u00b6 URL pattern http://yournode:9200/_plugin/<plugin name> On Debian, the script is in: /usr/share/elasticsearch/bin/plugin . Install various plugins ./bin/plugin --install mobz/elasticsearch-head ./bin/plugin --install lmenezes/elasticsearch-kopf/1.2 ./bin/plugin --install elasticsearch/marvel/latest Remove a plugin ./bin/plugin --remove List installed plugins ./bin/plugin --list GET /_nodes?plugin=true Elasticsearch monitoring and management plugins Head Head elasticsearch/bin/plugin -install mobz/elasticsearch-head open http://localhost:9200/_plugin/head elastichq.org BigDesk Live charts and statistics for elasticsearch cluster: BigDesk Kopf Kopf ./bin/plugin --install lmenezes/elasticsearch-kopf/1.2` Marvel ./bin/plugin --install elasticsearch/marvel/latest Integrations (CMS, import/export, hadoop...) \u00b6 Integrations Aspire Aspire Aspire is a framework and libraries of extensible components designed to enable creation of solutions to acquire data from one or more content repositories (such as file systems, relational databases, cloud storage, or content management systems), extract metadata and text from the documents, analyze, modify and enhance the content and metadata if needed, and then publish each document, together with its metadata, to a search engine or other target application Docs Integration with Hadoop Integration with Hadoop Bulk loading for elastic search http://infochimps.com Integration with Spring Spring Data WordPress Wordpress TOOLS \u00b6 BI platforms that can use ES as an analytics engine: Kibana Grafana BIRT Birt Birt Adminer Adminer.org Database management in a single PHP file. Works with MySQL, PostgreSQL, SQLite, MS SQL, Oracle, SimpleDB, Elasticsearch, MongoDB. Needs a webserver + PHP: WAMP Mongolastic A tool that migrates data from MongoDB to Elasticsearch and vice versa Mongolastic Elasticsearch-exporter Elasticsearch-exporter Code Examples - developing a Web UI for ES \u00b6 Sitepoint CottageLabs scrutmydocs.org qbox.io Java API \u00b6 Java clients elasticsearch tutorial elasticsearchfr/ IBM dzone BASICS \u00b6 An Elasticsearch cluster can contain multiple indices, which in turn contain multiple types. These types hold multiple documents, and each document has multiple fields. Explore (using Sense) \u00b6 GET _stats/ # List indices GET /_cat/indices/ GET /_cat/indices/my_ind* # Get info about one index GET /twitter GET /my_index_nr_1*/_settings?pretty or ?v GET /twitter/_settings,_mappings The available features are _settings, _mappings, _warmers and _aliases # cluster GET /_nodes # insert data PUT my_index/user/1 { \"first_name\": \"John\", \"last_name\": \"Smith\", \"date_of_birth\": \"1970-10-24\" } #search GET my_index/_search GET _count?pretty # Data schema GET my_index/_mapping INSERT DOCUMENTS \u00b6 PUT /index/type/ID PUT /megacorp/employee/1 { \"first_name\" : \"John\", \"last_name\" : \"Smith\", \"age\" : 25, \"about\" : \"I love to go rock climbing\", \"interests\": [ \"sports\", \"music\" ]} PUT /megacorp/employee/2 { \"first_name\" : \"Jane\", \"last_name\" : \"Smith\", \"age\" : 32, \"about\" : \"I like to collect rock albums\", \"interests\": [ \"music\" ]} GET /megacorp/employee/1 Field names can be any valid string, but may not include periods. Every document in Elasticsearch has a version number. Every time a change is made to a document (including deleting it), the _version number is incremented. Optimistic concurrency control PUT /website/blog/1?version=1 { \"title\": \"My first blog entry\", \"text\": \"Starting to get the hang of this...\"} We want this update to succeed only if the current _version of this document in our index is version 1 External version: PUT /website/blog/2?version=5&version_type=external { \"title\": \"My first external blog entry\", \"text\": \"Starting to get the hang of this...\"} INSERT DOCUMENTS - AUTOGENERATED IDS \u00b6 POST /website/blog/ { \"title\": \"My second blog entry\", \"text\": \"Still trying this out...\", \"date\": \"2014/01/01\" } Response: { \"_index\": \"website\", \"_type\": \"blog\", \"_id\": \"AVFgSgVHUP18jI2wRx0w\", \"_version\": 1, \"created\": true } # creating an entirely new document and not overwriting an existing one PUT /website/blog/123?op_type=create { ... } PUT /website/blog/123/_create { ... } RETRIEVE DOCUMENTS \u00b6 GET /website/blog/123 # optional ?pretty { \"_index\" : \"website\", \"_type\" : \"blog\", \"_id\" : \"123\", \"_version\" : 1, \"found\" : true, \"_source\" : { \"title\": \"My first blog entry\", \"text\": \"Just trying this out...\", \"date\": \"2014/01/01\" }} # Contains just the fields that we requested GET /website/blog/123?_source=title,text # Just get the original doc GET /website/blog/123/_source # check if doc exists -- HTTP 200 or 404 curl -i -XHEAD http://localhost:9200/website/blog/123 # Note: HEAD/exists requests do not work in Sense # because they only return HTTP headers, not # a JSON body # multiple docs at once GET /website/blog/_mget { \"ids\" : [ \"2\", \"1\" ]} UPDATE \u00b6 Documents in Elasticsearch are immutable; we cannot change them. Instead, if we need to update an existing document, we reindex or replace it # Accepts a partial document as the doc parameter, which just gets merged with the existing document. POST /website/blog/1/_update { \"doc\" : { \"tags\" : [ \"testing\" ], \"views\": 0 }} # Script POST /website/blog/1/_update { \"script\" : \"ctx._source.views+=1\"} # script with parameters POST /website/blog/1/_update { \"script\" : \"ctx._source.tags+=new_tag\", \"params\" : { \"new_tag\" : \"search\" }} # upsert POST/website/pageviews/1/_update {\"script\":\"ctx._source.views+=1\",\"upsert\":{\"views\":1}} DELETE \u00b6 DELETE /website/blog/123 # delete doc based on its contents POST /website/blog/1/_update { \"script\" : \"ctx.op = ctx._source.views == count ? 'delete' : 'none'\", \"params\" : { \"count\": 1 }} BULK \u00b6 POST /_bulk {\"delete\":{\"_index\":\"website\",\"_type\":\"blog\",\"_id\":\"123\"}} {\"create\":{\"_index\":\"website\",\"_type\":\"blog\",\"_id\":\"123\"}} # Create a document only if the document does not already exist {\"title\":\"My first blog post\"} {\"index\":{\"_index\":\"website\",\"_type\":\"blog\"}} {\"title\":\"My second blog post\"} {\"update\":{\"_index\":\"website\",\"_type\":\"blog\",\"_id\":\"123\",\"_retry_on_conflict\":3}} {\"doc\":{\"title\":\"My updated blog post\"}} Bulk in the same index or index/type POST /website/_bulk {\"index\":{\"_type\":\"log\"}} {\"event\":\"User logged in\"} {\"index\":{\"_type\":\"blog\"}} {\"title\":\"My second blog post\"} Try around 5-15MB in size. SEARCH \u00b6 Every field in a document is indexed and can be queried. # Search for all employees in the megacorp index: GET /megacorp/employee/_search # Search for all employees in the megacorp index # who have \"Smith\" in the last_name field GET /megacorp/employee/_search?q=last_name:Smith # Same query as above, but using the Query DSL GET /megacorp/employee/_search { \"query\": { \"match\": { \"last_name\": \"smith\" } } } # SEARCH QUERY STRING GET /_all/tweet/_search?q=tweet:elasticsearch Don't forget to URL encode special characters e.g. +name:john +tweet:mary GET /_search?q=%2Bname%3Ajohn+%2Btweet%3Amary The + prefix indicates conditions that must be satisfied for our query to match. Similarly a - prefix would indicate conditions that must not match. All conditions without a + or - are optional +name:(mary john) +date:>2014-09-10 +(aggregations geo) # last part searches _all QUERY DSL \u00b6 When used in filtering context, the query is said to be a \"non-scoring\" or \"filtering\" query. That is, the query simply asks the question: \"Does this document match?\". The answer is always a simple, binary yes|no. When used in a querying context, the query becomes a \"scoring\" query. # Find all employees whose `last_name` is Smith # and who are older than 30 GET /megacorp/employee/_search { \"query\" : { \"filtered\" : { \"filter\" : { \"range\" : { \"age\" : { \"gt\" : 30 } } }, \"query\" : { \"match\" : { \"last_name\" : \"smith\" } } } } } MATCH \u00b6 # Find all employees who enjoy \"rock\" or \"climbing\" GET /megacorp/employee/_search { \"query\" : { \"match\" : { \"about\" : \"rock climbing\" } } } The match query should be the standard query that you reach for whenever you want to query for a full-text or exact value in almost any field. If you run a match query against a full-text field, it will analyze the query string by using the correct analyzer for that field before executing the search If you use it on a field containing an exact value, such as a number, a date, a Boolean, or a not_analyzedstring field, then it will search for that exact value MATCH ON MULTIPLE FIELDS \u00b6 { \"multi_match\": { \"query\": \"full text search\", \"fields\": [ \"title\", \"body\" ] }} EXACT SEARCH \u00b6 # Find all employees who enjoy \"rock climbing\" GET /megacorp/employee/_search { \"query\" : { \"match_phrase\" : { \"about\" : \"rock climbing\" } } } # EXACT VALUES The term query is used to search by exact values, be they numbers, dates, Booleans, or not_analyzed exact-value string fields The terms query is the same as the term query, but allows you to specify multiple values to match. If the field contains any of the specified values, the document matches { \"terms\": { \"tag\": [ \"search\", \"full_text\", \"nosql\" ] }} # Compound Queries { \"bool\": { \"must\": { \"match\": { \"tweet\": \"elasticsearch\" }}, \"must_not\": { \"match\": { \"name\": \"mary\" }}, \"should\": { \"match\": { \"tweet\": \"full text\" }}, \"filter\": { \"range\": { \"age\" : { \"gt\" : 30 }} } } } # VALIDATE A QUERY GET /gb/tweet/_validate/query?explain { \"query\": { \"tweet\" : { \"match\" : \"really powerful\" } }} # understand why one particular document matched or, more important, why it didn\u2019t match GET /us/tweet/12/_explain { \"query\" : { \"bool\" : { \"filter\" : { \"term\" : { \"user_id\" : 2 }}, \"must\" : { \"match\" : { \"tweet\" : \"honeymoon\" }} } }} MULTIPLE INDICES OR TYPES \u00b6 # all documents all indices /_search /gb,us/_search Search all types in the gb and us indices /g ,u /_search Search all types in any indices beginning with g or beginning with u /gb/user/_search Search type user in the gb index /gb,us/user,tweet/_search Search types user and tweet in the gb and us indices /_all/user,tweet/_search Search types user and tweet in all indices PAGINATION \u00b6 GET /_search?size=5GET /_search?size=5&from=5 SORTING \u00b6 GET /_search { \"query\" : { \"bool\" : { \"filter\" : { \"term\" : { \"user_id\" : 1 }} } }, \"sort\": { \"date\": { \"order\": \"desc\" }}} For string sorting, use multi-field mapping: \"tweet\": { \"type\": \"string\", \"analyzer\": \"english\", \"fields\": { \"raw\": {\"type\": \"string\", \"index\": \"not_analyzed\" } }} The main tweet field is just the same as before: an analyzed full-text field. The new tweet.raw subfield is not_analyzed. then sort on the new field GET /_search { \"query\": { \"match\": { \"tweet\": \"elasticsearch\" } }, \"sort\": \"tweet.raw\"} HIGHLIGHTS \u00b6 # Find all employees who enjoy \"rock climbing\" - highlights # and highlight the matches GET /megacorp/employee/_search { \"query\" : { \"match_phrase\" : { \"about\" : \"rock climbing\" } }, \"highlight\": { \"fields\" : { \"about\" : {} } } } ANALYSIS \u00b6 An analyzer is really just a wrapper that combines three functions into a single package: * Character filters * Tokenizer * Token filters # See how text is analyzed GET /_analyze { \"analyzer\": \"standard\", \"text\": \"Text to analyze\"} # test analyzer GET /gb/_analyze { \"field\": \"tweet\", \"text\": \"Black-cats\"} MAPPINGS (schemas) \u00b6 Every type has its own mapping, or schema definition. A mapping defines the fields within a type, the datatype for each field, and how the field should be handled by Elasticsearch. A mapping is also used to configure metadata associated with the type. You can control dynamic nature of mappings Mapping (or schema definition) for the tweet type in the gb index GET /gb/_mapping/tweet Elasticsearch supports the following simple field types: * String: string * Whole number: byte, short, integer, long * Floating-point: float, double * Boolean: boolean * Date: date Fields of type string are, by default, considered to contain full text. That is, their value will be passed through an analyzer before being indexed, and a full-text query on the field will pass the query string through an analyzer before searching. The two most important mapping attributes for string fields are index and analyzer. The index attribute controls how the string will be indexed. It can contain one of three values: * analyzed First analyze the string and then index it. In other words, index this field as full text. * not_analyzed Index this field, so it is searchable, but index the value exactly as specified. Do not analyze it. * no Don\u2019t index this field at all. This field will not be searchable. If we want to map the field as an exact value, we need to set it to not_analyzed: { \"tag\": { \"type\": \"string\", \"index\": \"not_analyzed\" } } For analyzed string fields, use the analyzer attribute to specify which analyzer to apply both at search time and at index time. By default, Elasticsearch uses the standard analyzer, but you can change this by specifying one of the built-in analyzers, such as whitespace, simple, or english: { \"tweet\": { \"type\": \"string\", \"analyzer\": \"english\" } } # create a new index, specifying that the tweet field should use the english analyzer PUT /gb { \"mappings\": { \"tweet\" : { \"properties\" : { \"tweet\" : { \"type\" : \"string\", \"analyzer\": \"english\" }, \"date\" : { \"type\" : \"date\" }, \"name\" : { \"type\" : \"string\" }, \"user_id\" : { \"type\" : \"long\" } }}}} null, arrays, objects: see complex core fields Parent Child Relationships \u00b6 DELETE /test_index PUT /test_index { \"mappings\" : { \"parent_type\" : { \"properties\" : { \"num_prop\" : { \"type\" : \"integer\" }, \"str_prop\" : { \"type\" : \"string\" } } }, \"child_type\" : { \"_parent\" : { \"type\" : \"parent_type\" }, \"properties\" : { \"child_num\" : { \"type\" : \"integer\" }, \"child_str\" : { \"type\" : \"string\" } } } } } POST /test_index/_bulk { \"index\" :{ \"_type\" : \"parent_type\" , \"_id\" : 1 }} { \"num_prop\" : 1 , \"str_prop\" : \"hello\" } { \"index\" :{ \"_type\" : \"child_type\" , \"_id\" : 1 , \"_parent\" : 1 }} { \"child_num\" : 11 , \"child_str\" : \"foo\" } { \"index\" :{ \"_type\" : \"child_type\" , \"_id\" : 2 , \"_parent\" : 1 }} { \"child_num\" : 12 , \"child_str\" : \"bar\" } { \"index\" :{ \"_type\" : \"parent_type\" , \"_id\" : 2 }} { \"num_prop\" : 2 , \"str_prop\" : \"goodbye\" } { \"index\" :{ \"_type\" : \"child_type\" , \"_id\" : 3 , \"_parent\" : 2 }} { \"child_num\" : 21 , \"child_str\" : \"baz\" } POST /test_index/child_type/_search POST /test_index/child_type/ 2 ?parent= 1 { \"child_num\" : 13 , \"child_str\" : \"bars\" } POST /test_index/child_type/_search POST /test_index/child_type/ 3 /_update?parent= 2 { \"script\" : \"ctx._source.child_num+=1\" } POST /test_index/child_type/_search POST /test_index/child_type/_search { \"query\" : { \"term\" : { \"child_str\" : { \"value\" : \"foo\" } } } } POST /test_index/parent_type/_search { \"query\" : { \"filtered\" : { \"query\" : { \"match_all\" : {} }, \"filter\" : { \"has_child\" : { \"type\" : \"child_type\" , \"filter\" : { \"term\" : { \"child_str\" : \"foo\" } } } } } } } AGGREGATES \u00b6 Aggregations and searches can span multiple indices # Calculate the most popular interests for all employees GET /megacorp/employee/_search { \"aggs\": { \"all_interests\": { \"terms\": { \"field\": \"interests\" } } } } # Calculate the most popular interests for # employees named \"Smith\" GET /megacorp/employee/_search { \"query\": { \"match\": { \"last_name\": \"smith\" } }, \"aggs\": { \"all_interests\": { \"terms\": { \"field\": \"interests\" } } } } # Calculate the average age of employee per interest - hierarchical aggregates GET /megacorp/employee/_search { \"aggs\" : { \"all_interests\" : { \"terms\" : { \"field\" : \"interests\" }, \"aggs\" : { \"avg_age\" : { \"avg\" : { \"field\" : \"age\" } } } } } } # requires in config/elasticsearch.yml # script.inline: true # script.indexed: true GET /tlo/contacts/_search { \"size\" : 0, \"query\": { \"constant_score\": { \"filter\": { \"terms\": { \"version\": [ \"20160301\", \"20160401\" ] } } } }, \"aggs\": { \"counts\": { \"cardinality\": { \"script\": \"doc['first_name'].value + ' ' + doc['last_name'].value + ' ' + doc['company'].value\", \"missing\": \"N/A\" } } } } INDEX MANAGEMENT \u00b6 By default, indices are assigned five primary shards. The number of primary shards can be set only when an index is created and never changed # Add an index PUT /blogs { \"settings\" : { \"number_of_shards\" : 3, \"number_of_replicas\" : 1 }} PUT /blogs/_settings { \"number_of_replicas\" : 2} ElasticSearch Shards should be 50 GB or less in size. Use aliases to shelter the underlying index (or indices) and allow index swapping CLUSTER MANAGEMENT \u00b6 GET /_cluster/health CONFIGURATION \u00b6 config directory yaml file Sets the JVM heap size to 0.5 memory size. The OS will use it for file system cache Prefer not to allocate 30GB !! --> uncompressed pointers Never let the JVM swap bootstrap.mlockall = true Keep the JVM defaults Do not use G1GC alternative garbage collector cluster.name: <my cluster> All nodes in the cluster must have the same cluster name node.name: <my_node_name> ./bin/elasticsearch --node.name=`hostname` to override the configuration file HTTP port: 9200 and successors Transport : 9300 (internal communications) Discovery \u00b6 AWS plugin available --> also include integration with S3 (snapshot to S3) AWS: multi-AZ is OK but replication across far data centers is not recommended See: resiliency Sites plugins -- kopf / head / paramedic / bigdesk / kibana - contain static web content (JS, HTML....) Install plugins on ALL machines of the cluster To install, ./bin/plugin install marvel-agent ./bin/plugin remove marvel-agent One type per index is recommended, except for parent child / nested indexes. index size optimization: - can disable _source and _all (the index that captures every field - not needed unless the top search bar changes) - by default, Kibana will search _all data types: string, number, bool, datetime, binary, array, object, geo_point, geo_shape, ip, multifield binary should be base64 encoded before storage MAINTENANCE \u00b6 Steps to restore elastic search data: Stop elastic search Extract the zip file (dump file) Start elastic search Reload elastic search The commands to do the above are as below: systemctl stop elasticsearch extract gz file to destination path systemctl start elasticsearch systemctl daemon-reload elasticsearch","title":"ElasticSearch Cheatsheet"},{"location":"Search/ElasticSearch/#cheatsheets","text":"Jolicode","title":"Cheatsheets"},{"location":"Search/ElasticSearch/#development-urls","text":"Kibana (port 5601) Sense ElasticSearch (port 9200)","title":"Development URLs"},{"location":"Search/ElasticSearch/#install","text":"Install curl Install Java Download ElasticSearch Optionally change the cluster.name in the elasticsearch.yml configuration cd elasticsearch-<version> ./bin/elasticsearch -d # or on Windows # bin\\elasticsearch.bat curl 'http://localhost:9200/?pretty' Install Kibana Open config/kibana.yml in an editor Set the elasticsearch.url to point at your Elasticsearch instance Run ./bin/kibana (orbin\\kibana.bat on Windows) Point your browser at http://localhost:5601 Install Sense ./bin/kibana plugin --install elastic/sense On Windows: bin \\k ibana.bat plugin --install elastic/sense Then go to http://localhost:5601/app/sense","title":"INSTALL"},{"location":"Search/ElasticSearch/#curl","text":"curl -X<VERB> '<PROTOCOL>://<HOST>:<PORT>/<PATH>?<QUERY_STRING>' -d '<BODY>' verb is GET, POST, PUT, HEAD, or DELETE","title":"CURL"},{"location":"Search/ElasticSearch/#examples","text":"curl -XGET 'http://localhost:9200/_count?pretty' -d '{ \"query\": { \"match_all\": {} }}' curl -XGET <id>.us-west-2.es.amazonaws.com curl -XGET 'https://<id>.us-west-2.es.amazonaws.com/_count?pretty' -d '{ \"query\": { \"match_all\": {} } }' curl -XPUT https://<id>.us-west-2.es.amazonaws.com/movies/movie/tt0116996 -d '{\"directors\" : [\"Tim Burton\"],\"genres\" : [\"Comedy\",\"Sci-Fi\"], \"plot\": \"The Earth is invaded by Martians with irresistible weapons and a cruel sense of humor.\", \"title\" : \"Mars Attacks!\", \"actors\" :[\"Jack Nicholson\",\"Pierce Brosnan\",\"Sarah Jessica Parker\"], \"year\" : 1996}'","title":"Examples"},{"location":"Search/ElasticSearch/#sense","text":"Sense syntax is similar to curl: Index a document PUT index/type/1 { \"body\": \"here\" } and retrieve it GET index/type/1","title":"Sense"},{"location":"Search/ElasticSearch/#plugins","text":"URL pattern http://yournode:9200/_plugin/<plugin name> On Debian, the script is in: /usr/share/elasticsearch/bin/plugin . Install various plugins ./bin/plugin --install mobz/elasticsearch-head ./bin/plugin --install lmenezes/elasticsearch-kopf/1.2 ./bin/plugin --install elasticsearch/marvel/latest Remove a plugin ./bin/plugin --remove List installed plugins ./bin/plugin --list GET /_nodes?plugin=true Elasticsearch monitoring and management plugins Head Head elasticsearch/bin/plugin -install mobz/elasticsearch-head open http://localhost:9200/_plugin/head elastichq.org BigDesk Live charts and statistics for elasticsearch cluster: BigDesk Kopf Kopf ./bin/plugin --install lmenezes/elasticsearch-kopf/1.2` Marvel ./bin/plugin --install elasticsearch/marvel/latest","title":"PLUGINS"},{"location":"Search/ElasticSearch/#integrations-cms-importexport-hadoop","text":"Integrations Aspire Aspire Aspire is a framework and libraries of extensible components designed to enable creation of solutions to acquire data from one or more content repositories (such as file systems, relational databases, cloud storage, or content management systems), extract metadata and text from the documents, analyze, modify and enhance the content and metadata if needed, and then publish each document, together with its metadata, to a search engine or other target application Docs Integration with Hadoop Integration with Hadoop Bulk loading for elastic search http://infochimps.com Integration with Spring Spring Data WordPress Wordpress","title":"Integrations (CMS, import/export, hadoop...)"},{"location":"Search/ElasticSearch/#tools","text":"BI platforms that can use ES as an analytics engine: Kibana Grafana BIRT Birt Birt Adminer Adminer.org Database management in a single PHP file. Works with MySQL, PostgreSQL, SQLite, MS SQL, Oracle, SimpleDB, Elasticsearch, MongoDB. Needs a webserver + PHP: WAMP Mongolastic A tool that migrates data from MongoDB to Elasticsearch and vice versa Mongolastic Elasticsearch-exporter Elasticsearch-exporter","title":"TOOLS"},{"location":"Search/ElasticSearch/#code-examples-developing-a-web-ui-for-es","text":"Sitepoint CottageLabs scrutmydocs.org qbox.io","title":"Code Examples - developing a Web UI for ES"},{"location":"Search/ElasticSearch/#java-api","text":"Java clients elasticsearch tutorial elasticsearchfr/ IBM dzone","title":"Java API"},{"location":"Search/ElasticSearch/#basics","text":"An Elasticsearch cluster can contain multiple indices, which in turn contain multiple types. These types hold multiple documents, and each document has multiple fields.","title":"BASICS"},{"location":"Search/ElasticSearch/#explore-using-sense","text":"GET _stats/ # List indices GET /_cat/indices/ GET /_cat/indices/my_ind* # Get info about one index GET /twitter GET /my_index_nr_1*/_settings?pretty or ?v GET /twitter/_settings,_mappings The available features are _settings, _mappings, _warmers and _aliases # cluster GET /_nodes # insert data PUT my_index/user/1 { \"first_name\": \"John\", \"last_name\": \"Smith\", \"date_of_birth\": \"1970-10-24\" } #search GET my_index/_search GET _count?pretty # Data schema GET my_index/_mapping","title":"Explore (using Sense)"},{"location":"Search/ElasticSearch/#insert-documents","text":"PUT /index/type/ID PUT /megacorp/employee/1 { \"first_name\" : \"John\", \"last_name\" : \"Smith\", \"age\" : 25, \"about\" : \"I love to go rock climbing\", \"interests\": [ \"sports\", \"music\" ]} PUT /megacorp/employee/2 { \"first_name\" : \"Jane\", \"last_name\" : \"Smith\", \"age\" : 32, \"about\" : \"I like to collect rock albums\", \"interests\": [ \"music\" ]} GET /megacorp/employee/1 Field names can be any valid string, but may not include periods. Every document in Elasticsearch has a version number. Every time a change is made to a document (including deleting it), the _version number is incremented. Optimistic concurrency control PUT /website/blog/1?version=1 { \"title\": \"My first blog entry\", \"text\": \"Starting to get the hang of this...\"} We want this update to succeed only if the current _version of this document in our index is version 1 External version: PUT /website/blog/2?version=5&version_type=external { \"title\": \"My first external blog entry\", \"text\": \"Starting to get the hang of this...\"}","title":"INSERT DOCUMENTS"},{"location":"Search/ElasticSearch/#insert-documents-autogenerated-ids","text":"POST /website/blog/ { \"title\": \"My second blog entry\", \"text\": \"Still trying this out...\", \"date\": \"2014/01/01\" } Response: { \"_index\": \"website\", \"_type\": \"blog\", \"_id\": \"AVFgSgVHUP18jI2wRx0w\", \"_version\": 1, \"created\": true } # creating an entirely new document and not overwriting an existing one PUT /website/blog/123?op_type=create { ... } PUT /website/blog/123/_create { ... }","title":"INSERT DOCUMENTS - AUTOGENERATED IDS"},{"location":"Search/ElasticSearch/#retrieve-documents","text":"GET /website/blog/123 # optional ?pretty { \"_index\" : \"website\", \"_type\" : \"blog\", \"_id\" : \"123\", \"_version\" : 1, \"found\" : true, \"_source\" : { \"title\": \"My first blog entry\", \"text\": \"Just trying this out...\", \"date\": \"2014/01/01\" }} # Contains just the fields that we requested GET /website/blog/123?_source=title,text # Just get the original doc GET /website/blog/123/_source # check if doc exists -- HTTP 200 or 404 curl -i -XHEAD http://localhost:9200/website/blog/123 # Note: HEAD/exists requests do not work in Sense # because they only return HTTP headers, not # a JSON body # multiple docs at once GET /website/blog/_mget { \"ids\" : [ \"2\", \"1\" ]}","title":"RETRIEVE DOCUMENTS"},{"location":"Search/ElasticSearch/#update","text":"Documents in Elasticsearch are immutable; we cannot change them. Instead, if we need to update an existing document, we reindex or replace it # Accepts a partial document as the doc parameter, which just gets merged with the existing document. POST /website/blog/1/_update { \"doc\" : { \"tags\" : [ \"testing\" ], \"views\": 0 }} # Script POST /website/blog/1/_update { \"script\" : \"ctx._source.views+=1\"} # script with parameters POST /website/blog/1/_update { \"script\" : \"ctx._source.tags+=new_tag\", \"params\" : { \"new_tag\" : \"search\" }} # upsert POST/website/pageviews/1/_update {\"script\":\"ctx._source.views+=1\",\"upsert\":{\"views\":1}}","title":"UPDATE"},{"location":"Search/ElasticSearch/#delete","text":"DELETE /website/blog/123 # delete doc based on its contents POST /website/blog/1/_update { \"script\" : \"ctx.op = ctx._source.views == count ? 'delete' : 'none'\", \"params\" : { \"count\": 1 }}","title":"DELETE"},{"location":"Search/ElasticSearch/#bulk","text":"POST /_bulk {\"delete\":{\"_index\":\"website\",\"_type\":\"blog\",\"_id\":\"123\"}} {\"create\":{\"_index\":\"website\",\"_type\":\"blog\",\"_id\":\"123\"}} # Create a document only if the document does not already exist {\"title\":\"My first blog post\"} {\"index\":{\"_index\":\"website\",\"_type\":\"blog\"}} {\"title\":\"My second blog post\"} {\"update\":{\"_index\":\"website\",\"_type\":\"blog\",\"_id\":\"123\",\"_retry_on_conflict\":3}} {\"doc\":{\"title\":\"My updated blog post\"}} Bulk in the same index or index/type POST /website/_bulk {\"index\":{\"_type\":\"log\"}} {\"event\":\"User logged in\"} {\"index\":{\"_type\":\"blog\"}} {\"title\":\"My second blog post\"} Try around 5-15MB in size.","title":"BULK"},{"location":"Search/ElasticSearch/#search","text":"Every field in a document is indexed and can be queried. # Search for all employees in the megacorp index: GET /megacorp/employee/_search # Search for all employees in the megacorp index # who have \"Smith\" in the last_name field GET /megacorp/employee/_search?q=last_name:Smith # Same query as above, but using the Query DSL GET /megacorp/employee/_search { \"query\": { \"match\": { \"last_name\": \"smith\" } } } # SEARCH QUERY STRING GET /_all/tweet/_search?q=tweet:elasticsearch Don't forget to URL encode special characters e.g. +name:john +tweet:mary GET /_search?q=%2Bname%3Ajohn+%2Btweet%3Amary The + prefix indicates conditions that must be satisfied for our query to match. Similarly a - prefix would indicate conditions that must not match. All conditions without a + or - are optional +name:(mary john) +date:>2014-09-10 +(aggregations geo) # last part searches _all","title":"SEARCH"},{"location":"Search/ElasticSearch/#query-dsl","text":"When used in filtering context, the query is said to be a \"non-scoring\" or \"filtering\" query. That is, the query simply asks the question: \"Does this document match?\". The answer is always a simple, binary yes|no. When used in a querying context, the query becomes a \"scoring\" query. # Find all employees whose `last_name` is Smith # and who are older than 30 GET /megacorp/employee/_search { \"query\" : { \"filtered\" : { \"filter\" : { \"range\" : { \"age\" : { \"gt\" : 30 } } }, \"query\" : { \"match\" : { \"last_name\" : \"smith\" } } } } }","title":"QUERY DSL"},{"location":"Search/ElasticSearch/#match","text":"# Find all employees who enjoy \"rock\" or \"climbing\" GET /megacorp/employee/_search { \"query\" : { \"match\" : { \"about\" : \"rock climbing\" } } } The match query should be the standard query that you reach for whenever you want to query for a full-text or exact value in almost any field. If you run a match query against a full-text field, it will analyze the query string by using the correct analyzer for that field before executing the search If you use it on a field containing an exact value, such as a number, a date, a Boolean, or a not_analyzedstring field, then it will search for that exact value","title":"MATCH"},{"location":"Search/ElasticSearch/#match-on-multiple-fields","text":"{ \"multi_match\": { \"query\": \"full text search\", \"fields\": [ \"title\", \"body\" ] }}","title":"MATCH ON MULTIPLE FIELDS"},{"location":"Search/ElasticSearch/#exact-search","text":"# Find all employees who enjoy \"rock climbing\" GET /megacorp/employee/_search { \"query\" : { \"match_phrase\" : { \"about\" : \"rock climbing\" } } } # EXACT VALUES The term query is used to search by exact values, be they numbers, dates, Booleans, or not_analyzed exact-value string fields The terms query is the same as the term query, but allows you to specify multiple values to match. If the field contains any of the specified values, the document matches { \"terms\": { \"tag\": [ \"search\", \"full_text\", \"nosql\" ] }} # Compound Queries { \"bool\": { \"must\": { \"match\": { \"tweet\": \"elasticsearch\" }}, \"must_not\": { \"match\": { \"name\": \"mary\" }}, \"should\": { \"match\": { \"tweet\": \"full text\" }}, \"filter\": { \"range\": { \"age\" : { \"gt\" : 30 }} } } } # VALIDATE A QUERY GET /gb/tweet/_validate/query?explain { \"query\": { \"tweet\" : { \"match\" : \"really powerful\" } }} # understand why one particular document matched or, more important, why it didn\u2019t match GET /us/tweet/12/_explain { \"query\" : { \"bool\" : { \"filter\" : { \"term\" : { \"user_id\" : 2 }}, \"must\" : { \"match\" : { \"tweet\" : \"honeymoon\" }} } }}","title":"EXACT SEARCH"},{"location":"Search/ElasticSearch/#multiple-indices-or-types","text":"# all documents all indices /_search /gb,us/_search Search all types in the gb and us indices /g ,u /_search Search all types in any indices beginning with g or beginning with u /gb/user/_search Search type user in the gb index /gb,us/user,tweet/_search Search types user and tweet in the gb and us indices /_all/user,tweet/_search Search types user and tweet in all indices","title":"MULTIPLE INDICES OR TYPES"},{"location":"Search/ElasticSearch/#pagination","text":"GET /_search?size=5GET /_search?size=5&from=5","title":"PAGINATION"},{"location":"Search/ElasticSearch/#sorting","text":"GET /_search { \"query\" : { \"bool\" : { \"filter\" : { \"term\" : { \"user_id\" : 1 }} } }, \"sort\": { \"date\": { \"order\": \"desc\" }}} For string sorting, use multi-field mapping: \"tweet\": { \"type\": \"string\", \"analyzer\": \"english\", \"fields\": { \"raw\": {\"type\": \"string\", \"index\": \"not_analyzed\" } }} The main tweet field is just the same as before: an analyzed full-text field. The new tweet.raw subfield is not_analyzed. then sort on the new field GET /_search { \"query\": { \"match\": { \"tweet\": \"elasticsearch\" } }, \"sort\": \"tweet.raw\"}","title":"SORTING"},{"location":"Search/ElasticSearch/#highlights","text":"# Find all employees who enjoy \"rock climbing\" - highlights # and highlight the matches GET /megacorp/employee/_search { \"query\" : { \"match_phrase\" : { \"about\" : \"rock climbing\" } }, \"highlight\": { \"fields\" : { \"about\" : {} } } }","title":"HIGHLIGHTS"},{"location":"Search/ElasticSearch/#analysis","text":"An analyzer is really just a wrapper that combines three functions into a single package: * Character filters * Tokenizer * Token filters # See how text is analyzed GET /_analyze { \"analyzer\": \"standard\", \"text\": \"Text to analyze\"} # test analyzer GET /gb/_analyze { \"field\": \"tweet\", \"text\": \"Black-cats\"}","title":"ANALYSIS"},{"location":"Search/ElasticSearch/#mappings-schemas","text":"Every type has its own mapping, or schema definition. A mapping defines the fields within a type, the datatype for each field, and how the field should be handled by Elasticsearch. A mapping is also used to configure metadata associated with the type. You can control dynamic nature of mappings Mapping (or schema definition) for the tweet type in the gb index GET /gb/_mapping/tweet Elasticsearch supports the following simple field types: * String: string * Whole number: byte, short, integer, long * Floating-point: float, double * Boolean: boolean * Date: date Fields of type string are, by default, considered to contain full text. That is, their value will be passed through an analyzer before being indexed, and a full-text query on the field will pass the query string through an analyzer before searching. The two most important mapping attributes for string fields are index and analyzer. The index attribute controls how the string will be indexed. It can contain one of three values: * analyzed First analyze the string and then index it. In other words, index this field as full text. * not_analyzed Index this field, so it is searchable, but index the value exactly as specified. Do not analyze it. * no Don\u2019t index this field at all. This field will not be searchable. If we want to map the field as an exact value, we need to set it to not_analyzed: { \"tag\": { \"type\": \"string\", \"index\": \"not_analyzed\" } } For analyzed string fields, use the analyzer attribute to specify which analyzer to apply both at search time and at index time. By default, Elasticsearch uses the standard analyzer, but you can change this by specifying one of the built-in analyzers, such as whitespace, simple, or english: { \"tweet\": { \"type\": \"string\", \"analyzer\": \"english\" } } # create a new index, specifying that the tweet field should use the english analyzer PUT /gb { \"mappings\": { \"tweet\" : { \"properties\" : { \"tweet\" : { \"type\" : \"string\", \"analyzer\": \"english\" }, \"date\" : { \"type\" : \"date\" }, \"name\" : { \"type\" : \"string\" }, \"user_id\" : { \"type\" : \"long\" } }}}} null, arrays, objects: see complex core fields","title":"MAPPINGS (schemas)"},{"location":"Search/ElasticSearch/#parent-child-relationships","text":"DELETE /test_index PUT /test_index { \"mappings\" : { \"parent_type\" : { \"properties\" : { \"num_prop\" : { \"type\" : \"integer\" }, \"str_prop\" : { \"type\" : \"string\" } } }, \"child_type\" : { \"_parent\" : { \"type\" : \"parent_type\" }, \"properties\" : { \"child_num\" : { \"type\" : \"integer\" }, \"child_str\" : { \"type\" : \"string\" } } } } } POST /test_index/_bulk { \"index\" :{ \"_type\" : \"parent_type\" , \"_id\" : 1 }} { \"num_prop\" : 1 , \"str_prop\" : \"hello\" } { \"index\" :{ \"_type\" : \"child_type\" , \"_id\" : 1 , \"_parent\" : 1 }} { \"child_num\" : 11 , \"child_str\" : \"foo\" } { \"index\" :{ \"_type\" : \"child_type\" , \"_id\" : 2 , \"_parent\" : 1 }} { \"child_num\" : 12 , \"child_str\" : \"bar\" } { \"index\" :{ \"_type\" : \"parent_type\" , \"_id\" : 2 }} { \"num_prop\" : 2 , \"str_prop\" : \"goodbye\" } { \"index\" :{ \"_type\" : \"child_type\" , \"_id\" : 3 , \"_parent\" : 2 }} { \"child_num\" : 21 , \"child_str\" : \"baz\" } POST /test_index/child_type/_search POST /test_index/child_type/ 2 ?parent= 1 { \"child_num\" : 13 , \"child_str\" : \"bars\" } POST /test_index/child_type/_search POST /test_index/child_type/ 3 /_update?parent= 2 { \"script\" : \"ctx._source.child_num+=1\" } POST /test_index/child_type/_search POST /test_index/child_type/_search { \"query\" : { \"term\" : { \"child_str\" : { \"value\" : \"foo\" } } } } POST /test_index/parent_type/_search { \"query\" : { \"filtered\" : { \"query\" : { \"match_all\" : {} }, \"filter\" : { \"has_child\" : { \"type\" : \"child_type\" , \"filter\" : { \"term\" : { \"child_str\" : \"foo\" } } } } } } }","title":"Parent Child Relationships"},{"location":"Search/ElasticSearch/#aggregates","text":"Aggregations and searches can span multiple indices # Calculate the most popular interests for all employees GET /megacorp/employee/_search { \"aggs\": { \"all_interests\": { \"terms\": { \"field\": \"interests\" } } } } # Calculate the most popular interests for # employees named \"Smith\" GET /megacorp/employee/_search { \"query\": { \"match\": { \"last_name\": \"smith\" } }, \"aggs\": { \"all_interests\": { \"terms\": { \"field\": \"interests\" } } } } # Calculate the average age of employee per interest - hierarchical aggregates GET /megacorp/employee/_search { \"aggs\" : { \"all_interests\" : { \"terms\" : { \"field\" : \"interests\" }, \"aggs\" : { \"avg_age\" : { \"avg\" : { \"field\" : \"age\" } } } } } } # requires in config/elasticsearch.yml # script.inline: true # script.indexed: true GET /tlo/contacts/_search { \"size\" : 0, \"query\": { \"constant_score\": { \"filter\": { \"terms\": { \"version\": [ \"20160301\", \"20160401\" ] } } } }, \"aggs\": { \"counts\": { \"cardinality\": { \"script\": \"doc['first_name'].value + ' ' + doc['last_name'].value + ' ' + doc['company'].value\", \"missing\": \"N/A\" } } } }","title":"AGGREGATES"},{"location":"Search/ElasticSearch/#index-management","text":"By default, indices are assigned five primary shards. The number of primary shards can be set only when an index is created and never changed # Add an index PUT /blogs { \"settings\" : { \"number_of_shards\" : 3, \"number_of_replicas\" : 1 }} PUT /blogs/_settings { \"number_of_replicas\" : 2} ElasticSearch Shards should be 50 GB or less in size. Use aliases to shelter the underlying index (or indices) and allow index swapping","title":"INDEX MANAGEMENT"},{"location":"Search/ElasticSearch/#cluster-management","text":"GET /_cluster/health","title":"CLUSTER MANAGEMENT"},{"location":"Search/ElasticSearch/#configuration","text":"config directory yaml file Sets the JVM heap size to 0.5 memory size. The OS will use it for file system cache Prefer not to allocate 30GB !! --> uncompressed pointers Never let the JVM swap bootstrap.mlockall = true Keep the JVM defaults Do not use G1GC alternative garbage collector cluster.name: <my cluster> All nodes in the cluster must have the same cluster name node.name: <my_node_name> ./bin/elasticsearch --node.name=`hostname` to override the configuration file HTTP port: 9200 and successors Transport : 9300 (internal communications)","title":"CONFIGURATION"},{"location":"Search/ElasticSearch/#discovery","text":"AWS plugin available --> also include integration with S3 (snapshot to S3) AWS: multi-AZ is OK but replication across far data centers is not recommended See: resiliency Sites plugins -- kopf / head / paramedic / bigdesk / kibana - contain static web content (JS, HTML....) Install plugins on ALL machines of the cluster To install, ./bin/plugin install marvel-agent ./bin/plugin remove marvel-agent One type per index is recommended, except for parent child / nested indexes. index size optimization: - can disable _source and _all (the index that captures every field - not needed unless the top search bar changes) - by default, Kibana will search _all data types: string, number, bool, datetime, binary, array, object, geo_point, geo_shape, ip, multifield binary should be base64 encoded before storage","title":"Discovery"},{"location":"Search/ElasticSearch/#maintenance","text":"Steps to restore elastic search data: Stop elastic search Extract the zip file (dump file) Start elastic search Reload elastic search The commands to do the above are as below: systemctl stop elasticsearch extract gz file to destination path systemctl start elasticsearch systemctl daemon-reload elasticsearch","title":"MAINTENANCE"},{"location":"Search/Logstash/","text":"LogStash \u00b6 Operations \u00b6 logstash -w 4 to set the number of worker threads Use path.data to distribute the data on multiple (EBS) disks Outputs \u00b6 MongoDB PagerDuty Nagios Graphite Ganglia StatsD Redis RabbitMQ output { elasticsearch { } # http://localhost:9200 } output { redis { host => \"redis.example.com\" data_type =>: \"list\" } } Output to file \u00b6 output { file { } } Filtering \u00b6 Use \"date\" for normalizing dates: filter { date{ timezone => \"America/Los_Angeles\" locale => \"en\" # English } geoip { source => \"clientip\" # will read from clientip field database => ... # use MaxMind's GeoLiteCity by default } useragent { } } Mutate a field \u00b6 filter { if [action] == \"login { mutate { remove_field => \"secret\" } } } Conditionals both in filter and outputs \u00b6 regexp =~ !~ output { if [loglevel] == \"ERROR\" } Interesting Plugins \u00b6 Stanford NLP library logstash plugin","title":"LogStash"},{"location":"Search/Logstash/#logstash","text":"","title":"LogStash"},{"location":"Search/Logstash/#operations","text":"logstash -w 4 to set the number of worker threads Use path.data to distribute the data on multiple (EBS) disks","title":"Operations"},{"location":"Search/Logstash/#outputs","text":"MongoDB PagerDuty Nagios Graphite Ganglia StatsD Redis RabbitMQ output { elasticsearch { } # http://localhost:9200 } output { redis { host => \"redis.example.com\" data_type =>: \"list\" } }","title":"Outputs"},{"location":"Search/Logstash/#output-to-file","text":"output { file { } }","title":"Output to file"},{"location":"Search/Logstash/#filtering","text":"Use \"date\" for normalizing dates: filter { date{ timezone => \"America/Los_Angeles\" locale => \"en\" # English } geoip { source => \"clientip\" # will read from clientip field database => ... # use MaxMind's GeoLiteCity by default } useragent { } }","title":"Filtering"},{"location":"Search/Logstash/#mutate-a-field","text":"filter { if [action] == \"login { mutate { remove_field => \"secret\" } } }","title":"Mutate a field"},{"location":"Search/Logstash/#conditionals-both-in-filter-and-outputs","text":"regexp =~ !~ output { if [loglevel] == \"ERROR\" }","title":"Conditionals both in filter and outputs"},{"location":"Search/Logstash/#interesting-plugins","text":"Stanford NLP library logstash plugin","title":"Interesting Plugins"},{"location":"Software_Development/Development_Tools/","text":"Communication / IM \u00b6 Slack Trillian / Pandion Skype, WeChat, Viber, Hangouts Wiki / Knowledge Base \u00b6 Confluence Evernote Project / Bug Tracking \u00b6 JIRA Bugzilla Mantis RedMine TFS Enterprise Architecture / UML \u00b6 Violet UML Editor Visio Rational Rose Terminals / SSH \u00b6 Putty MobaXterm mRemoteNG Remote Desktop Connection Manager Editors/ IDEs \u00b6 Comparison of integrated development environments Notepad++ Plugins Gedit Sublime Text Eclipse List of Eclipse-based software Visual Studio Visual Studio Code for Python Python Tools for Visual Studio PyDev PyCharm Anaconda Source Control \u00b6 SourceTree (Atlassian) GitHub TortoiseGit Code Quality \u00b6 SonarQube Phabricator Code Coverage Virtual Machines and Containers \u00b6 Oracle Virtualbox Docker Kubernetes SQL Tools \u00b6 MySQL Workbench HeidiSQL Aginity Workbench for Redshift (AWS) MongoDB tools \u00b6 RoboMongo MongoChef MongoDB Compass Data Quality \u00b6 DQ Analyzer (Attacama) Data Science \u00b6 Jupyter / IPython Rodeo Gephi AWS \u00b6 S3 Browser FastGlacier File Handling \u00b6 7zip FileZilla FolderSize Other \u00b6 Cygwin Log Tail Diff - WinMerge","title":"Development Tools"},{"location":"Software_Development/Development_Tools/#communication-im","text":"Slack Trillian / Pandion Skype, WeChat, Viber, Hangouts","title":"Communication / IM"},{"location":"Software_Development/Development_Tools/#wiki-knowledge-base","text":"Confluence Evernote","title":"Wiki / Knowledge Base"},{"location":"Software_Development/Development_Tools/#project-bug-tracking","text":"JIRA Bugzilla Mantis RedMine TFS","title":"Project / Bug Tracking"},{"location":"Software_Development/Development_Tools/#enterprise-architecture-uml","text":"Violet UML Editor Visio Rational Rose","title":"Enterprise Architecture / UML"},{"location":"Software_Development/Development_Tools/#terminals-ssh","text":"Putty MobaXterm mRemoteNG Remote Desktop Connection Manager","title":"Terminals / SSH"},{"location":"Software_Development/Development_Tools/#editors-ides","text":"Comparison of integrated development environments Notepad++ Plugins Gedit Sublime Text Eclipse List of Eclipse-based software Visual Studio Visual Studio Code for Python Python Tools for Visual Studio PyDev PyCharm Anaconda","title":"Editors/ IDEs"},{"location":"Software_Development/Development_Tools/#source-control","text":"SourceTree (Atlassian) GitHub TortoiseGit","title":"Source Control"},{"location":"Software_Development/Development_Tools/#code-quality","text":"SonarQube Phabricator Code Coverage","title":"Code Quality"},{"location":"Software_Development/Development_Tools/#virtual-machines-and-containers","text":"Oracle Virtualbox Docker Kubernetes","title":"Virtual Machines and Containers"},{"location":"Software_Development/Development_Tools/#sql-tools","text":"MySQL Workbench HeidiSQL Aginity Workbench for Redshift (AWS)","title":"SQL Tools"},{"location":"Software_Development/Development_Tools/#mongodb-tools","text":"RoboMongo MongoChef MongoDB Compass","title":"MongoDB tools"},{"location":"Software_Development/Development_Tools/#data-quality","text":"DQ Analyzer (Attacama)","title":"Data Quality"},{"location":"Software_Development/Development_Tools/#data-science","text":"Jupyter / IPython Rodeo Gephi","title":"Data Science"},{"location":"Software_Development/Development_Tools/#aws","text":"S3 Browser FastGlacier","title":"AWS"},{"location":"Software_Development/Development_Tools/#file-handling","text":"7zip FileZilla FolderSize","title":"File Handling"},{"location":"Software_Development/Development_Tools/#other","text":"Cygwin Log Tail Diff - WinMerge","title":"Other"},{"location":"Software_Development/Eclipse/","text":"Useful Links \u00b6 Eclipse AWS Toolkit for Eclipse Spring Tool Suite To install the AWS Toolkit \u00b6 Open Help > Install New Software . Enter https://aws.amazon.com/eclipse in the text box labeled \u201cWork with\u201d at the top of the dialog. Select \u201cAWS Toolkit for Eclipse\u201d from the list below.","title":"Eclipse"},{"location":"Software_Development/Eclipse/#useful-links","text":"Eclipse AWS Toolkit for Eclipse Spring Tool Suite","title":"Useful Links"},{"location":"Software_Development/Eclipse/#to-install-the-aws-toolkit","text":"Open Help > Install New Software . Enter https://aws.amazon.com/eclipse in the text box labeled \u201cWork with\u201d at the top of the dialog. Select \u201cAWS Toolkit for Eclipse\u201d from the list below.","title":"To install the AWS Toolkit"},{"location":"Software_Development/IntelliJ/","text":"IntelliJ \u00b6 Shortcuts \u00b6 Search Anywhere Double Shift Got to file Ctrl + Shift + N Recent files Ctrl + E Code Completion Ctrl + Space Parameters Ctrl + P Highlight usages in file Ctrl + Shift + F7 Declaration of the current method Alt + Q Code Templates Ctrl + J","title":"IntelliJ Cheatsheet"},{"location":"Software_Development/IntelliJ/#intellij","text":"","title":"IntelliJ"},{"location":"Software_Development/IntelliJ/#shortcuts","text":"Search Anywhere Double Shift Got to file Ctrl + Shift + N Recent files Ctrl + E Code Completion Ctrl + Space Parameters Ctrl + P Highlight usages in file Ctrl + Shift + F7 Declaration of the current method Alt + Q Code Templates Ctrl + J","title":"Shortcuts"},{"location":"Web/Bootstrap/","text":"Bootstrap \u00b6 Useful Links \u00b6 Bootstrap Install \u00b6 $ npm install bootstrap CDN \u00b6 <!-- Latest compiled and minified CSS --> < link rel = \"stylesheet\" href = \"https://maxcdn.bootstrapcdn.com/bootstrap/3.3.6/css/bootstrap.min.css\" integrity = \"sha384-1q8mTJOASx8j1Au+a5WDVnPi2lkFfwwEAa8hDDdjZlpLegxhjVME1fgjWPGmkzs7\" crossorigin = \"anonymous\" > <!-- Optional theme --> < link rel = \"stylesheet\" href = \"https://maxcdn.bootstrapcdn.com/bootstrap/3.3.6/css/bootstrap-theme.min.css\" integrity = \"sha384-fLW2N01lMqjakBkx3l/M9EahuwpSfeNvV63J5ezn3uZzapT0u7EYsXMjQV+0En5r\" crossorigin = \"anonymous\" > <!-- Latest compiled and minified JavaScript --> < script src = \"https://maxcdn.bootstrapcdn.com/bootstrap/3.3.6/js/bootstrap.min.js\" integrity = \"sha384-0mSbJDEHialfmuBBQP6A4Qrprq5OVfW37PRR3j5ELqxss1yVqOtnepnHVP9aJ7xS\" crossorigin = \"anonymous\" ></ script >","title":"Bootstrap"},{"location":"Web/Bootstrap/#bootstrap","text":"","title":"Bootstrap"},{"location":"Web/Bootstrap/#useful-links","text":"Bootstrap","title":"Useful Links"},{"location":"Web/Bootstrap/#install","text":"$ npm install bootstrap","title":"Install"},{"location":"Web/Bootstrap/#cdn","text":"<!-- Latest compiled and minified CSS --> < link rel = \"stylesheet\" href = \"https://maxcdn.bootstrapcdn.com/bootstrap/3.3.6/css/bootstrap.min.css\" integrity = \"sha384-1q8mTJOASx8j1Au+a5WDVnPi2lkFfwwEAa8hDDdjZlpLegxhjVME1fgjWPGmkzs7\" crossorigin = \"anonymous\" > <!-- Optional theme --> < link rel = \"stylesheet\" href = \"https://maxcdn.bootstrapcdn.com/bootstrap/3.3.6/css/bootstrap-theme.min.css\" integrity = \"sha384-fLW2N01lMqjakBkx3l/M9EahuwpSfeNvV63J5ezn3uZzapT0u7EYsXMjQV+0En5r\" crossorigin = \"anonymous\" > <!-- Latest compiled and minified JavaScript --> < script src = \"https://maxcdn.bootstrapcdn.com/bootstrap/3.3.6/js/bootstrap.min.js\" integrity = \"sha384-0mSbJDEHialfmuBBQP6A4Qrprq5OVfW37PRR3j5ELqxss1yVqOtnepnHVP9aJ7xS\" crossorigin = \"anonymous\" ></ script >","title":"CDN"},{"location":"Web/CORS/","text":"CORS \u00b6 CORS on Wikipedia Same-origin_policy CORS Certain \"cross-domain\" requests, notably AJAX requests, are forbidden by default by the same-origin security policy of web browsers. The same-origin policy is an important security concept implemented by web browsers to prevent Javascript code from making requests against a different origin (e.g., different domain, more precisely combination of URI scheme, hostname, and port number ) than the one from which it was served. Although the same-origin policy is effective in preventing resources from different origins, it also prevents legitimate interactions between a server and clients of a known and trusted origin. Cross-Origin Resource Sharing (CORS) is a technique for relaxing the same-origin policy, allowing Javascript on a web page to consume a REST API served from a different origin. Cross-origin requests come in two flavors: 1. simple requests 2. \"not-so-simple requests\" (a term just made up) Simple Requests \u00b6 Simple requests are requests that meet the following criteria: HTTP Method matches (case-sensitive) one of: - HEAD - GET - POST HTTP Headers matches (case-insensitive): - Accept - Accept-Language - Content-Language - Last-Event-ID - Content-Type, but only if the value is one of application/x-www-form-urlencoded , multipart/form-data , text/plain Handling a not-so-simple request \u00b6 A not-so-simple request looks like a single request to the client, but it actually consists of two requests under the hood. The browser first issues a preflight request, which is like asking the server for permission to make the actual request. Once permissions have been granted, the browser makes the actual request. The browser handles the details of these two requests transparently. The preflight response can also be cached so that it is not issued on every request. Some Javascript libraries, such as AngularJS and Sencha Touch, send preflight requests for any kind of request. This approach is arguably safer, because it doesn't assume that a service adheres to HTTP method semantics (i.e., a GET endpoint could have been written to have side effects.) API Gateway and CORS \u00b6 How to CORS (AWS)","title":"CORS"},{"location":"Web/CORS/#cors","text":"CORS on Wikipedia Same-origin_policy CORS Certain \"cross-domain\" requests, notably AJAX requests, are forbidden by default by the same-origin security policy of web browsers. The same-origin policy is an important security concept implemented by web browsers to prevent Javascript code from making requests against a different origin (e.g., different domain, more precisely combination of URI scheme, hostname, and port number ) than the one from which it was served. Although the same-origin policy is effective in preventing resources from different origins, it also prevents legitimate interactions between a server and clients of a known and trusted origin. Cross-Origin Resource Sharing (CORS) is a technique for relaxing the same-origin policy, allowing Javascript on a web page to consume a REST API served from a different origin. Cross-origin requests come in two flavors: 1. simple requests 2. \"not-so-simple requests\" (a term just made up)","title":"CORS"},{"location":"Web/CORS/#simple-requests","text":"Simple requests are requests that meet the following criteria: HTTP Method matches (case-sensitive) one of: - HEAD - GET - POST HTTP Headers matches (case-insensitive): - Accept - Accept-Language - Content-Language - Last-Event-ID - Content-Type, but only if the value is one of application/x-www-form-urlencoded , multipart/form-data , text/plain","title":"Simple Requests"},{"location":"Web/CORS/#handling-a-not-so-simple-request","text":"A not-so-simple request looks like a single request to the client, but it actually consists of two requests under the hood. The browser first issues a preflight request, which is like asking the server for permission to make the actual request. Once permissions have been granted, the browser makes the actual request. The browser handles the details of these two requests transparently. The preflight response can also be cached so that it is not issued on every request. Some Javascript libraries, such as AngularJS and Sencha Touch, send preflight requests for any kind of request. This approach is arguably safer, because it doesn't assume that a service adheres to HTTP method semantics (i.e., a GET endpoint could have been written to have side effects.)","title":"Handling a not-so-simple request"},{"location":"Web/CORS/#api-gateway-and-cors","text":"How to CORS (AWS)","title":"API Gateway and CORS"},{"location":"Web/jQuery/","text":"jQuery \u00b6 Links \u00b6 jQuery API documentation Learn jQuery The jQuery library exposes its methods and properties via two properties of the window object called jQuery and $. $ is simply an alias for jQuery and it's often employed because it's shorter and faster to write. Embed JQuery \u00b6 Either directly <!doctype html> < html > < head > < meta charset = \"utf-8\" > < title > Demo </ title > </ head > < body > < a href = \"http://jquery.com/\" > jQuery </ a > < script src = \"jquery.js\" ></ script > < script > // Your code goes here. </ script > </ body > or via a CDN < head > < script src = \"https://ajax.googleapis.com/ajax/libs/jquery/1.12.4/jquery.min.js\" ></ script > </ head > Launching Code on Document Ready \u00b6 $ ( document ). ready ( function () { // Your code here. }); // Shorthand for $( document ).ready() $ ( function () { console . log ( \"ready!\" ); }); Selecting elements \u00b6 $ ( \"#myId\" ); // Note IDs must be unique per page. $ ( \".myClass\" ); $ ( \"input[name='first_name']\" ); $ ( \"#contents ul.people li\" ); $ ( \"div.myClass, ul.people\" ); If you have a variable containing a DOM element, and want to select elements related to that DOM element, simply wrap it in a jQuery object. var myDomElement = document . getElementById ( \"foo\" ); // A plain DOM element. $ ( myDomElement ). find ( \"a\" ); // Finds all anchors inside the DOM element. Pull a native DOM element from a jQuery object \u00b6 A jQuery object is an array-like wrapper around one or more DOM elements. $ ( \"#foo\" )[ 0 ]; // Equivalent to document.getElementById( \"foo\" ) $ ( \"#foo\" ). get ( 0 ); // Testing whether a selection contains elements. if ( $ ( \"div.foo\" ). length ) { ... } Refining selections. \u00b6 $ ( \"div.foo\" ). has ( \"p\" ); // div.foo elements that contain <p> tags $ ( \"h1\" ). not ( \".bar\" ); // h1 elements that don't have a class of bar $ ( \"ul li\" ). filter ( \".current\" ); // unordered list items with class of current $ ( \"ul li\" ). first (); // just the first unordered list item $ ( \"ul li\" ). eq ( 5 ); // the sixth $ ( \"form :checked\" ); // :checked targets checked checkboxes DOM Traversal and Manipulation \u00b6 Get the <button> element with the class 'continue' and change its HTML to 'Next Step...' $ ( \"button.continue\" ). html ( \"Next Step...\" ) $ ( \"h1\" ). html (); // returns the html Chaining \u00b6 $ ( \"#content\" ). find ( \"h3\" ). eq ( 2 ). html ( \"new text for the third h3!\" ); $ ( \"#content\" ) . find ( \"h3\" ) . eq ( 2 ) . html ( \"new text for the third h3!\" ) . end () // Restores the selection to all h3s in #content . eq ( 0 ) . html ( \"new text for the first h3!\" ); Manipulation \u00b6 .html() \u2013 Get or set the HTML contents. .text() \u2013 Get or set the text contents; HTML will be stripped. .attr() \u2013 Get or set the value of the provided attribute. .width() \u2013 Get or set the width in pixels of the first element in the selection as an integer. .height() \u2013 Get or set the height in pixels of the first element in the selection as an integer. .position() \u2013 Get an object with position information for the first element in the selection, relative to its first positioned ancestor. This is a getter only. .val() \u2013 Get or set the value of form elements. Many jQuery methods implicitly iterate over the entire collection, applying their behavior to each matched element. In most cases, the \"getter\" signature returns the result from the first element in a jQuery collection while the setter acts over the entire collection of matched elements. $ ( \"li\" ). addClass ( \"newClass\" ); // Each <li> in the document will have the class \"newClass\" added. Add / remove class \u00b6 $ ( \"a\" ). addClass ( \"test\" ); $ ( \"a\" ). removeClass ( \"test\" ); $ ( \"div\" ). click ( function () { if ( $ ( this ). hasClass ( \"protected\" ) ) { $ ( this ) . animate ({ left : - 10 }) . animate ({ left : 10 }) . animate ({ left : - 10 }) . animate ({ left : 10 }) . animate ({ left : 0 }); } }); if ( $ ( \"#myDiv\" ). is ( \".pretty.awesome\" ) ) { $ ( \"#myDiv\" ). show (); } var isVisible = $ ( \"#myDiv\" ). is ( \":visible\" ); if ( $ ( \"#myDiv\" ). is ( \":hidden\" ) ) { $ ( \"#myDiv\" ). show (); } Set / get element attributes \u00b6 $ ( \"a\" ). attr ( \"href\" , \"allMyHrefsAreTheSameNow.html\" ); $ ( \"a\" ). attr ({ title : \"all titles are the same too!\" , href : \"somethingNew.html\" }); CSS \u00b6 Getting CSS properties. $ ( \"h1\" ). css ( \"fontSize\" ); // Returns a string such as \"19px\".$( \"h1\" ).css( \"font-size\" ); // Also works. Setting CSS properties. $ ( \"h1\" ). css ( \"fontSize\" , \"100px\" ); // Setting an individual property.// Setting multiple properties.$( \"h1\" ).css({fontSize: \"100px\",color: \"red\"}); Data \u00b6 Storing and retrieving data related to an element. $ ( \"#myDiv\" ). data ( \"keyName\" , { foo : \"bar\" } ); $ ( \"#myDiv\" ). data ( \"keyName\" ); // Returns { foo: \"bar\" } Storing a relationship between elements using .data() $ ( \"#myList li\" ). each ( function () { var li = $ ( this ); var div = li . find ( \"div.content\" ); li . data ( \"contentDiv\" , div ); }); Later, we don't have to find the div again; we can just read it from the list item's data var firstLi = $ ( \"#myList li:first\" ); firstLi . data ( \"contentDiv\" ). html ( \"new content\" ); Utility functions \u00b6 .trim, .each, .map, inArray, isArray, isFunction, isNumeric, .type Returns \"lots of extra whitespace\" $ . trim ( \" lots of extra whitespace \" ); Iterate over an JS array or object \u00b6 $ . each ([ \"foo\" , \"bar\" , \"baz\" ], function ( idx , val ) { console . log ( \"element \" + idx + \" is \" + val ); }); $ . each ({ foo : \"bar\" , baz : \"bim\" }, function ( k , v ) { console . log ( k + \" : \" + v ); }); HOWEVER, use this form for jQuery objects $ ( \"li\" ). each ( function ( index , element ){ console . log ( $ ( this ). text () ); }); var myArray = [ 1 , 2 , 3 , 5 ]; if ( $ . inArray ( 4 , myArray ) !== - 1 ) { console . log ( \"found it!\" ); } $ . isArray ([]); // true $ . isFunction ( function () {}); // true $ . isNumeric ( 3.14 ); // true $ . type ( true ); // \"boolean\" $ . type ( 3 ); // \"number\" $ . type ( \"test\" ); // \"string\" $ . type ( function () {} ); // \"function\" $ . type ( new Boolean () ); // \"boolean\" $ . type ( new Number ( 3 ) ); // \"number\" $ . type ( new String ( 'test' ) ); // \"string\" $ . type ( new Function () ); // \"function\" $ . type ( [] ); // \"array\" $ . type ( null ); // \"null\" $ . type ( /test/ ); // \"regexp\" $ . type ( new Date () ); // \"date\" $.map and .map \u00b6 < li id = \"a\" >< /li> < li id = \"b\" >< /li> < li id = \"c\" >< /li> < script > var arr = [{ id : \"a\" , tagName : \"li\" }, { id : \"b\" , tagName : \"li\" }, { id : \"c\" , tagName : \"li\" }]; // Returns [ \"a\", \"b\", \"c\" ] $ ( \"li\" ). map ( function ( index , element ) { return element . id ; }). get (); // Also returns [ \"a\", \"b\", \"c\" ] // Note that the value comes first with $.map $ . map ( arr , function ( value , index ) { return value . id ; }); Event Handling \u00b6 var hiddenBox = $ ( \"#banner-message\" ); $ ( \"#button-container button\" ). on ( \"click\" , function ( event ) { hiddenBox . show (); }); The on method is useful for binding the same handler function to multiple events, when you want to provide data to the event handler, when you are working with custom events, or when you want to pass an object of multiple events and handlers. Event setup using a convenience method like .click(), .focus(), .blur(), .change() \u00b6 $ ( \"p\" ). click ( function () { console . log ( \"You clicked a paragraph!\" ); }); The hover helper function \u00b6 $ ( \"#menu li\" ). hover ( function () { $ ( this ). toggleClass ( \"hover\" ); }); The event object is most commonly used to prevent the default action of the event via the .preventDefault() method. However, the event object contains a number of other useful properties and methods, including: pageX, pageY, type, which, data Use this code to inspect it in your browser console $ ( \"div\" ). on ( \"click\" , function ( event ) { console . log ( \"event object:\" ); console . dir ( event ); }); Preventing a link from being followed \u00b6 $ ( \"a\" ). click ( function ( eventObject ) { var elem = $ ( this ); if ( elem . attr ( \"href\" ). match ( /evil/ ) ) { eventObject . preventDefault (); elem . addClass ( \"evil\" ); } }); Event setup using the .on() method with data \u00b6 $ ( \"input\" ). on ( \"change\" , { foo : \"bar\" }, // Associate data with event binding function ( eventObject ) { console . log ( \"An input value has changed! \" , eventObject . data . foo ); } ); // Binding multiple events with different handlers $ ( \"p\" ). on ({ \"click\" : function () { console . log ( \"clicked!\" ); }, \"mouseover\" : function () { console . log ( \"hovered!\" ); } }); // Tearing down all click handlers on a selection $ ( \"p\" ). off ( \"click\" ); // As of jQuery 1.7, attach an event handler to the body element that // is listening for clicks, and will respond whenever any button is // clicked on the page. $ ( \"body\" ). on ({ click : function ( event ) { alert ( \"Hello.\" ); } }, \"button\" ); // An alternative to the previous example, using slightly different syntax. $ ( \"body\" ). on ( \"click\" , \"button\" , function ( event ) { alert ( \"Hello.\" ); }); // Attach a delegated event handler with a more refined selector $ ( \"#list\" ). on ( \"click\" , \"a[href^='http']\" , function ( event ) { $ ( this ). attr ( \"target\" , \"_blank\" ); }); Effects \u00b6 Instantaneously hide all paragraphs $ ( \"p\" ). hide (); // slowly $ ( \"p\" ). hide ( \"slow\" ); Instantaneously show all divs that have the hidden style class $ ( \"div.hidden\" ). show (); Instantaneously toggle the display of all paragraphs $ ( \"p\" ). toggle (); Fade in all hidden paragraphs; then add a style class to them (correct with animation callback) $ ( \"p.hidden\" ). fadeIn ( 750 , function () { // this = DOM element which has just finished being animated $ ( this ). addClass ( \"lookAtMe\" ); }); Ajax \u00b6 $ . ajax ({ url : \"/api/getWeather\" , data : { zipcode : 97201 }, success : function ( result ) { $ ( \"#weather-temp\" ). html ( \"<strong>\" + result + \"</strong> degrees\" ); } }); Using the core $.ajax() method $ . ajax ({ // The URL for the request url : \"post.php\" , // The data to send (will be converted to a query string) data : { id : 123 }, // Whether this is a POST or GET request type : \"GET\" , // The type of data we expect back dataType : \"json\" , }) // Code to run if the request succeeds (is done); // The response is passed to the function . done ( function ( json ) { $ ( \"<h1>\" ). text ( json . title ). appendTo ( \"body\" ); $ ( \"<div class=\\\"content\\\">\" ). html ( json . html ). appendTo ( \"body\" ); }) // Code to run if the request fails; the raw request and // status codes are passed to the function . fail ( function ( xhr , status , errorThrown ) { alert ( \"Sorry, there was a problem!\" ); console . log ( \"Error: \" + errorThrown ); console . log ( \"Status: \" + status ); console . dir ( xhr ); }) Code to run regardless of success or failure; . always ( function ( xhr , status ) { alert ( \"The request is complete!\" ); }); Simple convenience methods such as $.get(), $.getScript(), $.getJSON(), $.post(), and $().load(). $ . get ( \"myhtmlpage.html\" , myCallBack ); // myCallback needs to be a parameterless function # with parameters $ . get ( \"myhtmlpage.html\" , function () { myCallBack ( param1 , param2 ); }); Load \u00b6 $ ( selector ). load ( URL , data , callback ); Using .load() to populate an element $ ( \"#newContent\" ). load ( \"/foo.html\" ); Using .load() to populate an element based on a selector $ ( \"#newContent\" ). load ( \"/foo.html #myDiv h1:first\" , function ( html ) { alert ( \"Content updated!\" ); }); Forms \u00b6 Turning form data into a query string $ ( \"#myForm\" ). serialize (); // Creates a query string like this: // field_1=something&field2=somethingElse Create an array of objects containing form data $ ( \"#myForm\" ). serializeArray (); Use validation to check for the presence of an input $ ( \"#form\" ). submit ( function ( event ) { // If .required's value's length is zero if ( $ ( \".required\" ). val (). length === 0 ) { // Usually show some kind of error message here // Prevent the form from submitting event . preventDefault (); } else { // Run $.ajax() here } }); Validate a phone number field $ ( \"#form\" ). submit ( function ( event ) { var inputtedPhoneNumber = $ ( \"#phone\" ). val (); // Match only numbers var phoneNumberRegex = /^\\d*$/ ; // If the phone number doesn't match the regex if ( ! phoneNumberRegex . test ( inputtedPhoneNumber ) ) { // Usually show some kind of error message here // Prevent the form from submitting event . preventDefault (); } else { // Run $.ajax() here } }); Feature testing \u00b6 Helper libraries (like Modernizr) that provide a simple, high-level API for determining if a browser has a specific feature available or not. if ( Modernizr . canvas ) { showGraphWithCanvas (); } else { showTable (); }","title":"jQuery"},{"location":"Web/jQuery/#jquery","text":"","title":"jQuery"},{"location":"Web/jQuery/#links","text":"jQuery API documentation Learn jQuery The jQuery library exposes its methods and properties via two properties of the window object called jQuery and $. $ is simply an alias for jQuery and it's often employed because it's shorter and faster to write.","title":"Links"},{"location":"Web/jQuery/#embed-jquery","text":"Either directly <!doctype html> < html > < head > < meta charset = \"utf-8\" > < title > Demo </ title > </ head > < body > < a href = \"http://jquery.com/\" > jQuery </ a > < script src = \"jquery.js\" ></ script > < script > // Your code goes here. </ script > </ body > or via a CDN < head > < script src = \"https://ajax.googleapis.com/ajax/libs/jquery/1.12.4/jquery.min.js\" ></ script > </ head >","title":"Embed JQuery"},{"location":"Web/jQuery/#launching-code-on-document-ready","text":"$ ( document ). ready ( function () { // Your code here. }); // Shorthand for $( document ).ready() $ ( function () { console . log ( \"ready!\" ); });","title":"Launching Code on Document Ready"},{"location":"Web/jQuery/#selecting-elements","text":"$ ( \"#myId\" ); // Note IDs must be unique per page. $ ( \".myClass\" ); $ ( \"input[name='first_name']\" ); $ ( \"#contents ul.people li\" ); $ ( \"div.myClass, ul.people\" ); If you have a variable containing a DOM element, and want to select elements related to that DOM element, simply wrap it in a jQuery object. var myDomElement = document . getElementById ( \"foo\" ); // A plain DOM element. $ ( myDomElement ). find ( \"a\" ); // Finds all anchors inside the DOM element.","title":"Selecting elements"},{"location":"Web/jQuery/#pull-a-native-dom-element-from-a-jquery-object","text":"A jQuery object is an array-like wrapper around one or more DOM elements. $ ( \"#foo\" )[ 0 ]; // Equivalent to document.getElementById( \"foo\" ) $ ( \"#foo\" ). get ( 0 ); // Testing whether a selection contains elements. if ( $ ( \"div.foo\" ). length ) { ... }","title":"Pull a native DOM element from a jQuery object"},{"location":"Web/jQuery/#refining-selections","text":"$ ( \"div.foo\" ). has ( \"p\" ); // div.foo elements that contain <p> tags $ ( \"h1\" ). not ( \".bar\" ); // h1 elements that don't have a class of bar $ ( \"ul li\" ). filter ( \".current\" ); // unordered list items with class of current $ ( \"ul li\" ). first (); // just the first unordered list item $ ( \"ul li\" ). eq ( 5 ); // the sixth $ ( \"form :checked\" ); // :checked targets checked checkboxes","title":"Refining selections."},{"location":"Web/jQuery/#dom-traversal-and-manipulation","text":"Get the <button> element with the class 'continue' and change its HTML to 'Next Step...' $ ( \"button.continue\" ). html ( \"Next Step...\" ) $ ( \"h1\" ). html (); // returns the html","title":"DOM Traversal and Manipulation"},{"location":"Web/jQuery/#chaining","text":"$ ( \"#content\" ). find ( \"h3\" ). eq ( 2 ). html ( \"new text for the third h3!\" ); $ ( \"#content\" ) . find ( \"h3\" ) . eq ( 2 ) . html ( \"new text for the third h3!\" ) . end () // Restores the selection to all h3s in #content . eq ( 0 ) . html ( \"new text for the first h3!\" );","title":"Chaining"},{"location":"Web/jQuery/#manipulation","text":".html() \u2013 Get or set the HTML contents. .text() \u2013 Get or set the text contents; HTML will be stripped. .attr() \u2013 Get or set the value of the provided attribute. .width() \u2013 Get or set the width in pixels of the first element in the selection as an integer. .height() \u2013 Get or set the height in pixels of the first element in the selection as an integer. .position() \u2013 Get an object with position information for the first element in the selection, relative to its first positioned ancestor. This is a getter only. .val() \u2013 Get or set the value of form elements. Many jQuery methods implicitly iterate over the entire collection, applying their behavior to each matched element. In most cases, the \"getter\" signature returns the result from the first element in a jQuery collection while the setter acts over the entire collection of matched elements. $ ( \"li\" ). addClass ( \"newClass\" ); // Each <li> in the document will have the class \"newClass\" added.","title":"Manipulation"},{"location":"Web/jQuery/#add-remove-class","text":"$ ( \"a\" ). addClass ( \"test\" ); $ ( \"a\" ). removeClass ( \"test\" ); $ ( \"div\" ). click ( function () { if ( $ ( this ). hasClass ( \"protected\" ) ) { $ ( this ) . animate ({ left : - 10 }) . animate ({ left : 10 }) . animate ({ left : - 10 }) . animate ({ left : 10 }) . animate ({ left : 0 }); } }); if ( $ ( \"#myDiv\" ). is ( \".pretty.awesome\" ) ) { $ ( \"#myDiv\" ). show (); } var isVisible = $ ( \"#myDiv\" ). is ( \":visible\" ); if ( $ ( \"#myDiv\" ). is ( \":hidden\" ) ) { $ ( \"#myDiv\" ). show (); }","title":"Add / remove class"},{"location":"Web/jQuery/#set-get-element-attributes","text":"$ ( \"a\" ). attr ( \"href\" , \"allMyHrefsAreTheSameNow.html\" ); $ ( \"a\" ). attr ({ title : \"all titles are the same too!\" , href : \"somethingNew.html\" });","title":"Set / get element attributes"},{"location":"Web/jQuery/#css","text":"Getting CSS properties. $ ( \"h1\" ). css ( \"fontSize\" ); // Returns a string such as \"19px\".$( \"h1\" ).css( \"font-size\" ); // Also works. Setting CSS properties. $ ( \"h1\" ). css ( \"fontSize\" , \"100px\" ); // Setting an individual property.// Setting multiple properties.$( \"h1\" ).css({fontSize: \"100px\",color: \"red\"});","title":"CSS"},{"location":"Web/jQuery/#data","text":"Storing and retrieving data related to an element. $ ( \"#myDiv\" ). data ( \"keyName\" , { foo : \"bar\" } ); $ ( \"#myDiv\" ). data ( \"keyName\" ); // Returns { foo: \"bar\" } Storing a relationship between elements using .data() $ ( \"#myList li\" ). each ( function () { var li = $ ( this ); var div = li . find ( \"div.content\" ); li . data ( \"contentDiv\" , div ); }); Later, we don't have to find the div again; we can just read it from the list item's data var firstLi = $ ( \"#myList li:first\" ); firstLi . data ( \"contentDiv\" ). html ( \"new content\" );","title":"Data"},{"location":"Web/jQuery/#utility-functions","text":".trim, .each, .map, inArray, isArray, isFunction, isNumeric, .type Returns \"lots of extra whitespace\" $ . trim ( \" lots of extra whitespace \" );","title":"Utility functions"},{"location":"Web/jQuery/#iterate-over-an-js-array-or-object","text":"$ . each ([ \"foo\" , \"bar\" , \"baz\" ], function ( idx , val ) { console . log ( \"element \" + idx + \" is \" + val ); }); $ . each ({ foo : \"bar\" , baz : \"bim\" }, function ( k , v ) { console . log ( k + \" : \" + v ); }); HOWEVER, use this form for jQuery objects $ ( \"li\" ). each ( function ( index , element ){ console . log ( $ ( this ). text () ); }); var myArray = [ 1 , 2 , 3 , 5 ]; if ( $ . inArray ( 4 , myArray ) !== - 1 ) { console . log ( \"found it!\" ); } $ . isArray ([]); // true $ . isFunction ( function () {}); // true $ . isNumeric ( 3.14 ); // true $ . type ( true ); // \"boolean\" $ . type ( 3 ); // \"number\" $ . type ( \"test\" ); // \"string\" $ . type ( function () {} ); // \"function\" $ . type ( new Boolean () ); // \"boolean\" $ . type ( new Number ( 3 ) ); // \"number\" $ . type ( new String ( 'test' ) ); // \"string\" $ . type ( new Function () ); // \"function\" $ . type ( [] ); // \"array\" $ . type ( null ); // \"null\" $ . type ( /test/ ); // \"regexp\" $ . type ( new Date () ); // \"date\"","title":"Iterate over an JS array or object"},{"location":"Web/jQuery/#map-and-map","text":"< li id = \"a\" >< /li> < li id = \"b\" >< /li> < li id = \"c\" >< /li> < script > var arr = [{ id : \"a\" , tagName : \"li\" }, { id : \"b\" , tagName : \"li\" }, { id : \"c\" , tagName : \"li\" }]; // Returns [ \"a\", \"b\", \"c\" ] $ ( \"li\" ). map ( function ( index , element ) { return element . id ; }). get (); // Also returns [ \"a\", \"b\", \"c\" ] // Note that the value comes first with $.map $ . map ( arr , function ( value , index ) { return value . id ; });","title":"$.map and .map"},{"location":"Web/jQuery/#event-handling","text":"var hiddenBox = $ ( \"#banner-message\" ); $ ( \"#button-container button\" ). on ( \"click\" , function ( event ) { hiddenBox . show (); }); The on method is useful for binding the same handler function to multiple events, when you want to provide data to the event handler, when you are working with custom events, or when you want to pass an object of multiple events and handlers.","title":"Event Handling"},{"location":"Web/jQuery/#event-setup-using-a-convenience-method-like-click-focus-blur-change","text":"$ ( \"p\" ). click ( function () { console . log ( \"You clicked a paragraph!\" ); });","title":"Event setup using a convenience method like  .click(), .focus(), .blur(), .change()"},{"location":"Web/jQuery/#the-hover-helper-function","text":"$ ( \"#menu li\" ). hover ( function () { $ ( this ). toggleClass ( \"hover\" ); }); The event object is most commonly used to prevent the default action of the event via the .preventDefault() method. However, the event object contains a number of other useful properties and methods, including: pageX, pageY, type, which, data Use this code to inspect it in your browser console $ ( \"div\" ). on ( \"click\" , function ( event ) { console . log ( \"event object:\" ); console . dir ( event ); });","title":"The hover helper function"},{"location":"Web/jQuery/#preventing-a-link-from-being-followed","text":"$ ( \"a\" ). click ( function ( eventObject ) { var elem = $ ( this ); if ( elem . attr ( \"href\" ). match ( /evil/ ) ) { eventObject . preventDefault (); elem . addClass ( \"evil\" ); } });","title":"Preventing a link from being followed"},{"location":"Web/jQuery/#event-setup-using-the-on-method-with-data","text":"$ ( \"input\" ). on ( \"change\" , { foo : \"bar\" }, // Associate data with event binding function ( eventObject ) { console . log ( \"An input value has changed! \" , eventObject . data . foo ); } ); // Binding multiple events with different handlers $ ( \"p\" ). on ({ \"click\" : function () { console . log ( \"clicked!\" ); }, \"mouseover\" : function () { console . log ( \"hovered!\" ); } }); // Tearing down all click handlers on a selection $ ( \"p\" ). off ( \"click\" ); // As of jQuery 1.7, attach an event handler to the body element that // is listening for clicks, and will respond whenever any button is // clicked on the page. $ ( \"body\" ). on ({ click : function ( event ) { alert ( \"Hello.\" ); } }, \"button\" ); // An alternative to the previous example, using slightly different syntax. $ ( \"body\" ). on ( \"click\" , \"button\" , function ( event ) { alert ( \"Hello.\" ); }); // Attach a delegated event handler with a more refined selector $ ( \"#list\" ). on ( \"click\" , \"a[href^='http']\" , function ( event ) { $ ( this ). attr ( \"target\" , \"_blank\" ); });","title":"Event setup using the .on() method with data"},{"location":"Web/jQuery/#effects","text":"Instantaneously hide all paragraphs $ ( \"p\" ). hide (); // slowly $ ( \"p\" ). hide ( \"slow\" ); Instantaneously show all divs that have the hidden style class $ ( \"div.hidden\" ). show (); Instantaneously toggle the display of all paragraphs $ ( \"p\" ). toggle (); Fade in all hidden paragraphs; then add a style class to them (correct with animation callback) $ ( \"p.hidden\" ). fadeIn ( 750 , function () { // this = DOM element which has just finished being animated $ ( this ). addClass ( \"lookAtMe\" ); });","title":"Effects"},{"location":"Web/jQuery/#ajax","text":"$ . ajax ({ url : \"/api/getWeather\" , data : { zipcode : 97201 }, success : function ( result ) { $ ( \"#weather-temp\" ). html ( \"<strong>\" + result + \"</strong> degrees\" ); } }); Using the core $.ajax() method $ . ajax ({ // The URL for the request url : \"post.php\" , // The data to send (will be converted to a query string) data : { id : 123 }, // Whether this is a POST or GET request type : \"GET\" , // The type of data we expect back dataType : \"json\" , }) // Code to run if the request succeeds (is done); // The response is passed to the function . done ( function ( json ) { $ ( \"<h1>\" ). text ( json . title ). appendTo ( \"body\" ); $ ( \"<div class=\\\"content\\\">\" ). html ( json . html ). appendTo ( \"body\" ); }) // Code to run if the request fails; the raw request and // status codes are passed to the function . fail ( function ( xhr , status , errorThrown ) { alert ( \"Sorry, there was a problem!\" ); console . log ( \"Error: \" + errorThrown ); console . log ( \"Status: \" + status ); console . dir ( xhr ); }) Code to run regardless of success or failure; . always ( function ( xhr , status ) { alert ( \"The request is complete!\" ); }); Simple convenience methods such as $.get(), $.getScript(), $.getJSON(), $.post(), and $().load(). $ . get ( \"myhtmlpage.html\" , myCallBack ); // myCallback needs to be a parameterless function # with parameters $ . get ( \"myhtmlpage.html\" , function () { myCallBack ( param1 , param2 ); });","title":"Ajax"},{"location":"Web/jQuery/#load","text":"$ ( selector ). load ( URL , data , callback ); Using .load() to populate an element $ ( \"#newContent\" ). load ( \"/foo.html\" ); Using .load() to populate an element based on a selector $ ( \"#newContent\" ). load ( \"/foo.html #myDiv h1:first\" , function ( html ) { alert ( \"Content updated!\" ); });","title":"Load"},{"location":"Web/jQuery/#forms","text":"Turning form data into a query string $ ( \"#myForm\" ). serialize (); // Creates a query string like this: // field_1=something&field2=somethingElse Create an array of objects containing form data $ ( \"#myForm\" ). serializeArray (); Use validation to check for the presence of an input $ ( \"#form\" ). submit ( function ( event ) { // If .required's value's length is zero if ( $ ( \".required\" ). val (). length === 0 ) { // Usually show some kind of error message here // Prevent the form from submitting event . preventDefault (); } else { // Run $.ajax() here } }); Validate a phone number field $ ( \"#form\" ). submit ( function ( event ) { var inputtedPhoneNumber = $ ( \"#phone\" ). val (); // Match only numbers var phoneNumberRegex = /^\\d*$/ ; // If the phone number doesn't match the regex if ( ! phoneNumberRegex . test ( inputtedPhoneNumber ) ) { // Usually show some kind of error message here // Prevent the form from submitting event . preventDefault (); } else { // Run $.ajax() here } });","title":"Forms"},{"location":"Web/jQuery/#feature-testing","text":"Helper libraries (like Modernizr) that provide a simple, high-level API for determining if a browser has a specific feature available or not. if ( Modernizr . canvas ) { showGraphWithCanvas (); } else { showTable (); }","title":"Feature testing"},{"location":"Windows/Command_Prompt_Here/","text":"Just type \"cmd\" to the location bar. It will start a new command prompt in current path. OR Hold the \"Shift\" key while right-clicking a blank space in the desired folder to bring up a more verbose context menu. One of the options is \"Open PowerShell Here\". To re-enable the \"Open Command Prompt Here\" (disabled by the Windows 10 Creators Update): Link Link 2","title":"Command Line"},{"location":"dotNET/ASPdotNET/","text":"Using Yeoman to generate a ASP.NET Core app from a template \u00b6 Install Node.js and npm \u00b6 To get started with Yeoman, install Node.js . The installer includes Node.js and npm . for Mac OS X brew install node for Windows OS choco install nodejs Install Yeoman and Bower \u00b6 npm install -g yo npm install -g bower Install generator-aspnet \u00b6 npm install -g generator-aspnet Run with yo aspnet See also: Building Projects with Yeoman on docs.asp.net Optionaly install the yeoman extension in Visual Studio Code \u00b6 Architecture \u00b6 Onion Architecture In ASP.NET Core MVC Example of a Web API built on ASP.NET Core Routing Examples \u00b6 public class TestController : Controller { // /hello [Route(\"/hello\")] public IActionResult Hello () => Ok ( \"Hello\" ); // /hi only GET method [Route(\"/hi\")] [ HttpGet ] public IActionResult Hi () => Ok ( \"Hi\" ); //Alternative for previous [HttpGet(\"/hi\")] public IActionResult Hi () => Ok ( \"Hi\" ); } //Route prefix [Route(\"test\")] public class TestController2 : Controller { //You can have multiple routes on an action [Route(\"\")] // /test [Route(\"hello\")] // /test/hello public IActionResult Hello () => Ok ( \"Hello\" ); // Maps to both: // /test/hi, and: // /hi [Route(\"/hi\")] // Overrides the prefix with /, you can also use ~/ [Route(\"hi\")] public IActionResult Hi () => Ok ( \"Hi\" ); // /test/greet/Joon -> maps Joon to the name parameter [Route(\"greet/{name}\")] public IActionResult Greet ( string name ) => Ok ( $ \"Hello {name}!\" ); //Parameters can be optional // /test/greetopt -> name == null // /test/greetopt/Joon -> name == Joon [Route(\"greetopt/{name?}\")] public IActionResult GreetOptional ( string name ) => Ok ( name == null ? \"No name\" : \"Hi!\" ); } // You can use [controller], [action], and [area] to create generic templates [Route(\"[controller] /[ action ] \")] public class MyController : Controller { // /my/info public IActionResult Info () => Ok ( \"Info\" ); // /my/i [Route(\"/[controller] / i \")] public IActionResult Info2 () => Ok ( \"Info2\" ); } [Route(\"users\")] public class SelectionController : Controller { //You can use constraints to influence route selection //Do not use for validation! // /users/123 [Route(\"{id:int}\")] public IActionResult Int ( int id ) => Ok ( $ \"Looked up user id {id}\" ); // /users/joonas [Route(\"{name:alpha}\")] public IActionResult String ( string name ) => Ok ( $ \"User name {name}\" ); }","title":"ASP.NET Core app"},{"location":"dotNET/ASPdotNET/#using-yeoman-to-generate-a-aspnet-core-app-from-a-template","text":"","title":"Using Yeoman to generate a ASP.NET Core app from a template"},{"location":"dotNET/ASPdotNET/#install-nodejs-and-npm","text":"To get started with Yeoman, install Node.js . The installer includes Node.js and npm . for Mac OS X brew install node for Windows OS choco install nodejs","title":"Install Node.js and npm"},{"location":"dotNET/ASPdotNET/#install-yeoman-and-bower","text":"npm install -g yo npm install -g bower","title":"Install Yeoman and Bower"},{"location":"dotNET/ASPdotNET/#install-generator-aspnet","text":"npm install -g generator-aspnet Run with yo aspnet See also: Building Projects with Yeoman on docs.asp.net","title":"Install generator-aspnet"},{"location":"dotNET/ASPdotNET/#optionaly-install-the-yeoman-extension-in-visual-studio-code","text":"","title":"Optionaly install the yeoman extension in Visual Studio Code"},{"location":"dotNET/ASPdotNET/#architecture","text":"Onion Architecture In ASP.NET Core MVC Example of a Web API built on ASP.NET Core","title":"Architecture"},{"location":"dotNET/ASPdotNET/#routing-examples","text":"public class TestController : Controller { // /hello [Route(\"/hello\")] public IActionResult Hello () => Ok ( \"Hello\" ); // /hi only GET method [Route(\"/hi\")] [ HttpGet ] public IActionResult Hi () => Ok ( \"Hi\" ); //Alternative for previous [HttpGet(\"/hi\")] public IActionResult Hi () => Ok ( \"Hi\" ); } //Route prefix [Route(\"test\")] public class TestController2 : Controller { //You can have multiple routes on an action [Route(\"\")] // /test [Route(\"hello\")] // /test/hello public IActionResult Hello () => Ok ( \"Hello\" ); // Maps to both: // /test/hi, and: // /hi [Route(\"/hi\")] // Overrides the prefix with /, you can also use ~/ [Route(\"hi\")] public IActionResult Hi () => Ok ( \"Hi\" ); // /test/greet/Joon -> maps Joon to the name parameter [Route(\"greet/{name}\")] public IActionResult Greet ( string name ) => Ok ( $ \"Hello {name}!\" ); //Parameters can be optional // /test/greetopt -> name == null // /test/greetopt/Joon -> name == Joon [Route(\"greetopt/{name?}\")] public IActionResult GreetOptional ( string name ) => Ok ( name == null ? \"No name\" : \"Hi!\" ); } // You can use [controller], [action], and [area] to create generic templates [Route(\"[controller] /[ action ] \")] public class MyController : Controller { // /my/info public IActionResult Info () => Ok ( \"Info\" ); // /my/i [Route(\"/[controller] / i \")] public IActionResult Info2 () => Ok ( \"Info2\" ); } [Route(\"users\")] public class SelectionController : Controller { //You can use constraints to influence route selection //Do not use for validation! // /users/123 [Route(\"{id:int}\")] public IActionResult Int ( int id ) => Ok ( $ \"Looked up user id {id}\" ); // /users/joonas [Route(\"{name:alpha}\")] public IActionResult String ( string name ) => Ok ( $ \"User name {name}\" ); }","title":"Routing Examples"},{"location":"dotNET/AkkadotNET/","text":"Akka.NET Examples \u00b6 // To install Akka.NET Distributed Actor Framework, run the following command in the Package Manager Console // PM> Install-Package Akka // PM> Install-Package Akka.Remote // Installing with Topshelf is as easy as calling .exe install on the command line. using System ; using System.Collections.Generic ; using System.Linq ; using System.Text ; using System.Threading.Tasks ; // Add these two lines using Akka ; using Akka.Actor ; using Topshelf ; // http://topshelf.readthedocs.io/en/latest/configuration/quickstart.html internal class Program { private static void Main ( string [] args ) { // \u2018x\u2019 exposes all of the host level configuration HostFactory . Run ( x => { x . Service < ActorService >( s => // telling Topshelf that there is a service of type ActorService. service configuration options exposed through the \u2018s\u2019 parameter. { s . ConstructUsing ( name => new ActorService ()); // build an instance of the service; new or pull from IoC container s . WhenStarted ( service => service . Start ()); s . WhenStopped ( service => service . Stop ()); ////continue and restart directives are also available //s.WhenPaused(service => service.Pause()); //s.WhenContinued(service => service.Continue()); //s.WhenShutdown(service => service.Shutdown()); }); x . RunAsLocalSystem (); // service \u2018run as\u2019 the \u2018local system\u2019. Alternatively x.RunAsLocalSystem(); x.RunAs(\"username\", \"password\"); x.RunAsPrompt(); x . UseAssemblyInfoForServiceInfo (); //x.SetDescription(\"Orchestrator Host\"); //x.SetDisplayName(\"Orchestrator\"); // display name for the winservice in the windows service monitor //x.SetServiceName(\"Orchestrator\"); // service name for the winservice in the windows service monitor //x.SetInstanceName(\"MyService\"); // instance name of the service, which is combined with the base service name and separated by a $. }); } } // <summary> /// This class acts as an interface between the application and TopShelf /// </summary> public class ActorService { private ActorSystem system ; public void Start () { // Create a new actor system (a container for actors) this . system = ActorSystem . Create ( \"MainSystem\" ); } public async void Stop () { //this is where you stop your actor system await this . system . Terminate (); } private void Create () { // Create your actor and get a reference to it. // This will be an \"ActorRef\", which is not a // reference to the actual actor instance // but rather a client or proxy to it. var job = system . ActorOf < JobActor >( \"Job\" ); //or: var myActor = system.ActorOf(Props.Create<JobActor>()); // Send a message to the actor job . Tell ( new Message < string >( \"Hello World\" )); } } // Example immutable message class - C# 7.0 public class Message < T > { public Message ( T data ) { this . Data = data ; } public T Data { get ; } // Allow convesion to a tuple public void Deconstruct ( out T data ) { data = this . Data ; } } // Example of ReceiveActor public class JobActor : ReceiveActor { private readonly ILoggingAdapter log = Context . GetLogger (); public JobActor () { Receive < Message < string >>( message => { log . Info ( \"Received String message: {0}\" , message . Data ); // Console.WriteLine(message.Data); // reply back Sender . Tell ( message ); }); } } // Example of untyped actor public class MyActor : UntypedActor { private ActorRef logger = Context . ActorOf < LogActor >(); // if any child, e.g. the logger above. throws an exception // apply the rules below // e.g. Restart the child, if 10 exceptions occur in 30 seconds or // less, then stop the actor protected override SupervisorStrategy SupervisorStrategy () { return new OneForOneStrategy ( //or AllForOneStrategy maxNumberOfRetries : 10 , duration : TimeSpan . FromSeconds ( 30 ), decider : Decider . From ( x => { //Maybe we consider ArithmeticException to not be application critical //so we just ignore the error and keep going. if ( x is ArithmeticException ) return Directive . Resume ; //Error that we cannot recover from, stop the failing actor else if ( x is NotSupportedException ) return Directive . Stop ; //In all other cases, just restart the failing actor else return Directive . Restart ; })); } } // Example of long-running operation in an Actor - PipeTo / Become / Stash /* If you stick a long-running operation inside your Receive method then your actors will be unable to process any messages, including system messages, until that operation finishes. And if it\u2019s possible that the operation will never finish, it\u2019s possible to deadlock your actor. The solution to this is simple: you need to encapsulate any long-running I/O-bound or CPU-bound operations inside a Task and make it possible to cancel that task from within the actor. Here\u2019s an example of how you can use behavior switching, stashing, and control messages to do this. https://petabridge.com/blog/akka-actors-finite-state-machines-switchable-behavior/ */ public class FooActor : ReceiveActor , IWithUnboundedStash { private Task _runningTask ; private CancellationTokenSource _cancel ; public IStash Stash { get ; set ;} public FooActor (){ _cancel = new CancellationTokenSource (); Ready (); } private void Ready (){ Receive < Start >( s => { var self = Self ; // closure _runningTask = Task . Run (() => { // ... work }, _cancel . Token ). ContinueWith ( x => { if ( x . IsCancelled || x . IsFaulted ) return new Failed (); return new Finished (); }, TaskContinuationOptions . ExecuteSynchronously ) . PipeTo ( self ); // switch behavior Become ( Working ); }) } private void Working (){ Receive < Cancel >( cancel => { _cancel . Cancel (); // cancel work BecomeReady (); }); Receive < Failed >( f => BecomeReady ()); Receive < Finished >( f => BecomeReady ()); ReceiveAny ( o => Stash . Stash ()); } private void BecomeReady (){ _cancel = new CancellationTokenSource (); Stash . UnstashAll (); Become ( Ready ); } }","title":"Akka.NET Examples"},{"location":"dotNET/AkkadotNET/#akkanet-examples","text":"// To install Akka.NET Distributed Actor Framework, run the following command in the Package Manager Console // PM> Install-Package Akka // PM> Install-Package Akka.Remote // Installing with Topshelf is as easy as calling .exe install on the command line. using System ; using System.Collections.Generic ; using System.Linq ; using System.Text ; using System.Threading.Tasks ; // Add these two lines using Akka ; using Akka.Actor ; using Topshelf ; // http://topshelf.readthedocs.io/en/latest/configuration/quickstart.html internal class Program { private static void Main ( string [] args ) { // \u2018x\u2019 exposes all of the host level configuration HostFactory . Run ( x => { x . Service < ActorService >( s => // telling Topshelf that there is a service of type ActorService. service configuration options exposed through the \u2018s\u2019 parameter. { s . ConstructUsing ( name => new ActorService ()); // build an instance of the service; new or pull from IoC container s . WhenStarted ( service => service . Start ()); s . WhenStopped ( service => service . Stop ()); ////continue and restart directives are also available //s.WhenPaused(service => service.Pause()); //s.WhenContinued(service => service.Continue()); //s.WhenShutdown(service => service.Shutdown()); }); x . RunAsLocalSystem (); // service \u2018run as\u2019 the \u2018local system\u2019. Alternatively x.RunAsLocalSystem(); x.RunAs(\"username\", \"password\"); x.RunAsPrompt(); x . UseAssemblyInfoForServiceInfo (); //x.SetDescription(\"Orchestrator Host\"); //x.SetDisplayName(\"Orchestrator\"); // display name for the winservice in the windows service monitor //x.SetServiceName(\"Orchestrator\"); // service name for the winservice in the windows service monitor //x.SetInstanceName(\"MyService\"); // instance name of the service, which is combined with the base service name and separated by a $. }); } } // <summary> /// This class acts as an interface between the application and TopShelf /// </summary> public class ActorService { private ActorSystem system ; public void Start () { // Create a new actor system (a container for actors) this . system = ActorSystem . Create ( \"MainSystem\" ); } public async void Stop () { //this is where you stop your actor system await this . system . Terminate (); } private void Create () { // Create your actor and get a reference to it. // This will be an \"ActorRef\", which is not a // reference to the actual actor instance // but rather a client or proxy to it. var job = system . ActorOf < JobActor >( \"Job\" ); //or: var myActor = system.ActorOf(Props.Create<JobActor>()); // Send a message to the actor job . Tell ( new Message < string >( \"Hello World\" )); } } // Example immutable message class - C# 7.0 public class Message < T > { public Message ( T data ) { this . Data = data ; } public T Data { get ; } // Allow convesion to a tuple public void Deconstruct ( out T data ) { data = this . Data ; } } // Example of ReceiveActor public class JobActor : ReceiveActor { private readonly ILoggingAdapter log = Context . GetLogger (); public JobActor () { Receive < Message < string >>( message => { log . Info ( \"Received String message: {0}\" , message . Data ); // Console.WriteLine(message.Data); // reply back Sender . Tell ( message ); }); } } // Example of untyped actor public class MyActor : UntypedActor { private ActorRef logger = Context . ActorOf < LogActor >(); // if any child, e.g. the logger above. throws an exception // apply the rules below // e.g. Restart the child, if 10 exceptions occur in 30 seconds or // less, then stop the actor protected override SupervisorStrategy SupervisorStrategy () { return new OneForOneStrategy ( //or AllForOneStrategy maxNumberOfRetries : 10 , duration : TimeSpan . FromSeconds ( 30 ), decider : Decider . From ( x => { //Maybe we consider ArithmeticException to not be application critical //so we just ignore the error and keep going. if ( x is ArithmeticException ) return Directive . Resume ; //Error that we cannot recover from, stop the failing actor else if ( x is NotSupportedException ) return Directive . Stop ; //In all other cases, just restart the failing actor else return Directive . Restart ; })); } } // Example of long-running operation in an Actor - PipeTo / Become / Stash /* If you stick a long-running operation inside your Receive method then your actors will be unable to process any messages, including system messages, until that operation finishes. And if it\u2019s possible that the operation will never finish, it\u2019s possible to deadlock your actor. The solution to this is simple: you need to encapsulate any long-running I/O-bound or CPU-bound operations inside a Task and make it possible to cancel that task from within the actor. Here\u2019s an example of how you can use behavior switching, stashing, and control messages to do this. https://petabridge.com/blog/akka-actors-finite-state-machines-switchable-behavior/ */ public class FooActor : ReceiveActor , IWithUnboundedStash { private Task _runningTask ; private CancellationTokenSource _cancel ; public IStash Stash { get ; set ;} public FooActor (){ _cancel = new CancellationTokenSource (); Ready (); } private void Ready (){ Receive < Start >( s => { var self = Self ; // closure _runningTask = Task . Run (() => { // ... work }, _cancel . Token ). ContinueWith ( x => { if ( x . IsCancelled || x . IsFaulted ) return new Failed (); return new Finished (); }, TaskContinuationOptions . ExecuteSynchronously ) . PipeTo ( self ); // switch behavior Become ( Working ); }) } private void Working (){ Receive < Cancel >( cancel => { _cancel . Cancel (); // cancel work BecomeReady (); }); Receive < Failed >( f => BecomeReady ()); Receive < Finished >( f => BecomeReady ()); ReceiveAny ( o => Stash . Stash ()); } private void BecomeReady (){ _cancel = new CancellationTokenSource (); Stash . UnstashAll (); Become ( Ready ); } }","title":"Akka.NET Examples"},{"location":"dotNET/C%23/","text":"C# Cheatsheets \u00b6 Quick Reference Cheatsheet C# 6.0 / 7.0 - what is new \u00b6 Readonly properties \u00b6 public string FirstName { get ; private set ; } // private set is accessible from the entire class public string LastName { get ; } // accessible only in constructor public ICollection < double > Grades { get ; } = new List < double >(); // property initializer Expression-bodied function members \u00b6 public override string ToString () => \"Hi!\" ; Using static \u00b6 using static System . String ; // also common: // using static System.Math; // using static System.Linq.Enumerable; if ( IsNullOrWhiteSpace ( lastName )) throw new ArgumentException ( message : \"Cannot be blank\" , paramName : nameof ( lastName )); Null checking \u00b6 var first = person ?. FirstName ; first = person ?. FirstName ?? \"Unspecified\" ; // preferred event handing in C# 6: this . SomethingHappened ?. Invoke ( this , eventArgs ); String interpolation \u00b6 public string GetFormattedGradePoint () => $ \"Name: {LastName}, {FirstName}. G.P.A: {Grades.Average():F2}\" ; Exception Filters \u00b6 public static async Task < string > MakeRequest () { var client = new System . Net . Http . HttpClient (); var streamTask = client . GetStringAsync ( \"https://localHost:10000\" ); try { var responseText = await streamTask ; return responseText ; } catch ( System . Net . Http . HttpRequestException e ) when ( e . Message . Contains ( \"301\" )) { return \"Site Moved\" ; } } List and dict initializers \u00b6 private List < string > messages = new List < string > { \"Page not Found\" , \"Page moved, but left a forwarding address.\" , \"The web server can't come out to play today.\" }; private Dictionary < int , string > webErrors = new Dictionary < int , string > { [404] = \"Page not Found\" , [302] = \"Page moved, but left a forwarding address.\" , [500] = \"The web server can't come out to play today.\" }; Out variables \u00b6 if ( int . TryParse ( input , out int result )) WriteLine ( result ); else WriteLine ( \"Could not parse input\" ); Tuples \u00b6 var letters = ( \"a\" , \"b\" ); ( string Alpha , string Beta ) namedLetters = ( \"a\" , \"b\" ); var alphabetStart = ( Alpha : \"a\" , Beta : \"b\" ); public class Point { public Point ( double x , double y ) { this . X = x ; this . Y = y ; } public double X { get ; } public double Y { get ; } // Deconstruct method public void Deconstruct ( out double x , out double y ) { x = this . X ; y = this . Y ; } } Ref return values \u00b6 public static ref int Find3 ( int [,] matrix , Func < int , bool > predicate ) { for ( int i = 0 ; i < matrix . GetLength ( 0 ); i ++) for ( int j = 0 ; j < matrix . GetLength ( 1 ); j ++) if ( predicate ( matrix [ i , j ])) return ref matrix [ i , j ]; throw new InvalidOperationException ( \"Not found\" ); } ref var item = ref MatrixSearch . Find3 ( matrix , ( val ) => val == 42 ); Console . WriteLine ( item ); item = 24 ; Console . WriteLine ( matrix [ 4 , 2 ]); Local functions \u00b6 public static IEnumerable < char > AlphabetSubset3 ( char start , char end ) { if (( start < 'a' ) || ( start > 'z' )) throw new ArgumentOutOfRangeException ( paramName : nameof ( start ), message : \"start must be a letter\" ); if (( end < 'a' ) || ( end > 'z' )) throw new ArgumentOutOfRangeException ( paramName : nameof ( end ), message : \"end must be a letter\" ); if ( end <= start ) throw new ArgumentException ( $ \"{nameof(end)} must be greater than {nameof(start)}\" ); return alphabetSubsetImplementation (); IEnumerable < char > alphabetSubsetImplementation () { for ( var c = start ; c < end ; c ++) yield return c ; } }","title":"C# Cheatsheet"},{"location":"dotNET/C%23/#c-cheatsheets","text":"Quick Reference Cheatsheet","title":"C# Cheatsheets"},{"location":"dotNET/C%23/#c-60-70-what-is-new","text":"","title":"C# 6.0 / 7.0 - what is new"},{"location":"dotNET/C%23/#readonly-properties","text":"public string FirstName { get ; private set ; } // private set is accessible from the entire class public string LastName { get ; } // accessible only in constructor public ICollection < double > Grades { get ; } = new List < double >(); // property initializer","title":"Readonly properties"},{"location":"dotNET/C%23/#expression-bodied-function-members","text":"public override string ToString () => \"Hi!\" ;","title":"Expression-bodied function members"},{"location":"dotNET/C%23/#using-static","text":"using static System . String ; // also common: // using static System.Math; // using static System.Linq.Enumerable; if ( IsNullOrWhiteSpace ( lastName )) throw new ArgumentException ( message : \"Cannot be blank\" , paramName : nameof ( lastName ));","title":"Using static"},{"location":"dotNET/C%23/#null-checking","text":"var first = person ?. FirstName ; first = person ?. FirstName ?? \"Unspecified\" ; // preferred event handing in C# 6: this . SomethingHappened ?. Invoke ( this , eventArgs );","title":"Null checking"},{"location":"dotNET/C%23/#string-interpolation","text":"public string GetFormattedGradePoint () => $ \"Name: {LastName}, {FirstName}. G.P.A: {Grades.Average():F2}\" ;","title":"String interpolation"},{"location":"dotNET/C%23/#exception-filters","text":"public static async Task < string > MakeRequest () { var client = new System . Net . Http . HttpClient (); var streamTask = client . GetStringAsync ( \"https://localHost:10000\" ); try { var responseText = await streamTask ; return responseText ; } catch ( System . Net . Http . HttpRequestException e ) when ( e . Message . Contains ( \"301\" )) { return \"Site Moved\" ; } }","title":"Exception Filters"},{"location":"dotNET/C%23/#list-and-dict-initializers","text":"private List < string > messages = new List < string > { \"Page not Found\" , \"Page moved, but left a forwarding address.\" , \"The web server can't come out to play today.\" }; private Dictionary < int , string > webErrors = new Dictionary < int , string > { [404] = \"Page not Found\" , [302] = \"Page moved, but left a forwarding address.\" , [500] = \"The web server can't come out to play today.\" };","title":"List and dict initializers"},{"location":"dotNET/C%23/#out-variables","text":"if ( int . TryParse ( input , out int result )) WriteLine ( result ); else WriteLine ( \"Could not parse input\" );","title":"Out variables"},{"location":"dotNET/C%23/#tuples","text":"var letters = ( \"a\" , \"b\" ); ( string Alpha , string Beta ) namedLetters = ( \"a\" , \"b\" ); var alphabetStart = ( Alpha : \"a\" , Beta : \"b\" ); public class Point { public Point ( double x , double y ) { this . X = x ; this . Y = y ; } public double X { get ; } public double Y { get ; } // Deconstruct method public void Deconstruct ( out double x , out double y ) { x = this . X ; y = this . Y ; } }","title":"Tuples"},{"location":"dotNET/C%23/#ref-return-values","text":"public static ref int Find3 ( int [,] matrix , Func < int , bool > predicate ) { for ( int i = 0 ; i < matrix . GetLength ( 0 ); i ++) for ( int j = 0 ; j < matrix . GetLength ( 1 ); j ++) if ( predicate ( matrix [ i , j ])) return ref matrix [ i , j ]; throw new InvalidOperationException ( \"Not found\" ); } ref var item = ref MatrixSearch . Find3 ( matrix , ( val ) => val == 42 ); Console . WriteLine ( item ); item = 24 ; Console . WriteLine ( matrix [ 4 , 2 ]);","title":"Ref return values"},{"location":"dotNET/C%23/#local-functions","text":"public static IEnumerable < char > AlphabetSubset3 ( char start , char end ) { if (( start < 'a' ) || ( start > 'z' )) throw new ArgumentOutOfRangeException ( paramName : nameof ( start ), message : \"start must be a letter\" ); if (( end < 'a' ) || ( end > 'z' )) throw new ArgumentOutOfRangeException ( paramName : nameof ( end ), message : \"end must be a letter\" ); if ( end <= start ) throw new ArgumentException ( $ \"{nameof(end)} must be greater than {nameof(start)}\" ); return alphabetSubsetImplementation (); IEnumerable < char > alphabetSubsetImplementation () { for ( var c = start ; c < end ; c ++) yield return c ; } }","title":"Local functions"},{"location":"dotNET/Multithreading/","text":"Advanced .NET Threading \u00b6 Advanced .NET Threading, Part 1: Thread Fundamentals Advanced .NET Threading, Part 2: Compute-Bound Async Operations Advanced .NET Threading, Part 3: I/O-Bound Async Operations Advanced .NET Threading, Part 4: Thread Synchronization Primitives Advanced .NET Threading, Part 5: Thread Synchronization Locks","title":".NET Multithreading"},{"location":"dotNET/Multithreading/#advanced-net-threading","text":"Advanced .NET Threading, Part 1: Thread Fundamentals Advanced .NET Threading, Part 2: Compute-Bound Async Operations Advanced .NET Threading, Part 3: I/O-Bound Async Operations Advanced .NET Threading, Part 4: Thread Synchronization Primitives Advanced .NET Threading, Part 5: Thread Synchronization Locks","title":"Advanced .NET Threading"},{"location":"dotNET/WPF/","text":"Useful Links \u00b6 WPF tutorial WPF Documentation WPF Samples WPF Tools Application \u00b6 <Application x:Class= \"ExpenseIt.App\" xmlns= \"http://schemas.microsoft.com/winfx/2006/xaml/presentation\" xmlns:x= \"http://schemas.microsoft.com/winfx/2006/xaml\" StartupUri= \"MainWindow.xaml\" > <Application.Resources> </Application.Resources> </Application> Commands \u00b6 WPF Commands WPF provides a set of predefined commands. The command library consists of the following classes: ApplicationCommands, NavigationCommands, MediaCommands, EditingCommands, and the ComponentCommands. <StackPanel> <StackPanel.ContextMenu> <ContextMenu> <MenuItem Command= \"ApplicationCommands.Properties\" /> </ContextMenu> </StackPanel.ContextMenu> <Menu> <MenuItem Command= \"ApplicationCommands.Paste\" /> </Menu> <TextBox /></StackPanel> <Window.InputBindings> <KeyBinding Key= \"B\" Modifiers= \"Control\" Command= \"ApplicationCommands.Open\" /></Window.InputBindings> Page \u00b6 <Page xmlns= \"http://schemas.microsoft.com/winfx/2006/xaml/presentation\" xmlns:x= \"http://schemas.microsoft.com/winfx/2006/xaml\" x:Class= \"ExampleNamespace.ExampleCode\" > <StackPanel> <Button> Button 1 </Button> <Button> Button 2 </Button> <Button> Button 3 </Button> </StackPanel> </Page> Styles \u00b6 <Window.Resources> <!--A Style that affects all TextBlocks--> <Style TargetType= \"TextBlock\" > <Setter Property= \"HorizontalAlignment\" Value= \"Center\" /> <Setter Property= \"FontFamily\" Value= \"Comic Sans MS\" /> <Setter Property= \"FontSize\" Value= \"14\" /></Style></Window.Resources> <!--A Style that extends the previous TextBlock Style--><!--This is a \"named style\" with an x:Key of TitleText--> <Style BasedOn= \"{StaticResource {x:Type TextBlock}}\" TargetType= \"TextBlock\" x:Key= \"TitleText\" > <Setter Property= \"FontSize\" Value= \"26\" /> <Setter Property= \"Foreground\" > <Setter.Value> <LinearGradientBrush StartPoint= \"0.5,0\" EndPoint= \"0.5,1\" > <LinearGradientBrush.GradientStops> <GradientStop Offset= \"0.0\" Color= \"#90DDDD\" /> <GradientStop Offset= \"1.0\" Color= \"#5BFFFF\" /> </LinearGradientBrush.GradientStops> </LinearGradientBrush> </Setter.Value> </Setter></Style> Triggers \u00b6 <Style x:Key= \"SpecialButton\" TargetType= \"{x:Type Button}\" > <Style.Triggers> <Trigger Property= \"Button.IsMouseOver\" Value= \"true\" > <Setter Property = \"Background\" Value= \"Red\" /> </Trigger> <Trigger Property= \"Button.IsPressed\" Value= \"true\" > <Setter Property = \"Foreground\" Value= \"Green\" /> </Trigger> </Style.Triggers></Style>","title":"WPF"},{"location":"dotNET/WPF/#useful-links","text":"WPF tutorial WPF Documentation WPF Samples WPF Tools","title":"Useful Links"},{"location":"dotNET/WPF/#application","text":"<Application x:Class= \"ExpenseIt.App\" xmlns= \"http://schemas.microsoft.com/winfx/2006/xaml/presentation\" xmlns:x= \"http://schemas.microsoft.com/winfx/2006/xaml\" StartupUri= \"MainWindow.xaml\" > <Application.Resources> </Application.Resources> </Application>","title":"Application"},{"location":"dotNET/WPF/#commands","text":"WPF Commands WPF provides a set of predefined commands. The command library consists of the following classes: ApplicationCommands, NavigationCommands, MediaCommands, EditingCommands, and the ComponentCommands. <StackPanel> <StackPanel.ContextMenu> <ContextMenu> <MenuItem Command= \"ApplicationCommands.Properties\" /> </ContextMenu> </StackPanel.ContextMenu> <Menu> <MenuItem Command= \"ApplicationCommands.Paste\" /> </Menu> <TextBox /></StackPanel> <Window.InputBindings> <KeyBinding Key= \"B\" Modifiers= \"Control\" Command= \"ApplicationCommands.Open\" /></Window.InputBindings>","title":"Commands"},{"location":"dotNET/WPF/#page","text":"<Page xmlns= \"http://schemas.microsoft.com/winfx/2006/xaml/presentation\" xmlns:x= \"http://schemas.microsoft.com/winfx/2006/xaml\" x:Class= \"ExampleNamespace.ExampleCode\" > <StackPanel> <Button> Button 1 </Button> <Button> Button 2 </Button> <Button> Button 3 </Button> </StackPanel> </Page>","title":"Page"},{"location":"dotNET/WPF/#styles","text":"<Window.Resources> <!--A Style that affects all TextBlocks--> <Style TargetType= \"TextBlock\" > <Setter Property= \"HorizontalAlignment\" Value= \"Center\" /> <Setter Property= \"FontFamily\" Value= \"Comic Sans MS\" /> <Setter Property= \"FontSize\" Value= \"14\" /></Style></Window.Resources> <!--A Style that extends the previous TextBlock Style--><!--This is a \"named style\" with an x:Key of TitleText--> <Style BasedOn= \"{StaticResource {x:Type TextBlock}}\" TargetType= \"TextBlock\" x:Key= \"TitleText\" > <Setter Property= \"FontSize\" Value= \"26\" /> <Setter Property= \"Foreground\" > <Setter.Value> <LinearGradientBrush StartPoint= \"0.5,0\" EndPoint= \"0.5,1\" > <LinearGradientBrush.GradientStops> <GradientStop Offset= \"0.0\" Color= \"#90DDDD\" /> <GradientStop Offset= \"1.0\" Color= \"#5BFFFF\" /> </LinearGradientBrush.GradientStops> </LinearGradientBrush> </Setter.Value> </Setter></Style>","title":"Styles"},{"location":"dotNET/WPF/#triggers","text":"<Style x:Key= \"SpecialButton\" TargetType= \"{x:Type Button}\" > <Style.Triggers> <Trigger Property= \"Button.IsMouseOver\" Value= \"true\" > <Setter Property = \"Background\" Value= \"Red\" /> </Trigger> <Trigger Property= \"Button.IsPressed\" Value= \"true\" > <Setter Property = \"Foreground\" Value= \"Green\" /> </Trigger> </Style.Triggers></Style>","title":"Triggers"}]}